{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "Jrem3faKxQ-e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processor\t: 0\r\n",
      "vendor_id\t: GenuineIntel\r\n",
      "cpu family\t: 6\r\n",
      "model\t\t: 42\r\n",
      "model name\t: Intel(R) Core(TM) i5-2410M CPU @ 2.30GHz\r\n",
      "stepping\t: 7\r\n",
      "microcode\t: 0x1b\r\n",
      "cpu MHz\t\t: 2693.745\r\n",
      "cache size\t: 3072 KB\r\n",
      "physical id\t: 0\r\n",
      "siblings\t: 4\r\n",
      "core id\t\t: 0\r\n",
      "cpu cores\t: 2\r\n",
      "apicid\t\t: 0\r\n",
      "initial apicid\t: 0\r\n",
      "fpu\t\t: yes\r\n",
      "fpu_exception\t: yes\r\n",
      "cpuid level\t: 13\r\n",
      "wp\t\t: yes\r\n",
      "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic popcnt tsc_deadline_timer aes xsave avx lahf_lm epb pti tpr_shadow vnmi flexpriority ept vpid xsaveopt dtherm ida arat pln pts\r\n",
      "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs itlb_multihit\r\n",
      "bogomips\t: 4589.46\r\n",
      "clflush size\t: 64\r\n",
      "cache_alignment\t: 64\r\n",
      "address sizes\t: 36 bits physical, 48 bits virtual\r\n",
      "power management:\r\n",
      "\r\n",
      "processor\t: 1\r\n",
      "vendor_id\t: GenuineIntel\r\n",
      "cpu family\t: 6\r\n",
      "model\t\t: 42\r\n",
      "model name\t: Intel(R) Core(TM) i5-2410M CPU @ 2.30GHz\r\n",
      "stepping\t: 7\r\n",
      "microcode\t: 0x1b\r\n",
      "cpu MHz\t\t: 2875.411\r\n",
      "cache size\t: 3072 KB\r\n",
      "physical id\t: 0\r\n",
      "siblings\t: 4\r\n",
      "core id\t\t: 1\r\n",
      "cpu cores\t: 2\r\n",
      "apicid\t\t: 2\r\n",
      "initial apicid\t: 2\r\n",
      "fpu\t\t: yes\r\n",
      "fpu_exception\t: yes\r\n",
      "cpuid level\t: 13\r\n",
      "wp\t\t: yes\r\n",
      "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic popcnt tsc_deadline_timer aes xsave avx lahf_lm epb pti tpr_shadow vnmi flexpriority ept vpid xsaveopt dtherm ida arat pln pts\r\n",
      "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs itlb_multihit\r\n",
      "bogomips\t: 4589.46\r\n",
      "clflush size\t: 64\r\n",
      "cache_alignment\t: 64\r\n",
      "address sizes\t: 36 bits physical, 48 bits virtual\r\n",
      "power management:\r\n",
      "\r\n",
      "processor\t: 2\r\n",
      "vendor_id\t: GenuineIntel\r\n",
      "cpu family\t: 6\r\n",
      "model\t\t: 42\r\n",
      "model name\t: Intel(R) Core(TM) i5-2410M CPU @ 2.30GHz\r\n",
      "stepping\t: 7\r\n",
      "microcode\t: 0x1b\r\n",
      "cpu MHz\t\t: 2693.984\r\n",
      "cache size\t: 3072 KB\r\n",
      "physical id\t: 0\r\n",
      "siblings\t: 4\r\n",
      "core id\t\t: 0\r\n",
      "cpu cores\t: 2\r\n",
      "apicid\t\t: 1\r\n",
      "initial apicid\t: 1\r\n",
      "fpu\t\t: yes\r\n",
      "fpu_exception\t: yes\r\n",
      "cpuid level\t: 13\r\n",
      "wp\t\t: yes\r\n",
      "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic popcnt tsc_deadline_timer aes xsave avx lahf_lm epb pti tpr_shadow vnmi flexpriority ept vpid xsaveopt dtherm ida arat pln pts\r\n",
      "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs itlb_multihit\r\n",
      "bogomips\t: 4589.46\r\n",
      "clflush size\t: 64\r\n",
      "cache_alignment\t: 64\r\n",
      "address sizes\t: 36 bits physical, 48 bits virtual\r\n",
      "power management:\r\n",
      "\r\n",
      "processor\t: 3\r\n",
      "vendor_id\t: GenuineIntel\r\n",
      "cpu family\t: 6\r\n",
      "model\t\t: 42\r\n",
      "model name\t: Intel(R) Core(TM) i5-2410M CPU @ 2.30GHz\r\n",
      "stepping\t: 7\r\n",
      "microcode\t: 0x1b\r\n",
      "cpu MHz\t\t: 2780.453\r\n",
      "cache size\t: 3072 KB\r\n",
      "physical id\t: 0\r\n",
      "siblings\t: 4\r\n",
      "core id\t\t: 1\r\n",
      "cpu cores\t: 2\r\n",
      "apicid\t\t: 3\r\n",
      "initial apicid\t: 3\r\n",
      "fpu\t\t: yes\r\n",
      "fpu_exception\t: yes\r\n",
      "cpuid level\t: 13\r\n",
      "wp\t\t: yes\r\n",
      "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic popcnt tsc_deadline_timer aes xsave avx lahf_lm epb pti tpr_shadow vnmi flexpriority ept vpid xsaveopt dtherm ida arat pln pts\r\n",
      "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs itlb_multihit\r\n",
      "bogomips\t: 4589.46\r\n",
      "clflush size\t: 64\r\n",
      "cache_alignment\t: 64\r\n",
      "address sizes\t: 36 bits physical, 48 bits virtual\r\n",
      "power management:\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!cat /proc/cpuinfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dwytQjvowfJn"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zouBvUL5wfJ1"
   },
   "source": [
    "Install the dependencies:\n",
    "```sh\n",
    "pip install gym\n",
    "pip install gym[atari]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UO2Qiac0wfJ2"
   },
   "outputs": [],
   "source": [
    "#!pip install gym\n",
    "#!pip install gym[atari]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EoGoxWUKwfJ2"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Dg_L4q7wfJ3"
   },
   "source": [
    "# Useful Resources\n",
    "* [Manual of the game](https://www.gamesdatabase.org/Media/SYSTEM/Atari_2600/Manual/formated/Freeway_-_1981_-_Zellers.pdf)\n",
    "* [Freeway Disassembly](http://www.bjars.com/disassemblies.html)\n",
    "* [Atari Ram Annotations](https://github.com/mila-iqia/atari-representation-learning/blob/master/atariari/benchmark/ram_annotations.py)\n",
    "* [Freeway Benchmarks](https://paperswithcode.com/sota/atari-games-on-atari-2600-freeway)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FRMKbhDnwfJ3"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K6a-SnJSwfJ4"
   },
   "source": [
    "# Description of the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d7zqsIqYwfJ4"
   },
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ElEulV-zwfJ5"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "THKZimFLwfJ5"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xaNNWhVvwfJ6"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')  # Enable importing from `src` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_xr8d1wNwfJ6"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from collections import defaultdict\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import gym\n",
    "\n",
    "import src.agents as agents\n",
    "import src.episode as episode\n",
    "import src.environment as environment\n",
    "import src.aux_plots as aux_plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MfYSYsBewfJ7"
   },
   "outputs": [],
   "source": [
    "def print_result(i, scores, total_reward, score):\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Run [{i:4}] - Total reward: {total_reward:7.2f} Mean scores: {sum(scores) / len(scores):.2f} Means Scores[:-10]: {sum(scores[-10:]) / len(scores[-10:]):5.2f} Score: {score:2} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5flFQhAjwfJ7"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q3DPZ-mywfJ8"
   },
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "as9iSi1ZwfJ8"
   },
   "source": [
    "We will be using the Open AI Gym framework in this study......."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HFea7m9jwfJ8",
    "outputId": "cc1699a0-6734-44a9-9d1b-32439bf19371"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space: Discrete(3)\n",
      "Observation Space: Box(0, 255, (128,), uint8)\n"
     ]
    }
   ],
   "source": [
    "env, initial_state = environment.get_env()\n",
    "\n",
    "print(\"Action Space:\", env.action_space)\n",
    "print(\"Observation Space:\", env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZXxMlWzrwfJ-"
   },
   "source": [
    "The agent in this game has three possible actions:\n",
    "\n",
    "* 0: Stay\n",
    "* 1: Move forward\n",
    "* 2: Move back"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WHKLZZ3gwfJ-"
   },
   "source": [
    "TODO: Talk a bit about the observation space of 128 bytes of RAM..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "903u-z_KwfJ_"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Qb9jtwzwfJ_"
   },
   "source": [
    "# Representing the state of the game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CKs3bkr4wfJ_"
   },
   "source": [
    "TODO: explain why we must reduce the state space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "acOfGcPIwfKA"
   },
   "source": [
    "```\n",
    "      14  # Chicken Y\n",
    "    , 16  # Chicken Lane Collide\n",
    "    , 18  # Chicken Collision flag (with the bottom car)\n",
    "    , 22  # Car X Direction\n",
    "    , 23, 24, 25, 26, 27, 28, 29, 30, 31, 32  # Z Car Patterns\n",
    "    , 33, 34, 35, 36, 37, 38, 39, 40, 41, 42  # Car Motion Timmers\n",
    "    , 43, 44, 45, 46, 47, 48, 49, 50, 51, 52  # Car Motions\n",
    "    , 87, 88  # Car Shape Ptr\n",
    "    # TODO: test if this makes any difference\n",
    "    , 89, 90  # Chicken Shape Ptr\n",
    "    # TODO: test if this makes any difference\n",
    "    , 106, 107  # Chicken Sounds\n",
    "    , 108, 109, 110, 111, 112, 113, 114, 115, 116, 117  # Car X Coords\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mG7MMdV3wfKA"
   },
   "outputs": [],
   "source": [
    "RAM_mask = [\n",
    "      14  # Chicken Y\n",
    "    , 16  # Chicken Lane Collide\n",
    "    , 108, 109, 110, 111, 112, 113, 114, 115, 116, 117  # Car X Coords\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mbOycyQ6wfKB"
   },
   "outputs": [],
   "source": [
    "def reduce_state(ob):\n",
    "    # Doesn't matter where we were hit\n",
    "    ob[16] = 1 if ob[16] != 255 else 0\n",
    "\n",
    "    # Reduce chicken y-position \n",
    "    ob[14] = ob[14] // 3\n",
    "\n",
    "    for b in range(108, 118):\n",
    "        # The chicken is in the x-posistion ~49\n",
    "        if ob[b] < 19 or ob[b] > 79:\n",
    "            # We don't need to represent cars far from the chicken\n",
    "            ob[b] = 0\n",
    "        else:\n",
    "            # Reduce the cars x-positions sample space \n",
    "            ob[b] = ob[b] // 3\n",
    "\n",
    "    return ob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p8PMoBIwwfKB"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sBRWJs8uwfKB"
   },
   "source": [
    "# Reward Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8RooW0vqwfKB"
   },
   "outputs": [],
   "source": [
    "def reward_policy(reward, ob, action):\n",
    "    if reward == 1:\n",
    "        reward = reward_policy.REWARD_IF_CROSS\n",
    "    \n",
    "    elif ob[16] == 1:  # Collision!\n",
    "        reward = reward_policy.REWARD_IF_COLISION\n",
    "       \n",
    "    elif action == 0:  # Don't move\n",
    "        reward = reward_policy.REWARD_IF_STILL\n",
    "        \n",
    "    elif action == 1:  # Move forward\n",
    "        reward = reward_policy.REWARD_IF_FW\n",
    "    \n",
    "    elif action == 2:  # Move backward\n",
    "        reward = reward_policy.REWARD_IF_BW\n",
    "\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7utUDH-BwfKC"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pfepciFnwfKC"
   },
   "source": [
    "# Baseline: 1 action (only move forward):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7PTWFOnlwfKC"
   },
   "outputs": [],
   "source": [
    "baseline_scores = environment.run(agents.Baseline, render=False, n_runs=50, verbose=False)\n",
    "\n",
    "with open(\"baseline_scores.txt\", \"w\") as f:\n",
    "    for item in baseline_scores:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zj1ZVfp_wfKD"
   },
   "outputs": [],
   "source": [
    "# with open(\"baseline_scores.txt\") as f:\n",
    "#     baseline_scores = [int(x) for x in  f.read().splitlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bLFkvChowfKD",
    "outputId": "b6bb3899-97c7-498c-e130-d21fd2fe38f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline mean score: 21.7\n"
     ]
    }
   ],
   "source": [
    "# Mean score\n",
    "baseline_mean_score = sum(baseline_scores) / len(baseline_scores) \n",
    "print(\"Baseline mean score:\", baseline_mean_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CV-IQCWTwfKD"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xq2EdkmSwfKD"
   },
   "source": [
    "# Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3rjTCHMkwfKE"
   },
   "source": [
    "## Changing hyper parameters: number of actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CLHAPwXCwfKE"
   },
   "source": [
    "### - 2 actions (move forward or stay):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "j9uEblyrwfKE"
   },
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "AVAILABLE_ACTIONS = 2\n",
    "N0 = 2.5\n",
    "\n",
    "reward_policy.REWARD_IF_CROSS = 500\n",
    "reward_policy.REWARD_IF_COLISION = -10\n",
    "reward_policy.REWARD_IF_STILL = -1\n",
    "reward_policy.REWARD_IF_FW = 0 \n",
    "\n",
    "n_runs = 7000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "wn1RiDkMwfKE"
   },
   "outputs": [],
   "source": [
    "env, initial_state = environment.get_env()\n",
    "agent = agents.QLearning(gamma=GAMMA, available_actions=AVAILABLE_ACTIONS, N0=N0)\n",
    "\n",
    "scores_2act = []\n",
    "total_rewards_2act = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "46ZNmEfawfKE",
    "outputId": "1e2ce4a4-709f-4630-d124-7d7e8a0b5917"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run [   0] - Total reward: 2908.00 Mean scores: 10.00 Means Scores[:-10]: 10.00 Score: 10 \n",
      "CPU times: user 1min 53s, sys: 32 ms, total: 1min 53s\n",
      "Wall time: 1min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for i in range(n_runs):\n",
    "    #render = i % 100 == 0\n",
    "    render = 0\n",
    "\n",
    "    game_over = False\n",
    "    state = env.reset()\n",
    "    state = reduce_state(state)[RAM_mask].data.tobytes()  # Select useful bytes\n",
    "    action = agent.act(state)\n",
    "   \n",
    "    score = 0\n",
    "    total_reward = 0\n",
    "\n",
    "    while not game_over:\n",
    "        if render:\n",
    "            time.sleep(0.015)\n",
    "            env.render()\n",
    "\n",
    "        old_state = state\n",
    "        ob, reward, game_over, _ = env.step(action)\n",
    "\n",
    "        ob = reduce_state(ob)\n",
    "        reward = reward_policy(reward, ob, action)\n",
    "\n",
    "        total_reward += reward\n",
    "\n",
    "        if reward == reward_policy.REWARD_IF_CROSS:\n",
    "            score += 1\n",
    "\n",
    "        state = ob[RAM_mask].data.tobytes()\n",
    "\n",
    "        agent.update_Q(old_state, state, action, reward)\n",
    "\n",
    "        action = agent.act(state)  # Next action\n",
    "\n",
    "    scores_2act.append(score)\n",
    "    total_rewards_2act.append(total_reward)\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print_result(i, scores_2act, total_reward, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "mScfZv4YwfKF"
   },
   "outputs": [],
   "source": [
    "with open(\"QL/QL_scores_2act.txt\", \"w\") as f:\n",
    "   for item in scores_2act:\n",
    "       f.write(\"%s\\n\" % item)\n",
    "\n",
    "with open(\"QL/QL_total_rewards_2act.txt\", \"w\") as f:\n",
    "   for item in total_rewards_2act:\n",
    "       f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IOMD49DlwfKF"
   },
   "outputs": [],
   "source": [
    "# with open(\"QL/QL_scores_2act.txt\") as f:\n",
    "#    scores_2act = [int(x) for x in  f.read().splitlines()]\n",
    "\n",
    "# with open(\"QL/QL_total_rewards_2act.txt\") as f:\n",
    "#    total_rewards_2act = [int(x) for x in  f.read().splitlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Px_X3kacwfKH"
   },
   "source": [
    "### - 3 actions (move forward, stay or move backard):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ANTat1jXwfKH"
   },
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "AVAILABLE_ACTIONS = 3\n",
    "N0 = 2.5\n",
    "\n",
    "reward_policy.REWARD_IF_CROSS = 500\n",
    "reward_policy.REWARD_IF_COLISION = -10\n",
    "reward_policy.REWARD_IF_STILL = -1\n",
    "reward_policy.REWARD_IF_FW = 0 \n",
    "reward_policy.REWARD_IF_BW = -9\n",
    "\n",
    "n_runs = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1UhNEICuwfKH"
   },
   "outputs": [],
   "source": [
    "env, initial_state = environment.get_env()\n",
    "agent = agents.QLearning(gamma=GAMMA, available_actions=AVAILABLE_ACTIONS, N0=N0)\n",
    "\n",
    "scores_3act = []\n",
    "total_rewards_3act = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "12O86FFvwfKH",
    "outputId": "81a4af04-4995-4cae-8f79-d34f90081bbc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run [   0] - Total reward: -9239.00 Mean scores: 0.00 Means Scores[:-10]:  0.00 Score:  0 \n",
      "CPU times: user 1min 54s, sys: 80 ms, total: 1min 54s\n",
      "Wall time: 1min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for i in range(n_runs):\n",
    "    #render = i % 100 == 0\n",
    "    render = 0\n",
    "\n",
    "    game_over = False\n",
    "    state = env.reset()\n",
    "    state = reduce_state(state)[RAM_mask].data.tobytes()  # Select useful bytes\n",
    "    action = agent.act(state)\n",
    "   \n",
    "    score = 0\n",
    "    total_reward = 0\n",
    "\n",
    "    while not game_over:\n",
    "        if render:\n",
    "            time.sleep(0.015)\n",
    "            env.render()\n",
    "\n",
    "        old_state = state\n",
    "        ob, reward, game_over, _ = env.step(action)\n",
    "\n",
    "        ob = reduce_state(ob)\n",
    "        reward = reward_policy(reward, ob, action)\n",
    "\n",
    "        total_reward += reward\n",
    "\n",
    "        if reward == reward_policy.REWARD_IF_CROSS:\n",
    "            score += 1\n",
    "\n",
    "        state = ob[RAM_mask].data.tobytes()\n",
    "\n",
    "        agent.update_Q(old_state, state, action, reward)\n",
    "\n",
    "        action = agent.act(state)  # Next action\n",
    "\n",
    "    scores_3act.append(score)\n",
    "    total_rewards_3act.append(total_reward)\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print_result(i, scores_3act, total_reward, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KHXFIdQywfKI"
   },
   "outputs": [],
   "source": [
    "with open(\"QL/QL_scores_3act.txt\", \"w\") as f:\n",
    "   for item in scores_3act:\n",
    "       f.write(\"%s\\n\" % item)\n",
    "\n",
    "with open(\"QL/QL_total_rewards_3act.txt\", \"w\") as f:\n",
    "   for item in total_rewards_3act:\n",
    "       f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1s11r_hkwfKI"
   },
   "outputs": [],
   "source": [
    "# with open(\"QL/QL_scores_3act.txt\") as f:\n",
    "#    scores_3act = [int(x) for x in  f.read().splitlines()]\n",
    "\n",
    "# with open(\"QL/QL_total_rewards_3act.txt\") as f:\n",
    "#    total_rewards_3act = [int(x) for x in  f.read().splitlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l7N-xF7ewfKJ"
   },
   "source": [
    "### - Results with different number of actions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nJWgZyGkwfKJ",
    "outputId": "bad12e37-e2a6-4451-e165-2c85e406f8ba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'src.aux_plots' from '/home/aline/Documents/GitHub/FreewayGame/aline.almeida/src/aux_plots.py'>"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(aux_plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = [baseline_mean_score for i in range(3000)]\n",
    "aux_plots.plot_3scores(scores_3act[:3000], scores_2act[:3000], baseline[:3000], \"3 actions (BW, FW or STAY)\", \"2 actions (FW or STAY)\", \"Baseline mean score (FW only)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TmPY3tW_wfKK"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eJ_JiwOewfKK"
   },
   "source": [
    "## Changing hyper parameters: Reward values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uF-jelX7wfKK"
   },
   "source": [
    "### - Sparse reward: $+1$ if cross the street, $-1$ if collide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d9vK9JN2wfKL"
   },
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "AVAILABLE_ACTIONS = 2\n",
    "N0 = 2.5\n",
    "\n",
    "reward_policy.REWARD_IF_CROSS = 1\n",
    "reward_policy.REWARD_IF_COLISION = -1\n",
    "reward_policy.REWARD_IF_STILL = 0\n",
    "reward_policy.REWARD_IF_FW = 0 \n",
    "\n",
    "n_runs = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B7GlFdLjwfKL"
   },
   "outputs": [],
   "source": [
    "env, initial_state = environment.get_env()\n",
    "agent = agents.QLearning(gamma=GAMMA, available_actions=AVAILABLE_ACTIONS, N0=N0)\n",
    "\n",
    "scores_2act_sparseR = []\n",
    "total_rewards_2act_sparseR = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "alxlmpZWwfKL",
    "outputId": "bd3f86fc-c815-4536-da1c-63b55a7ba8bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run [   0] - Total reward:  -58.00 Mean scores: 11.00 Means Scores[:-10]: 11.00 Score: 11 \n",
      "CPU times: user 1min 58s, sys: 104 ms, total: 1min 59s\n",
      "Wall time: 1min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for i in range(n_runs):\n",
    "    #render = i % 100 == 0\n",
    "    render = 0\n",
    "\n",
    "    game_over = False\n",
    "    state = env.reset()\n",
    "    state = reduce_state(state)[RAM_mask].data.tobytes()  # Select useful bytes\n",
    "    action = agent.act(state)\n",
    "   \n",
    "    score = 0\n",
    "    total_reward = 0\n",
    "\n",
    "    while not game_over:\n",
    "        if render:\n",
    "            time.sleep(0.015)\n",
    "            env.render()\n",
    "\n",
    "        old_state = state\n",
    "        ob, reward, game_over, _ = env.step(action)\n",
    "\n",
    "        ob = reduce_state(ob)\n",
    "        reward = reward_policy(reward, ob, action)\n",
    "\n",
    "        total_reward += reward\n",
    "\n",
    "        if reward == reward_policy.REWARD_IF_CROSS:\n",
    "            score += 1\n",
    "\n",
    "        state = ob[RAM_mask].data.tobytes()\n",
    "\n",
    "        agent.update_Q(old_state, state, action, reward)\n",
    "\n",
    "        action = agent.act(state)  # Next action\n",
    "\n",
    "    scores_2act_sparseR.append(score)\n",
    "    total_rewards_2act_sparseR.append(total_reward)\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print_result(i, scores_2act_sparseR, total_reward, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IR0FEMu-wfKM"
   },
   "outputs": [],
   "source": [
    "with open(\"QL/QL_scores_2act_sparseR.txt\", \"w\") as f:\n",
    "   for item in scores_2act_sparseR:\n",
    "       f.write(\"%s\\n\" % item)\n",
    "\n",
    "with open(\"QL/QL_total_rewards_2act_sparseR.txt\", \"w\") as f:\n",
    "   for item in total_rewards_2act_sparseR:\n",
    "       f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AK_pLRpcwfKM"
   },
   "outputs": [],
   "source": [
    "# with open(\"QL/QL_scores_2act_sparseR.txt\") as f:\n",
    "#    scores_2act_sparseR = [int(x) for x in  f.read().splitlines()]\n",
    "\n",
    "# with open(\"QL/QL_total_rewards_2act_sparseR.txt\") as f:\n",
    "#    total_rewards_2act_sparseR = [int(x) for x in  f.read().splitlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b0_OosKUwfKM"
   },
   "source": [
    "### - Results with sparse and dense rewards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = [baseline_mean_score for i in range(3000)]\n",
    "aux_plots.plot_3scores(scores_2act_sparseR[:3000], scores_2act[:3000], baseline[:3000], \"Sparse Reward\", \"Dense Reward\", \"Baseline mean score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jIZrsQ3PwfKN"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xiogqiuFwfKN"
   },
   "source": [
    "## Changing hyper parameters: Learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zpVX0QAlwfKO"
   },
   "source": [
    "### - $N0$ = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "44azlmoDwfKO"
   },
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "AVAILABLE_ACTIONS = 2\n",
    "N0 =  0.001\n",
    "\n",
    "reward_policy.REWARD_IF_CROSS = 500\n",
    "reward_policy.REWARD_IF_COLISION = -10\n",
    "reward_policy.REWARD_IF_STILL = -1\n",
    "reward_policy.REWARD_IF_FW = 0 \n",
    "\n",
    "n_runs = 7000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2_n3VdHYwfKO"
   },
   "outputs": [],
   "source": [
    "env, initial_state = environment.get_env()\n",
    "agent = agents.QLearning(gamma=GAMMA, available_actions=AVAILABLE_ACTIONS, N0=N0)\n",
    "\n",
    "scores_2act_N0_0 = []\n",
    "total_rewards_2act_N0_0 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qrG2wIyywfKO",
    "outputId": "ca131a15-1daa-4208-8bd2-620e86ce947e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run [   0] - Total reward: 4732.00 Mean scores: 13.00 Means Scores[:-10]: 13.00 Score: 13 \n",
      "CPU times: user 1min 57s, sys: 60 ms, total: 1min 57s\n",
      "Wall time: 1min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for i in range(n_runs):\n",
    "    #render = i % 100 == 0\n",
    "    render = 0\n",
    "\n",
    "    game_over = False\n",
    "    state = env.reset()\n",
    "    state = reduce_state(state)[RAM_mask].data.tobytes()  # Select useful bytes\n",
    "    action = agent.act(state)\n",
    "   \n",
    "    score = 0\n",
    "    total_reward = 0\n",
    "\n",
    "    while not game_over:\n",
    "        if render:\n",
    "            time.sleep(0.015)\n",
    "            env.render()\n",
    "\n",
    "        old_state = state\n",
    "        ob, reward, game_over, _ = env.step(action)\n",
    "\n",
    "        ob = reduce_state(ob)\n",
    "        reward = reward_policy(reward, ob, action)\n",
    "\n",
    "        total_reward += reward\n",
    "\n",
    "        if reward == reward_policy.REWARD_IF_CROSS:\n",
    "            score += 1\n",
    "\n",
    "        state = ob[RAM_mask].data.tobytes()\n",
    "\n",
    "        agent.update_Q(old_state, state, action, reward)\n",
    "\n",
    "        action = agent.act(state)  # Next action\n",
    "\n",
    "    scores_2act_N0_0.append(score)\n",
    "    total_rewards_2act_N0_0.append(total_reward)\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print_result(i, scores_2act_N0_0, total_reward, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GVkTnBCBwfKP"
   },
   "outputs": [],
   "source": [
    "with open(\"QL/QL_scores_2act_N0_0.txt\", \"w\") as f:\n",
    "   for item in scores_2act_N0_0:\n",
    "       f.write(\"%s\\n\" % item)\n",
    "\n",
    "with open(\"QL/QL_total_rewards_2act_N0_0.txt\", \"w\") as f:\n",
    "   for item in total_rewards_2act_N0_0:\n",
    "       f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HeFMdhgHwfKP"
   },
   "outputs": [],
   "source": [
    "# with open(\"QL/QL_scores_2act_N0_0.txt\") as f:\n",
    "#    scores_2act_N0_0 = [int(x) for x in  f.read().splitlines()]\n",
    "\n",
    "# with open(\"QL/QL_total_rewards_2act_N0_0.txt\") as f:\n",
    "#    total_rewards_2act_N0_0 = [int(x) for x in  f.read().splitlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NMzF8aKqwfKP"
   },
   "source": [
    "### - $N0$ = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oc6LuFtbwfKP"
   },
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "AVAILABLE_ACTIONS = 2\n",
    "N0 =  5\n",
    "\n",
    "reward_policy.REWARD_IF_CROSS = 500\n",
    "reward_policy.REWARD_IF_COLISION = -10\n",
    "reward_policy.REWARD_IF_STILL = -1\n",
    "reward_policy.REWARD_IF_FW = 0 \n",
    "\n",
    "n_runs = 7000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n4iFS0glwfKP"
   },
   "outputs": [],
   "source": [
    "env, initial_state = environment.get_env()\n",
    "agent = agents.QLearning(gamma=GAMMA, available_actions=AVAILABLE_ACTIONS, N0=N0)\n",
    "\n",
    "scores_2act_N0_5 = []\n",
    "total_rewards_2act_N0_5 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "klOn-YMlwfKQ",
    "outputId": "13ec37c2-b128-4e29-f3a9-2ec0f8c97b81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run [   0] - Total reward: 4057.00 Mean scores: 12.00 Means Scores[:-10]: 12.00 Score: 12 \n",
      "CPU times: user 2min 1s, sys: 72 ms, total: 2min 1s\n",
      "Wall time: 2min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for i in range(n_runs):\n",
    "    #render = i % 100 == 0\n",
    "    render = 0\n",
    "\n",
    "    game_over = False\n",
    "    state = env.reset()\n",
    "    state = reduce_state(state)[RAM_mask].data.tobytes()  # Select useful bytes\n",
    "    action = agent.act(state)\n",
    "   \n",
    "    score = 0\n",
    "    total_reward = 0\n",
    "\n",
    "    while not game_over:\n",
    "        if render:\n",
    "            time.sleep(0.015)\n",
    "            env.render()\n",
    "\n",
    "        old_state = state\n",
    "        ob, reward, game_over, _ = env.step(action)\n",
    "\n",
    "        ob = reduce_state(ob)\n",
    "        reward = reward_policy(reward, ob, action)\n",
    "\n",
    "        total_reward += reward\n",
    "\n",
    "        if reward == reward_policy.REWARD_IF_CROSS:\n",
    "            score += 1\n",
    "\n",
    "        state = ob[RAM_mask].data.tobytes()\n",
    "\n",
    "        agent.update_Q(old_state, state, action, reward)\n",
    "\n",
    "        action = agent.act(state)  # Next action\n",
    "\n",
    "    scores_2act_N0_5.append(score)\n",
    "    total_rewards_2act_N0_5.append(total_reward)\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print_result(i, scores_2act_N0_5, total_reward, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5HA9nIyTwfKQ"
   },
   "outputs": [],
   "source": [
    "with open(\"QL/QL_scores_2act_N0_5.txt\", \"w\") as f:\n",
    "   for item in scores_2act_N0_5:\n",
    "       f.write(\"%s\\n\" % item)\n",
    "\n",
    "with open(\"QL/QL_total_rewards_2act_N0_5.txt\", \"w\") as f:\n",
    "   for item in total_rewards_2act_N0_5:\n",
    "       f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kK2FHrurwfKQ"
   },
   "outputs": [],
   "source": [
    "# with open(\"QL/QL_scores_2act_N0_5.txt\") as f:\n",
    "#    scores_2act_N0_5 = [int(x) for x in  f.read().splitlines()]\n",
    "\n",
    "# with open(\"QL/QL_total_rewards_2act_N0_5.txt\") as f:\n",
    "#    total_rewards_2act_N0_5 = [int(x) for x in  f.read().splitlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GCwyaMkSwfKR"
   },
   "source": [
    "### - Results with different $N0$ values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_plots.plot_3scores(scores_2act_N0_0d, scores_2act, scores_2act_N0_5, \"N0=0.0\", \"N0=2.5\", \"N0=5.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KTQQO9mXwfKR"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U4zV0AFhwfKR"
   },
   "source": [
    "## Changing hyper parameters: Discount factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jnSRP0KCwfKR"
   },
   "source": [
    "### - $ɣ$ = 0.90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8J2K1rdewfKS"
   },
   "outputs": [],
   "source": [
    "GAMMA = 0.90\n",
    "AVAILABLE_ACTIONS = 2\n",
    "N0 =  2.5\n",
    "\n",
    "reward_policy.REWARD_IF_CROSS = 500\n",
    "reward_policy.REWARD_IF_COLISION = -10\n",
    "reward_policy.REWARD_IF_STILL = -1\n",
    "reward_policy.REWARD_IF_FW = 0 \n",
    "\n",
    "n_runs = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YnlQuIsOwfKS"
   },
   "outputs": [],
   "source": [
    "env, initial_state = environment.get_env()\n",
    "agent = agents.QLearning(gamma=GAMMA, available_actions=AVAILABLE_ACTIONS, N0=N0)\n",
    "\n",
    "scores_2act_gamma_0p9 = []\n",
    "total_rewards_2act_gamma_0p9 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fCXZBfgPwfKS",
    "outputId": "663c60af-dd12-4d6c-8883-4333e96a8ae0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run [   0] - Total reward: 3505.00 Mean scores: 11.00 Means Scores[:-10]: 11.00 Score: 11 \n",
      "CPU times: user 2min 7s, sys: 124 ms, total: 2min 7s\n",
      "Wall time: 2min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for i in range(n_runs):\n",
    "    #render = i % 100 == 0\n",
    "    render = 0\n",
    "\n",
    "    game_over = False\n",
    "    state = env.reset()\n",
    "    state = reduce_state(state)[RAM_mask].data.tobytes()  # Select useful bytes\n",
    "    action = agent.act(state)\n",
    "   \n",
    "    score = 0\n",
    "    total_reward = 0\n",
    "\n",
    "    while not game_over:\n",
    "        if render:\n",
    "            time.sleep(0.015)\n",
    "            env.render()\n",
    "\n",
    "        old_state = state\n",
    "        ob, reward, game_over, _ = env.step(action)\n",
    "\n",
    "        ob = reduce_state(ob)\n",
    "        reward = reward_policy(reward, ob, action)\n",
    "\n",
    "        total_reward += reward\n",
    "\n",
    "        if reward == reward_policy.REWARD_IF_CROSS:\n",
    "            score += 1\n",
    "\n",
    "        state = ob[RAM_mask].data.tobytes()\n",
    "\n",
    "        agent.update_Q(old_state, state, action, reward)\n",
    "\n",
    "        action = agent.act(state)  # Next action\n",
    "\n",
    "    scores_2act_gamma_0p9.append(score)\n",
    "    total_rewards_2act_gamma_0p9.append(total_reward)\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print_result(i, scores_2act_gamma_0p9, total_reward, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6UPEWByLwfKT"
   },
   "outputs": [],
   "source": [
    "with open(\"QL/QL_scores_2act_gamma_0p9.txt\", \"w\") as f:\n",
    "   for item in scores_2act_gamma_0p9:\n",
    "       f.write(\"%s\\n\" % item)\n",
    "\n",
    "with open(\"QL/QL_total_rewards_2act_gamma_0p9.txt\", \"w\") as f:\n",
    "   for item in total_rewards_2act_gamma_0p9:\n",
    "       f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QubI_Om_wfKT"
   },
   "outputs": [],
   "source": [
    "# with open(\"QL/QL_scores_2act_gamma_0p9.txt\") as f:\n",
    "#    scores_2act_gamma_0p9 = [int(x) for x in  f.read().splitlines()]\n",
    "\n",
    "# with open(\"QL/QL_total_rewards_2act_gamma_0p9.txt\") as f:\n",
    "#    total_rewards_2act_gamma_0p9 = [int(x) for x in  f.read().splitlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "35Ex64oZwfKT"
   },
   "source": [
    "### - $ɣ$ = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oSYzgEvvwfKT"
   },
   "outputs": [],
   "source": [
    "GAMMA = 0.75\n",
    "AVAILABLE_ACTIONS = 2\n",
    "N0 =  2.5\n",
    "\n",
    "reward_policy.REWARD_IF_CROSS = 500\n",
    "reward_policy.REWARD_IF_COLISION = -10\n",
    "reward_policy.REWARD_IF_STILL = -1\n",
    "reward_policy.REWARD_IF_FW = 0 \n",
    "\n",
    "n_runs = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UOqaU7_hwfKT"
   },
   "outputs": [],
   "source": [
    "env, initial_state = environment.get_env()\n",
    "agent = agents.QLearning(gamma=GAMMA, available_actions=AVAILABLE_ACTIONS, N0=N0)\n",
    "\n",
    "scores_2act_gamma_0p75 = []\n",
    "total_rewards_2act_gamma_0p75 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hNYqICX7wfKU",
    "outputId": "99ed35f0-a3d7-42b8-bcb8-2d4a41d68673"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run [   0] - Total reward: 4733.00 Mean scores: 13.00 Means Scores[:-10]: 13.00 Score: 13 \n",
      "CPU times: user 2min 4s, sys: 104 ms, total: 2min 4s\n",
      "Wall time: 2min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for i in range(n_runs):\n",
    "    #render = i % 100 == 0\n",
    "    render = 0\n",
    "\n",
    "    game_over = False\n",
    "    state = env.reset()\n",
    "    state = reduce_state(state)[RAM_mask].data.tobytes()  # Select useful bytes\n",
    "    action = agent.act(state)\n",
    "   \n",
    "    score = 0\n",
    "    total_reward = 0\n",
    "\n",
    "    while not game_over:\n",
    "        if render:\n",
    "            time.sleep(0.015)\n",
    "            env.render()\n",
    "\n",
    "        old_state = state\n",
    "        ob, reward, game_over, _ = env.step(action)\n",
    "\n",
    "        ob = reduce_state(ob)\n",
    "        reward = reward_policy(reward, ob, action)\n",
    "\n",
    "        total_reward += reward\n",
    "\n",
    "        if reward == reward_policy.REWARD_IF_CROSS:\n",
    "            score += 1\n",
    "\n",
    "        state = ob[RAM_mask].data.tobytes()\n",
    "\n",
    "        agent.update_Q(old_state, state, action, reward)\n",
    "\n",
    "        action = agent.act(state)  # Next action\n",
    "\n",
    "    scores_2act_gamma_0p75.append(score)\n",
    "    total_rewards_2act_gamma_0p75.append(total_reward)\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print_result(i, scores_2act_gamma_0p75, total_reward, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rLr3Tw5ZwfKU"
   },
   "outputs": [],
   "source": [
    "with open(\"QL/QL_scores_2act_gamma_0p75.txt\", \"w\") as f:\n",
    "   for item in scores_2act_gamma_0p75:\n",
    "       f.write(\"%s\\n\" % item)\n",
    "\n",
    "with open(\"QL/QL_total_rewards_2act_gamma_0p75.txt\", \"w\") as f:\n",
    "   for item in total_rewards_2act_gamma_0p75:\n",
    "       f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cW3XnlmKwfKU"
   },
   "outputs": [],
   "source": [
    "# with open(\"QL/QL_scores_2act_gamma_0p75.txt\") as f:\n",
    "#    scores_2act_gamma_0p75 = [int(x) for x in  f.read().splitlines()]\n",
    "\n",
    "# with open(\"QL/QL_total_rewards_2act_gamma_0p75.txt\") as f:\n",
    "#    total_rewards_2act_gamma_0p75 = [int(x) for x in  f.read().splitlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_nUDEzUJwfKU"
   },
   "source": [
    "### - Results with different $ɣ$ values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_plots.plot_3scores(scores_2act_gamma_0p9[:3000], scores_2act[:3000], scores_2act_gamma_0p75[:3000], \"gamma=0.90\", \"gamma=0.99\", \"gamma=0.75\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ynnoPpgwfKV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8kdXnFN8wfKV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XR3Y2nRbwfKV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kxBVRawTwfKW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OSiuKt_owfKW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zQ9IfPeLwfKW"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z6_qmgYBwfKX"
   },
   "source": [
    "# Monte Carlo Control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BHaOs0-xx9La"
   },
   "source": [
    "## Changing hyper parameters: number of actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wxhG6Kvgx9La"
   },
   "source": [
    "### - 2 actions (move forward or stay):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "X1NjH1vox9Lb"
   },
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "AVAILABLE_ACTIONS = 2\n",
    "N0 = 2.5\n",
    "\n",
    "reward_policy.REWARD_IF_CROSS = 500\n",
    "reward_policy.REWARD_IF_COLISION = -10\n",
    "reward_policy.REWARD_IF_STILL = -1\n",
    "reward_policy.REWARD_IF_FW = 0 \n",
    "\n",
    "n_runs = 7000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "env, initial_state = environment.get_env()\n",
    "agent = agents.MonteCarloControl(gamma=GAMMA, available_actions=AVAILABLE_ACTIONS, N0=N0)\n",
    "\n",
    "def MonteCarloES(RAM_mask: List[int], render: bool=False):\n",
    "    epi = episode.generate_episode(env, reduce_state=reduce_state, reward_policy=reward_policy, agent=agent, RAM_mask=RAM_mask, render=render)\n",
    "    return agent.update_policy(epi)\n",
    "\n",
    "scores_2act = []\n",
    "total_rewards_2act = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.74 s, sys: 4 ms, total: 3.75 s\n",
      "Wall time: 3.74 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(13, 4551)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "MonteCarloES(RAM_mask=RAM_mask, render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run [   0] - Total reward: 5797.00 Mean scores: 15.00 Means Scores[:-10]: 15.00 Score: 15 \n",
      "CPU times: user 2min 6s, sys: 76 ms, total: 2min 6s\n",
      "Wall time: 2min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for i in range(n_runs):\n",
    "    #render = i % 100 == 0\n",
    "    render = 0\n",
    "    \n",
    "    score, total_reward = MonteCarloES(RAM_mask=RAM_mask, render=render)\n",
    "\n",
    "    scores_2act.append(score)\n",
    "    total_rewards_2act.append(total_reward)\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print_result(i, scores_2act, total_reward, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "uZwUTnQ2x9Ld"
   },
   "outputs": [],
   "source": [
    "with open(\"MC/MC_scores_2act.txt\", \"w\") as f:\n",
    "   for item in scores_2act:\n",
    "       f.write(\"%s\\n\" % item)\n",
    "\n",
    "with open(\"MC/MC_total_rewards_2act.txt\", \"w\") as f:\n",
    "   for item in total_rewards_2act:\n",
    "       f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D7y-lpRUx9Ld"
   },
   "outputs": [],
   "source": [
    "# with open(\"MC/MC_scores_2act.txt\") as f:\n",
    "#    scores_2act = [int(x) for x in  f.read().splitlines()]\n",
    "\n",
    "# with open(\"MC/MC_total_rewards_2act.txt\") as f:\n",
    "#    total_rewards_2act = [int(x) for x in  f.read().splitlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xIAuOpGWx9Lg"
   },
   "source": [
    "### - 3 actions (move forward, stay or move backard):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "vFNzJurqx9Lh"
   },
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "AVAILABLE_ACTIONS = 3\n",
    "N0 = 2.5\n",
    "\n",
    "reward_policy.REWARD_IF_CROSS = 500\n",
    "reward_policy.REWARD_IF_COLISION = -10\n",
    "reward_policy.REWARD_IF_STILL = -1\n",
    "reward_policy.REWARD_IF_FW = 0 \n",
    "reward_policy.REWARD_IF_BW = -9\n",
    "\n",
    "n_runs = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "6A_mSKFax9Li"
   },
   "outputs": [],
   "source": [
    "env, initial_state = environment.get_env()\n",
    "agent = agents.MonteCarloControl(gamma=GAMMA, available_actions=AVAILABLE_ACTIONS, N0=N0)\n",
    "\n",
    "def MonteCarloES(RAM_mask: List[int], render: bool=False):\n",
    "    epi = episode.generate_episode(env, reduce_state=reduce_state, reward_policy=reward_policy, agent=agent, RAM_mask=RAM_mask, render=render)\n",
    "    return agent.update_policy(epi)\n",
    "\n",
    "scores_3act = []\n",
    "total_rewards_3act = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run [   0] - Total reward: -9550.00 Mean scores: 0.00 Means Scores[:-10]:  0.00 Score:  0 \n",
      "CPU times: user 2min, sys: 27.9 ms, total: 2min\n",
      "Wall time: 2min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for i in range(n_runs):\n",
    "    #render = i % 100 == 0\n",
    "    render = 0\n",
    "    \n",
    "    score, total_reward = MonteCarloES(RAM_mask=RAM_mask, render=render)\n",
    "\n",
    "    scores_3act.append(score)\n",
    "    total_rewards_3act.append(total_reward)\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print_result(i, scores_3act, total_reward, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "dK9KXr3bx9Lj"
   },
   "outputs": [],
   "source": [
    "with open(\"MC/MC_scores_3act.txt\", \"w\") as f:\n",
    "   for item in scores_3act:\n",
    "       f.write(\"%s\\n\" % item)\n",
    "\n",
    "with open(\"MC/MC_total_rewards_3act.txt\", \"w\") as f:\n",
    "   for item in total_rewards_3act:\n",
    "       f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "86Xnxff2x9Lj"
   },
   "outputs": [],
   "source": [
    "# with open(\"MC/MC_scores_3act.txt\") as f:\n",
    "#    scores_3act = [int(x) for x in  f.read().splitlines()]\n",
    "\n",
    "# with open(\"MC/MC_total_rewards_3act.txt\") as f:\n",
    "#    total_rewards_3act = [int(x) for x in  f.read().splitlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4YQ8Uhcux9Lk"
   },
   "source": [
    "### - Results with different number of actions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = [baseline_mean_score for i in range(3000)]\n",
    "aux_plots.plot_3scores(scores_3act[:3000], scores_2act[:3000], baseline[:3000], \"3 actions (BW, FW or STAY)\", \"2 actions (FW or STAY)\", \"Baseline mean score (FW only)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qFE40zGhx9Ll"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KUX1Bnz1x9Ll"
   },
   "source": [
    "## Changing hyper parameters: Reward values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K0LJw6kUx9Lm"
   },
   "source": [
    "### - Sparse reward: $+1$ if cross the street, $-1$ if collide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D_66YFEDx9Lm"
   },
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "AVAILABLE_ACTIONS = 2\n",
    "N0 = 2.5\n",
    "\n",
    "reward_policy.REWARD_IF_CROSS = 1\n",
    "reward_policy.REWARD_IF_COLISION = -1\n",
    "reward_policy.REWARD_IF_STILL = 0\n",
    "reward_policy.REWARD_IF_FW = 0 \n",
    "\n",
    "n_runs = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "unNawPF4x9Lm"
   },
   "outputs": [],
   "source": [
    "env, initial_state = environment.get_env()\n",
    "agent = agents.MonteCarloControl(gamma=GAMMA, available_actions=AVAILABLE_ACTIONS, N0=N0)\n",
    "\n",
    "def MonteCarloES(RAM_mask: List[int], render: bool=False):\n",
    "    epi = episode.generate_episode(env, reduce_state=reduce_state, reward_policy=reward_policy, agent=agent, RAM_mask=RAM_mask, render=render)\n",
    "    return agent.update_policy(epi)\n",
    "\n",
    "\n",
    "scores_2act_sparseR = []\n",
    "total_rewards_2act_sparseR = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for i in range(n_runs):\n",
    "    #render = i % 100 == 0\n",
    "    render = 0\n",
    "    \n",
    "    score, total_reward = MonteCarloES(RAM_mask=RAM_mask, render=render)\n",
    "\n",
    "    scores_2act_sparseR.append(score)\n",
    "    total_rewards_2act_sparseR.append(total_reward)\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print_result(i, scores_2act_sparseR, total_reward, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VjifchMux9Ln"
   },
   "outputs": [],
   "source": [
    "with open(\"MC/MC_scores_2act_sparseR.txt\", \"w\") as f:\n",
    "   for item in scores_2act_sparseR:\n",
    "       f.write(\"%s\\n\" % item)\n",
    "\n",
    "with open(\"MC/MC_total_rewards_2act_sparseR.txt\", \"w\") as f:\n",
    "   for item in total_rewards_2act_sparseR:\n",
    "       f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G1M3pQ1vx9Lo"
   },
   "outputs": [],
   "source": [
    "# with open(\"MC/MC_scores_2act_sparseR.txt\") as f:\n",
    "#    scores_2act_sparseR = [int(x) for x in  f.read().splitlines()]\n",
    "\n",
    "# with open(\"MC/MC_total_rewards_2act_sparseR.txt\") as f:\n",
    "#    total_rewards_2act_sparseR = [int(x) for x in  f.read().splitlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NqRunVwXx9Lo"
   },
   "source": [
    "### - Results with sparse and dense rewards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = [baseline_mean_score for i in range(3000)]\n",
    "aux_plots.plot_3scores(scores_2act_sparseR[:3000], scores_2act[:3000], baseline[:3000], \"Sparse Reward\", \"Dense Reward\", \"Baseline mean score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q37kgfgfx9Lp"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wDjX3Uhqx9Lp"
   },
   "source": [
    "## Changing hyper parameters: Learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lhDd8stvx9Lp"
   },
   "source": [
    "### - $N0$ = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QvEu1iuEx9Lp"
   },
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "AVAILABLE_ACTIONS = 2\n",
    "N0 =  0.001\n",
    "\n",
    "reward_policy.REWARD_IF_CROSS = 500\n",
    "reward_policy.REWARD_IF_COLISION = -10\n",
    "reward_policy.REWARD_IF_STILL = -1\n",
    "reward_policy.REWARD_IF_FW = 0 \n",
    "\n",
    "n_runs = 7000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H3FBcKnix9Lp"
   },
   "outputs": [],
   "source": [
    "env, initial_state = environment.get_env()\n",
    "agent = agents.MonteCarloControl(gamma=GAMMA, available_actions=AVAILABLE_ACTIONS, N0=N0)\n",
    "\n",
    "def MonteCarloES(RAM_mask: List[int], render: bool=False):\n",
    "    epi = episode.generate_episode(env, reduce_state=reduce_state, reward_policy=reward_policy, agent=agent, RAM_mask=RAM_mask, render=render)\n",
    "    return agent.update_policy(epi)\n",
    "\n",
    "scores_2act_N0_0 = []\n",
    "total_rewards_2act_N0_0 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for i in range(n_runs):\n",
    "    #render = i % 100 == 0\n",
    "    render = 0\n",
    "    \n",
    "    score, total_reward = MonteCarloES(RAM_mask=RAM_mask, render=render)\n",
    "\n",
    "    scores_2act_N0_0.append(score)\n",
    "    total_rewards_2act_N0_0.append(total_reward)\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print_result(i, scores_2act_N0_0, total_reward, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dPopd9jlx9Lq"
   },
   "outputs": [],
   "source": [
    "with open(\"MC/MC_scores_2act_N0_0.txt\", \"w\") as f:\n",
    "   for item in scores_2act_N0_0:\n",
    "       f.write(\"%s\\n\" % item)\n",
    "\n",
    "with open(\"MC/MC_total_rewards_2act_N0_0.txt\", \"w\") as f:\n",
    "   for item in total_rewards_2act_N0_0:\n",
    "       f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FpYuloURx9Lq"
   },
   "outputs": [],
   "source": [
    "# with open(\"MC/MC_scores_2act_N0_0.txt\") as f:\n",
    "#    scores_2act_N0_0 = [int(x) for x in  f.read().splitlines()]\n",
    "\n",
    "# with open(\"MC/MC_total_rewards_2act_N0_0.txt\") as f:\n",
    "#    total_rewards_2act_N0_0 = [int(x) for x in  f.read().splitlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0UbSCG0Px9Lr"
   },
   "source": [
    "### - $N0$ = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QIeXlZysx9Lr"
   },
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "AVAILABLE_ACTIONS = 2\n",
    "N0 =  5\n",
    "\n",
    "reward_policy.REWARD_IF_CROSS = 500\n",
    "reward_policy.REWARD_IF_COLISION = -10\n",
    "reward_policy.REWARD_IF_STILL = -1\n",
    "reward_policy.REWARD_IF_FW = 0 \n",
    "\n",
    "n_runs = 7000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S2FDqiiBx9Lr"
   },
   "outputs": [],
   "source": [
    "env, initial_state = environment.get_env()\n",
    "agent = agents.MonteCarloControl(gamma=GAMMA, available_actions=AVAILABLE_ACTIONS, N0=N0)\n",
    "\n",
    "def MonteCarloES(RAM_mask: List[int], render: bool=False):\n",
    "    epi = episode.generate_episode(env, reduce_state=reduce_state, reward_policy=reward_policy, agent=agent, RAM_mask=RAM_mask, render=render)\n",
    "    return agent.update_policy(epi)\n",
    "\n",
    "scores_2act_N0_5 = []\n",
    "total_rewards_2act_N0_5 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for i in range(n_runs):\n",
    "    #render = i % 100 == 0\n",
    "    render = 0\n",
    "    \n",
    "    score, total_reward = MonteCarloES(RAM_mask=RAM_mask, render=render)\n",
    "\n",
    "    scores_2act_N0_5.append(score)\n",
    "    total_rewards_2act_N0_5.append(total_reward)\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print_result(i, scores_2act_N0_5, total_reward, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-XudWve7x9Ls"
   },
   "outputs": [],
   "source": [
    "with open(\"MC/MC_scores_2act_N0_5.txt\", \"w\") as f:\n",
    "   for item in scores_2act_N0_5:\n",
    "       f.write(\"%s\\n\" % item)\n",
    "\n",
    "with open(\"MC/MC_total_rewards_2act_N0_5.txt\", \"w\") as f:\n",
    "   for item in total_rewards_2act_N0_5:\n",
    "       f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UFPBgUoJx9Ls"
   },
   "outputs": [],
   "source": [
    "# with open(\"MC/MC_scores_2act_N0_5.txt\") as f:\n",
    "#    scores_2act_N0_5 = [int(x) for x in  f.read().splitlines()]\n",
    "\n",
    "# with open(\"MC/MC_total_rewards_2act_N0_5.txt\") as f:\n",
    "#    total_rewards_2act_N0_5 = [int(x) for x in  f.read().splitlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tse0MWz9x9Lt"
   },
   "source": [
    "### - Results with different $N0$ values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_plots.plot_3scores(scores_2act_N0_0, scores_2act, scores_2act_N0_5, \"N0=0.0\", \"N0=2.5\", \"N0=5.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5KIsXgcAx9Lt"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xjXbOOO_x9Lt"
   },
   "source": [
    "## Changing hyper parameters: Discount factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "im6EopUvx9Lu"
   },
   "source": [
    "### - $ɣ$ = 0.90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "52oDh-3Ex9Lu"
   },
   "outputs": [],
   "source": [
    "GAMMA = 0.90\n",
    "AVAILABLE_ACTIONS = 2\n",
    "N0 =  2.5\n",
    "\n",
    "reward_policy.REWARD_IF_CROSS = 500\n",
    "reward_policy.REWARD_IF_COLISION = -10\n",
    "reward_policy.REWARD_IF_STILL = -1\n",
    "reward_policy.REWARD_IF_FW = 0 \n",
    "\n",
    "n_runs = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V3C772fwx9Lv"
   },
   "outputs": [],
   "source": [
    "env, initial_state = environment.get_env()\n",
    "agent = agents.MonteCarloControl(gamma=GAMMA, available_actions=AVAILABLE_ACTIONS, N0=N0)\n",
    "\n",
    "def MonteCarloES(RAM_mask: List[int], render: bool=False):\n",
    "    epi = episode.generate_episode(env, reduce_state=reduce_state, reward_policy=reward_policy, agent=agent, RAM_mask=RAM_mask, render=render)\n",
    "    return agent.update_policy(epi)\n",
    "\n",
    "scores_2act_gamma_0p9 = []\n",
    "total_rewards_2act_gamma_0p9 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for i in range(n_runs):\n",
    "    #render = i % 100 == 0\n",
    "    render = 0\n",
    "    \n",
    "    score, total_reward = MonteCarloES(RAM_mask=RAM_mask, render=render)\n",
    "\n",
    "    scores_2act_gamma_0p9.append(score)\n",
    "    total_rewards_2act_gamma_0p9.append(total_reward)\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print_result(i, scores_2act_gamma_0p9, total_reward, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q1yrpw-bx9Lw"
   },
   "outputs": [],
   "source": [
    "with open(\"MC/MC_scores_2act_gamma_0p9.txt\", \"w\") as f:\n",
    "   for item in scores_2act_gamma_0p9:\n",
    "       f.write(\"%s\\n\" % item)\n",
    "\n",
    "with open(\"MC/MC_total_rewards_2act_gamma_0p9.txt\", \"w\") as f:\n",
    "   for item in total_rewards_2act_gamma_0p9:\n",
    "       f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gf6bWSJdx9Lw"
   },
   "outputs": [],
   "source": [
    "# with open(\"MC/MC_scores_2act_gamma_0p9.txt\") as f:\n",
    "#    scores_2act_gamma_0p9 = [int(x) for x in  f.read().splitlines()]\n",
    "\n",
    "# with open(\"MC/MC_total_rewards_2act_gamma_0p9.txt\") as f:\n",
    "#    total_rewards_2act_gamma_0p9 = [int(x) for x in  f.read().splitlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oyL7aU51x9Lw"
   },
   "source": [
    "### - $ɣ$ = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nj5BGvLPx9Lx"
   },
   "outputs": [],
   "source": [
    "GAMMA = 0.75\n",
    "AVAILABLE_ACTIONS = 2\n",
    "N0 =  2.5\n",
    "\n",
    "reward_policy.REWARD_IF_CROSS = 500\n",
    "reward_policy.REWARD_IF_COLISION = -10\n",
    "reward_policy.REWARD_IF_STILL = -1\n",
    "reward_policy.REWARD_IF_FW = 0 \n",
    "\n",
    "n_runs = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZqPpzYffx9Lx"
   },
   "outputs": [],
   "source": [
    "env, initial_state = environment.get_env()\n",
    "agent = agents.MonteCarloControl(gamma=GAMMA, available_actions=AVAILABLE_ACTIONS, N0=N0)\n",
    "\n",
    "def MonteCarloES(RAM_mask: List[int], render: bool=False):\n",
    "    epi = episode.generate_episode(env, reduce_state=reduce_state, reward_policy=reward_policy, agent=agent, RAM_mask=RAM_mask, render=render)\n",
    "    return agent.update_policy(epi)\n",
    "\n",
    "scores_2act_gamma_0p75 = []\n",
    "total_rewards_2act_gamma_0p75 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for i in range(n_runs):\n",
    "    #render = i % 100 == 0\n",
    "    render = 0\n",
    "    \n",
    "    score, total_reward = MonteCarloES(RAM_mask=RAM_mask, render=render)\n",
    "\n",
    "    scores_2act_gamma_0p75.append(score)\n",
    "    total_rewards_2act_gamma_0p75.append(total_reward)\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print_result(i, scores_2act_gamma_0p75, total_reward, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QchKKxY5x9Lx"
   },
   "outputs": [],
   "source": [
    "with open(\"MC/MC_scores_2act_gamma_0p75.txt\", \"w\") as f:\n",
    "   for item in scores_2act_gamma_0p75:\n",
    "       f.write(\"%s\\n\" % item)\n",
    "\n",
    "with open(\"MC/MC_total_rewards_2act_gamma_0p75.txt\", \"w\") as f:\n",
    "   for item in total_rewards_2act_gamma_0p75:\n",
    "       f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sub1Qv1Gx9Ly"
   },
   "outputs": [],
   "source": [
    "# with open(\"MC/MC_scores_2act_gamma_0p75.txt\") as f:\n",
    "#    scores_2act_gamma_0p75 = [int(x) for x in  f.read().splitlines()]\n",
    "\n",
    "# with open(\"MC/MC_total_rewards_2act_gamma_0p75.txt\") as f:\n",
    "#    total_rewards_2act_gamma_0p75 = [int(x) for x in  f.read().splitlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KTV8jK4dx9Ly"
   },
   "source": [
    "### - Results with different $ɣ$ values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_plots.plot_3scores(scores_2act_gamma_0p9[:3000], scores_2act[:3000], scores_2act_gamma_0p75[:3000], \"gamma=0.90\", \"gamma=0.99\", \"gamma=0.75\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FMgv-zGGx9Ly"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FT8A5H40x9Lz"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "aa_freeway.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
