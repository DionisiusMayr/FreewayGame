{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Freeway"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the second project for the MC935A/MO436A - Reinforcement Learning course, taught by Prof. Esther Colombini.\n",
    "\n",
    "In this project we propose to apply Reinforcement Learning methods to teach an agent how to play the Freeway Atari game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Members of Group 13:**\n",
    "- Aline Gabriel de Almeida\n",
    "- Dionisius Oliveira Mayr (229060)\n",
    "- Marianna de Pinho Severo (264960)\n",
    "- Victor Jesús Sotelo Chico (265173)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Freeway game\n",
    "\n",
    "![Baseline 1](./img/Freeway_logo.png)\n",
    "\n",
    "Freeway is a video game written by David Crane for the Atari 2600 and published by Activision [[1]](https://en.wikipedia.org/wiki/Freeway_(video_game)).\n",
    "\n",
    "In the game, two players compete against each other trying to make their chickens cross the street, while evading the cars passing by.\n",
    "There are three possible actions: staying still, moving forward or moving backward.\n",
    "Each time a chicken collides with a car, it is forced back some spaces and takes a while until the chicken regains its control.\n",
    "\n",
    "When a chicken is successfully guided across the freeway, it is awarded one point and moved to the initial space, where it will try to cross the street again.\n",
    "The game offers multiple scenarios with different vehicles configurations (varying the type, frequency and speed of them) and plays for 2 minutes and 16 seconds.\n",
    "During the 8 last seconds the scores will start blinking to indicate that the game is close to end.\n",
    "Whoever has the most points after this, wins the game!\n",
    "\n",
    "The image was extracted from the [manual of the game](https://www.gamesdatabase.org/Media/SYSTEM/Atari_2600/Manual/formated/Freeway_-_1981_-_Zellers.pdf).\n",
    "\n",
    "[1 - Wikipedia - Freeway](https://en.wikipedia.org/wiki/Freeway_(video_game))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the [OpenAI Gym](https://gym.openai.com/) toolkit.\n",
    "This toolkit uses the [Arcade Learning Environment](https://github.com/mgbellemare/Arcade-Learning-Environment) to simulate the game through the [Stella](https://stella-emu.github.io/) emulator.\n",
    "\n",
    "Although the game offers multiple scenarios, we are going to consider only the first one. Also, we will be controlling a *single chicken*, while we try to maximize its score.\n",
    "\n",
    "In this configuration, there are ten lanes and each lane contains exactly one car (with a different speed and direction).\n",
    "Whenever an action is chosen, it is repeated for $k$ frames, $k \\in \\{2, 3, 4\\}$.\n",
    "\n",
    "This means that our environment is **stochastic** and it is also **episodic**, with its terminal state being reached whenever 2 minutes and 16 seconds have passed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stable Baselines Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the [Stable Baselines](https://stable-baselines.readthedocs.io/en/master/) implementations of the Deep Reinforcement algorithms.\n",
    "\n",
    "It is a fork of the OpenAI Baselines, fully compatible with the OpenAI Gym environments with straightforward usage.\n",
    "\n",
    "Under the hood, it is using the Tensorflow library to implement the neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the dependencies:\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "pip install stable-baselines[mpi]\n",
    "pip install tensorflow==1.15.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can find a list of useful links and materials that were used during this project.\n",
    "\n",
    "* [Freeway-ram-v0 from OpenAI Gym](https://gym.openai.com/envs/Freeway-ram-v0/)\n",
    "* [Manual of the game](https://www.gamesdatabase.org/Media/SYSTEM/Atari_2600/Manual/formated/Freeway_-_1981_-_Zellers.pdf)\n",
    "* [Freeway Benchmarks](https://paperswithcode.com/sota/atari-games-on-atari-2600-freeway)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import gym\n",
    "\n",
    "from stable_baselines.common import make_vec_env\n",
    "from stable_baselines.common.cmd_util import make_atari_env\n",
    "from stable_baselines.common.vec_env import VecFrameStack\n",
    "from stable_baselines.common.atari_wrappers import make_atari\n",
    "from stable_baselines.common.policies import MlpPolicy, CnnPolicy\n",
    "\n",
    "from stable_baselines.deepq.policies import CnnPolicy\n",
    "\n",
    "from stable_baselines import DQN\n",
    "from stable_baselines import PPO2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Action space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we said above, the agent in this game has three possible actions at each frame, each represented by an integer:\n",
    "\n",
    "* 0: Stay\n",
    "* 1: Move forward\n",
    "* 2: Move backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In theory, a perfect chicken wouldn't ever need to move backward, since it is possible to know if moving forward would lead you into a collision (in the immediate frame or in the future frames)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State of the art benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image bellow (extracted from https://paperswithcode.com/sota/atari-games-on-atari-2600-freeway) shows the evolution of the scores over time using different techniques.\n",
    "\n",
    "Today, the state of the art approaches are making 34.0 points, using Deep Reinforcement Learning methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Benchmarks](./img/state_of_art_scores.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Previous Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the previous assignment, we explored how classical Reinforcement Learning tabular methods performed on this game.\n",
    "\n",
    "In some runs, we were able to achieve a peak score of 34 points in a single run using the SARSA($\\lambda$) algorithm, which is the state-of-the-art score for this game; but on average we scored about 31 points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Function Approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Project 1, we also worked with linear function approximators, which were based on the Monte Carlo, Q-Learning and Sarsa($\\lambda$) algorithms. For each of them, we experimented different sets of features to represent the state-action pairs, varied reward functions, different exploration rates, and in all of them we used only two actions (move backward and move forward), which were the best actions found by the tabular methods.\n",
    "\n",
    "All function approximators obtained results close to those of the baseline, being slightly better. Of them, the Monte Carlo based approximator was the only one that managed to improve in relation to its tabular version, reaching an average score of 22.2, compared to the 13 points obtained by the latter.\n",
    "\n",
    "Throughout the experiments, we observed that some linear approximators can be faster than the original algorithms, as happened with Sarsa($\\lambda$). In addition, we saw that the Q-Learning and Sarsa($\\lambda$) approximators achieved better results when they were given greater exploration capacity, and that the Monte Carlo based approximator benefited when less sparse feature vectors were adopted.\n",
    "\n",
    "Given that, although linear function approximators are powerful tools, they were not good enough to improve the performance of our solutions, either due to the nature of the problem, the possible poor quality of the features we created or other factors that were not addressed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reward Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the base environment we are awarded on point each time we successfully cross the freeway."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of parameters to be tuned in our algorithms.\n",
    "However, we don't have the required computational power to properly optimize them all.\n",
    "\n",
    "Since we still want to at least experiment them to observe their impact on the obtained reward, we will be training them for 400k iterations, and then we will select a few of them to train for 1M iterations.\n",
    "\n",
    "We are aware that this is far from perfect in the optimization point of view, but this is the best we can do with our available hardware."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be comparing two different algorithms here: DQN and PPO.\n",
    "\n",
    "The idea is to compare an off-policy method (DQN) and an on-policy method (PPO) regarding their stability and number of samples to convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorboard allows us to visualize and compare the evolution of the algorithms being tested.\n",
    "It shows the metrics like the reward over time, loss and advantage, in an easy way.\n",
    "\n",
    "We will be using it to present our results, understand the impact of different parameters and compare our solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frame Stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before diving into the experiments' results, it is worth understanding some nuances of our testing environment.\n",
    "\n",
    "We will be using image representations (frames) of the game as our state.\n",
    "But by doing so, we end up losing some relavant information of the problem, like the direction the cars are moving.\n",
    "In other words, the problem we are trying to solve becomes non-markovian.\n",
    "\n",
    "To solve this, we use the `VecFrameStack` function, that stacks $n$ frames of the game, making our environment become markovian once again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorized Environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Vectorized environments](https://stable-baselines.readthedocs.io/en/master/guide/vec_envs.html) allows us to stack multiple environments, thus permiting us to traing an agent in $n$ environments per step.\n",
    "\n",
    "It is controlled by the `num_env` parameter in the `make_atari_env` function.\n",
    "\n",
    "Although it isn't possible to use vectorized environments with DQN, we will be using it for the PPO2 algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proximal Policy Optimization (PPO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an on-policy algorithm for solving our problem, we chose [Proximal Policy Optimization (PPO)](https://arxiv.org/abs/1707.06347). PPO is a policy optimization method that can be used in environments that work with continuous or discrete action spaces.\n",
    "\n",
    "As with trust region based approaches, such as the TRPO algorithm, it tries to reduce the size of the update that must be applied to a policy, since major updates tend to generate agents with worse performance. However, it seeks to overcome some of the limitations of TRPO  and other methods, being easier to implement and adjust parameters, and having a better sample complexity.\n",
    "\n",
    "Finally, this algorithm has some variants. One of them, which is used in this project, adopts a specialized clipping function to calculate the objective function, as shown in the equation below. For the experiments, we used the [PPO2](https://stable-baselines.readthedocs.io/en/master/modules/ppo2.html) implementation, from the Stable Baselines library.\n",
    "\n",
    "$$ L^{CLIP}(\\theta) = \\hat{\\mathbb{E}}_t[min(r_t(\\theta)\\hat{A}_t, clip(r_t(\\theta), 1-\\epsilon, 1+\\epsilon)\\hat{A}_t)] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To assess the performance of the PPO in solving our problem, we decided to vary some of its parameters, which were: the discount factor ($\\gamma$), the learning rate, the trade-off factor between bias and variance (lam), the number of frames in the input stack of the algorithm, the type of representation of the states (image or RAM), the number of environments and the total timesteps of training.\n",
    "\n",
    "For this, we created a baseline for comparison, with the following configurations:\n",
    "\n",
    "- Policy network: CnnPolicy\n",
    "- Discount factor ($\\gamma$): 0.99.\n",
    "- Learning rate: 0.00025.\n",
    "- Trade-off factor (lam): 0.95.\n",
    "- Number of frames in the stack: 4.\n",
    "- Type of representation: image.\n",
    "- Number of environments: 4.\n",
    "- Training timesteps: 400K.\n",
    "\n",
    "Below, we describe the experiments and results obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_TIMESTEPS = 400000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(tb_lob, gamma, learning_rate, lam, policy, timesteps=TOTAL_TIMESTEPS, n_stack=4, n_env=8, state_repr='frames'):\n",
    "    # multiprocess environment\n",
    "    if state_repr == 'ram':\n",
    "        env = make_atari('Freeway-ramNoFrameskip-v0')\n",
    "    else:\n",
    "        env = make_atari_env('FreewayNoFrameskip-v0', num_env=n_env, seed=0)\n",
    "    # Frame-stacking with 4 frames\n",
    "    env = VecFrameStack(env, n_stack=n_stack)\n",
    "\n",
    "    model = PPO2(policy, env, verbose=0, tensorboard_log=tb_lob, gamma=gamma, learning_rate=learning_rate, lam=lam)\n",
    "    model.learn(total_timesteps=timesteps)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 0.00025\n",
    "LAM = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment('Experiments_PPO_A', gamma=GAMMA, learning_rate=LEARNING_RATE, lam=LAM, policy=MlpPolicy);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment('Experiments_PPO_A', gamma=GAMMA, learning_rate=0.00025 * 0.25, lam=LAM, policy=MlpPolicy);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment('Experiments_PPO_A', gamma=GAMMA, learning_rate=0.00025 * 0.5, lam=LAM, policy=MlpPolicy);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment('Experiments_PPO_A', gamma=GAMMA, learning_rate=0.00025 * 2, lam=LAM, policy=MlpPolicy);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment('Experiments_PPO_A', gamma=GAMMA, learning_rate=0.00025 * 4, lam=LAM, policy=MlpPolicy);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Parameter|A1|A2|A3|A4|A5|\n",
    "|-|-|-|-|-|-|\n",
    "|GAMMA|0.99|0.99|0.99|0.99|0.99|\n",
    "|LEARNING_RATE|$$2.5 \\cdot 10^{-4}$$|$$6.25 \\cdot 10^{-5}$$|$$1.25 \\cdot 10^{-4}$$|$$5 \\cdot 10^{-4}$$|$$1 \\cdot 10^{-3}$$|\n",
    "|LAM|0.95|0.95|0.95|0.95|0.95|\n",
    "|Smoothed Reward|21.49|21.41|0.0|21.21|21.17|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|![](./img/alpha_complete.png)|\n",
    "|-| "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|LEARNING_RATE = $$2.5 \\cdot 10^{-4}$$|LEARNING_RATE = $$6.25 \\cdot 10^{-5}$$|LEARNING_RATE = $$1.25 \\cdot 10^{-4}$$|\n",
    "|-|-|-|\n",
    "|![](./img/alpha_2.5-4.png)|![](./img/alpha_6.25-5.png)|![](./img/alpha_1.25-4.png)|\n",
    "\n",
    "|LEARNING_RATE = $$5 \\cdot 10^{-4}$$|LEARNING_RATE = $$1 \\cdot 10^{-3}$$|\n",
    "|-|-|\n",
    "|![](./img/alpha_5-4.png)|![](./img/alpha_1-3.png)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing that catches our attention in these experiments in the one with Learning Rate $1.25 \\cdot 10^{-4}$, where it didn't score a single point.\n",
    "\n",
    "Comparing it with a smaller Learning Rate ($6.25 \\cdot 10^{-5}$) and with a higher Learning Rate ($5 \\cdot 10^{-4}$), we can't see a clear relationship between the performance of the chicken and the Learning Rate used.\n",
    "Thus, we came up with the hypothesis that this odd experiment is due to the random aspect of the algorithm, as we will show in the next session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variability of experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will be exploring how unstable a run of the algorithm is by repeating it multiple times with the same parameter settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to Nikishin et al \\[1\\], Deep Reinforcement Learning methods are notoriously unstable during training and isn't guaranteed to monotonically increase during training.\n",
    "We observed this exact behavior when training our agents, were between different runs with the same parameters, the results would vary a lot.\n",
    "\n",
    "You can find a comparison of three different runs of the PPO2 with $\\gamma$ = 0.95, learning rate = 0.00025 and lam = 0.95 bellow.\n",
    "\n",
    "\\[1\\] - [Improving Stability in Deep Reinforcement Learning with Weight Averaging](https://www.gatsby.ucl.ac.uk/~balaji/udl-camera-ready/UDL-24.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 0.00025\n",
    "LAM = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment('Experiments_PPO_Var', gamma=GAMMA, learning_rate=LEARNING_RATE, lam=LAM, policy=MlpPolicy);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment('Experiments_PPO_Var', gamma=GAMMA, learning_rate=LEARNING_RATE, lam=LAM, policy=MlpPolicy);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment('Experiments_PPO_Var', gamma=GAMMA, learning_rate=LEARNING_RATE, lam=LAM, policy=MlpPolicy);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Parameter|R1|R2|R3|\n",
    "|-|-|-|-|\n",
    "|GAMMA|0.99|0.99|0.99|\n",
    "|LEARNING_RATE|0.00025|0.00025|0.00025|\n",
    "|LAM|0.95|0.95|0.95|\n",
    "|Smoothed Reward|22.68|21.75|21.75|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|![](./img/var_complete.png)|\n",
    "|-|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|R1|R2|R3|\n",
    "|-|-|-|\n",
    "|![](./img/var_1.png)|![](./img/var_2.png)|![](./img/var_3.png)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the graphs above, we can see that the first iteration where the agents starts to score some points can go be ~20k, ~140k or ~70k, purely based on random factors.\n",
    "\n",
    "This is a big problem we are aware and it impacts our results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Influence of discount factor with image representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the table below, we can see the settings of the experiments carried out in this section, as well as the results achieved after being smoothed by a factor of 0,999. In addition, in the following graphs, we observe how the reward obtained by agents varies according to the value of the discount factor ($\\gamma$) and the number of timesteps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.00025\n",
    "LAM = 0.95\n",
    "N_STACK = 4\n",
    "N_ENV = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment('Experiments_PPO_gamma_75', gamma=0.75, learning_rate=LEARNING_RATE, lam=LAM, policy=CnnPolicy,\n",
    "           n_stack=N_STACK, n_env=N_ENV);\n",
    "experiment('Experiments_PPO_gamma_90', gamma=0.90, learning_rate=LEARNING_RATE, lam=LAM, policy=CnnPolicy,\n",
    "           n_stack=N_STACK, n_env=N_ENV);\n",
    "experiment('Experiments_PPO_baseline', gamma=0.99, learning_rate=LEARNING_RATE, lam=LAM, policy=CnnPolicy,\n",
    "           n_stack=N_STACK, n_env=N_ENV);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Parameter**  | **Exp 1** | **Exp 2** | **Exp 3** |\n",
    "|----------------|-----------|-----------|-----------|\n",
    "| **Gamma**      | 0,75      | 0,90      | 0,99      |\n",
    "| Learning Rate  | 0,00025   | 0,00025   | 0,00025   |\n",
    "| Lam            | 0,95      | 0,95      | 0,95      |\n",
    "| Stacks         | 4         | 4         | 4         |\n",
    "| Representation | image     | image     | image     |\n",
    "| Environments   | 4         | 4         | 4         |\n",
    "| Policy         | CnnPolicy | CnnPolicy | CnnPolicy |\n",
    "| Smothed reward | 9,55      | 15,74     | **20,04**     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| |\n",
    "|-|\n",
    "|<img src=https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/bae41b89189519f64aa78d12693f800f5d62e51c/marianna/project02-rl/01-figures-experiments/01-images-gamma/svg/image_gamma_all.svg width=\"400\">|\n",
    "\n",
    "| $\\gamma$=0.75 | $\\gamma$=0.90 | $\\gamma$=0.99 |  \n",
    "|---|---|---|  \n",
    "| <img src=https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/bae41b89189519f64aa78d12693f800f5d62e51c/marianna/project02-rl/01-figures-experiments/01-images-gamma/svg/image_gamma_75.svg width=\"250\"> | <img src =https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/bae41b89189519f64aa78d12693f800f5d62e51c/marianna/project02-rl/01-figures-experiments/01-images-gamma/svg/image_gamma_90.svg width=\"250\"> | <img src=https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/bae41b89189519f64aa78d12693f800f5d62e51c/marianna/project02-rl/01-figures-experiments/01-images-gamma/svg/image_gamma_baseline.svg width=\"250\"> |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that in all the graphics, there is a considerable variation in the reward and that the curve that proves to be more stable is the one constructed with $ \\gamma = 0,99 $ (baseline). On the other hand, we noticed that the other curves decrease a lot from certain timesteps, being the one constructed with $ \\gamma = 0,75 $  the first to decrease.\n",
    "\n",
    "This indicates that, for the problem addressed, a PPO agent with a far-sight view is able to achieve better performances and that, the more limited this view is, the worse their results will be. Finally, although the highest value of the best smoothed curve is the one shown in the table, the values of this curve vary around $22$ points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bellow you can find a gif showing an episode of the agent with $\\gamma = 0.75$ after the 400k timesteps.\n",
    "\n",
    "We can see its performance deterioration, where the agent un-learns how to score points and keeps moving meaninglessly back and forth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| |\n",
    "|-|\n",
    "|<img src=https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/01ad1553f86ea6038e4fa81bf338fc86bc1769b8/marianna/project02-rl/gif/ppo_gamma_75_gif.gif width=\"400\">|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Influence of the number of frames in the stack with image representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the learning algorithms receive images as input, to model the problem as a MDP, we need to group a set of frames (images) in structures called stacks, which are given as input to these algorithms. This allows models to access time information about the game.\n",
    "\n",
    "In order to analyze the influence of the size of the frame stacks on the performance of the created PPO agents, we experimented with the baseline with stacks of size 1, 4, 32 and 64, as described in the table below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.00025\n",
    "LAM = 0.95\n",
    "GAMMA = 0.99\n",
    "N_ENV = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment('Experiments_PPO_stack_1', gamma=GAMMA, learning_rate=LEARNING_RATE, lam=LAM, policy=CnnPolicy,\n",
    "           n_stack=1, n_env=N_ENV);\n",
    "experiment('Experiments_PPO_stack_4', gamma=GAMMA, learning_rate=LEARNING_RATE, lam=LAM, policy=CnnPolicy,\n",
    "           n_stack=4, n_env=N_ENV);\n",
    "experiment('Experiments_PPO_stack_32', gamma=GAMMA, learning_rate=LEARNING_RATE, lam=LAM, policy=CnnPolicy,\n",
    "           n_stack=32, n_env=N_ENV);\n",
    "experiment('Experiments_PPO_stack_64', gamma=GAMMA, learning_rate=LEARNING_RATE, lam=LAM, policy=CnnPolicy,\n",
    "           n_stack=64, n_env=N_ENV);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Parameter**  | **Exp 1** | **Exp 2** | **Exp 3** | **Exp 4** |\n",
    "|----------------|-----------|-----------|-----------|-----------|\n",
    "| Gamma          | 0,99      | 0,99      | 0,99      | 0,99      |\n",
    "| Learning Rate  | 0,00025   | 0,00025   | 0,00025   | 0,00025   |\n",
    "| Lam            | 0,95      | 0,95      | 0,95      | 0,95      |\n",
    "| **Stacks**     | 1         | 4         | 32        | 64        |\n",
    "| Representation | image     | image     | image     | image     |\n",
    "| Environments   | 4         | 4         | 4         | 4         |\n",
    "| Policy         | CnnPolicy | CnnPolicy | CnnPolicy | CnnPolicy |\n",
    "| Smothed reward | **21,16** | 20,04     | 18,23     | 9,72      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| |\n",
    "|-|\n",
    "|<img src=https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/bae41b89189519f64aa78d12693f800f5d62e51c/marianna/project02-rl/01-figures-experiments/02-images-stacks-gamma/images-stacks-gamma-all.svg width=\"400\">|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| stack = 1 | stack = 4 | stack = 32 | stack = 64 |  \n",
    "|---|---|---|---|  \n",
    "| <img src=https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/bae41b89189519f64aa78d12693f800f5d62e51c/marianna/project02-rl/01-figures-experiments/02-images-stacks-gamma/images-stacks-gamma-stack-1.svg width=\"200\"> | <img src =https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/bae41b89189519f64aa78d12693f800f5d62e51c/marianna/project02-rl/01-figures-experiments/02-images-stacks-gamma/images-stacks-gamma-baseline.svg width=\"200\"> | <img src=https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/bae41b89189519f64aa78d12693f800f5d62e51c/marianna/project02-rl/01-figures-experiments/02-images-stacks-gamma/images-stacks-gamma-stack-32.svg width=\"200\"> |<img src=https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/bae41b89189519f64aa78d12693f800f5d62e51c/marianna/project02-rl/01-figures-experiments/02-images-stacks-gamma/images-stacks-gamma-stack-64.svg width=\"200\"> |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the graphs, we observed that, as we increase the number of frames in the stacks, the agent needs more timesteps to learn a policy that allows it to earn good rewards. This behavior may indicate that knowledge about a more distant past doesn’t necessarily bring improvements to agents for the problem addressed.\n",
    "\n",
    "Despite this, all curves tend to average reward values that are close and are around $21$ to $22$ points. In addition, the training time for each agent also increases by adding more frames, which is expected, since more data will be processed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Influence of the discount factor with RAM representation\n",
    "\n",
    "In order to evaluate how the PPO behaves when receiving the RAM values of the game as input, instead of the image, we performed experiments using all the RAM, an MLP policy network and varied the discount factor. The results are shown on the graphics bellow and the smoothed rewards for $\\gamma = 0.75$, $\\gamma = 0.90$ and $\\gamma = 0.99$ are $20.92$, $20.84$ and $19.57$, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.00025\n",
    "LAM = 0.95\n",
    "N_STACK = 4\n",
    "N_ENV = 4\n",
    "REPR = 'ram'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment('Experiments_PPO_ram_gamma_75', gamma=0.75, learning_rate=LEARNING_RATE, lam=LAM, policy=MlpPolicy,\n",
    "           n_stack=N_STACK, n_env=N_ENV, state_repr=REPR);\n",
    "experiment('Experiments_PPO_ram_gamma_90', gamma=0.90, learning_rate=LEARNING_RATE, lam=LAM, policy=MlpPolicy,\n",
    "           n_stack=N_STACK, n_env=N_ENV, state_repr=REPR);\n",
    "experiment('Experiments_PPO_ram_baseline', gamma=0.99, learning_rate=LEARNING_RATE, lam=LAM, policy=MlpPolicy,\n",
    "           n_stack=N_STACK, n_env=N_ENV, state_repr=REPR);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| |\n",
    "|-|\n",
    "|<img src=https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/bae41b89189519f64aa78d12693f800f5d62e51c/marianna/project02-rl/01-figures-experiments/03-ram-gamma/ram-gamma-all.svg width=\"400\">|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| $\\gamma$=0.75 | $\\gamma$=0.90 | $\\gamma$=0.99 |  \n",
    "|---|---|---|  \n",
    "| <img src=https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/bae41b89189519f64aa78d12693f800f5d62e51c/marianna/project02-rl/01-figures-experiments/03-ram-gamma/ram-gamma-75.svg width=\"250\"> | <img src =https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/bae41b89189519f64aa78d12693f800f5d62e51c/marianna/project02-rl/01-figures-experiments/03-ram-gamma/ram-gamma-90.svg width=\"250\"> | <img src=https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/bae41b89189519f64aa78d12693f800f5d62e51c/marianna/project02-rl/01-figures-experiments/03-ram-gamma/ram-gamma-baseline.svg width=\"250\"> |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observing the curves, we see that they are quite noisy, indicating difficulties during the agent's learning process. Furthermore, unlike what we saw when using images, now a lower value of $ \\gamma $ leads to better results. This indicates that, for this agent, seeing values closer to the present time is more significant. This time, the average rewards were between 21 and 23 points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Influence of input representation\n",
    "\n",
    "Another important comparison, still in the context of the previous section, is between the rewards obtained by using RAM or images as input. In the graphics bellow, we can see the results for images and RAM representations and for the three discount factors experimented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|**Discount factor equal to 0.75** |\n",
    "|-|\n",
    "|<img src=https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/bae41b89189519f64aa78d12693f800f5d62e51c/marianna/project02-rl/01-figures-experiments/04-images-ram/images-ram-75-all.svg width=\"300\">|\n",
    "    \n",
    "\n",
    "| Image | RAM |  \n",
    "|---|---|\n",
    "| <img src=https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/bae41b89189519f64aa78d12693f800f5d62e51c/marianna/project02-rl/01-figures-experiments/04-images-ram/images-ram-75-image.svg width=\"200\"> | <img src =https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/bae41b89189519f64aa78d12693f800f5d62e51c/marianna/project02-rl/01-figures-experiments/04-images-ram/images-ram-75-ram.svg width=\"200\"> | \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|**Discount factor equal to 0.90** |\n",
    "|-|\n",
    "|<img src=https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/bae41b89189519f64aa78d12693f800f5d62e51c/marianna/project02-rl/01-figures-experiments/04-images-ram/images-ram-90-all.svg width=\"300\">|\n",
    "    \n",
    "| Image | RAM |  \n",
    "|---|---|\n",
    "| <img src=https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/bae41b89189519f64aa78d12693f800f5d62e51c/marianna/project02-rl/01-figures-experiments/04-images-ram/images-ram-90-image.svg width=\"200\"> | <img src =https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/bae41b89189519f64aa78d12693f800f5d62e51c/marianna/project02-rl/01-figures-experiments/04-images-ram/images-ram-90-ram.svg width=\"200\"> |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|**Discount factor equal to 0.99** |\n",
    "|-|\n",
    "|<img src=https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/bae41b89189519f64aa78d12693f800f5d62e51c/marianna/project02-rl/01-figures-experiments/04-images-ram/images-ram-baseline-all.svg width=\"300\">|\n",
    "    \n",
    "| Image | RAM |  \n",
    "|---|---|\n",
    "| <img src=https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/bae41b89189519f64aa78d12693f800f5d62e51c/marianna/project02-rl/01-figures-experiments/04-images-ram/images-ram-baseline-image.svg width=\"200\"> | <img src =https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/bae41b89189519f64aa78d12693f800f5d62e51c/marianna/project02-rl/01-figures-experiments/04-images-ram/images-ram-baseline-ram.svg width=\"200\"> |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the rewards obtained are similar, except for the regions of the curves for $ \\gamma = 0,75 $ and $ \\gamma = 0,90 $ that start to decrease. This indicates that RAM can also provide a good representation for our problem.\n",
    "\n",
    "Despite this, given the big variability of the experiments, more tests need to be carried out to verify this hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `lam` is a factor that controls the trade-off of bias vs variance for Generalized Advantage Estimator [(PPO2 doc)](https://stable-baselines.readthedocs.io/en/master/modules/ppo2.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 0.00025\n",
    "LAM = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /root/venv/lib/python3.5/site-packages/stable_baselines/common/misc_util.py:26: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/venv/lib/python3.5/site-packages/stable_baselines/common/tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/venv/lib/python3.5/site-packages/stable_baselines/common/tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/venv/lib/python3.5/site-packages/stable_baselines/common/policies.py:116: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/venv/lib/python3.5/site-packages/stable_baselines/common/input.py:25: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/venv/lib/python3.5/site-packages/stable_baselines/common/policies.py:561: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From /root/venv/lib/python3.5/site-packages/tensorflow_core/python/layers/core.py:332: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From /root/venv/lib/python3.5/site-packages/stable_baselines/common/tf_layers.py:123: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/venv/lib/python3.5/site-packages/stable_baselines/common/distributions.py:326: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/venv/lib/python3.5/site-packages/stable_baselines/common/distributions.py:327: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/venv/lib/python3.5/site-packages/stable_baselines/ppo2/ppo2.py:190: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/venv/lib/python3.5/site-packages/stable_baselines/ppo2/ppo2.py:198: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/venv/lib/python3.5/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /root/venv/lib/python3.5/site-packages/stable_baselines/ppo2/ppo2.py:206: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/venv/lib/python3.5/site-packages/stable_baselines/ppo2/ppo2.py:240: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/venv/lib/python3.5/site-packages/stable_baselines/ppo2/ppo2.py:242: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/venv/lib/python3.5/site-packages/stable_baselines/common/base_class.py:1169: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/venv/lib/python3.5/site-packages/stable_baselines/common/tf_util.py:502: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "experiment('Experiments_PPO_L', gamma=GAMMA, learning_rate=LEARNING_RATE, lam=LAM, policy=MlpPolicy);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment('Experiments_PPO_L', gamma=GAMMA, learning_rate=LEARNING_RATE, lam=1.00, policy=MlpPolicy);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment('Experiments_PPO_L', gamma=GAMMA, learning_rate=LEARNING_RATE, lam=0.99, policy=MlpPolicy);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment('Experiments_PPO_L', gamma=GAMMA, learning_rate=LEARNING_RATE, lam=0.5, policy=MlpPolicy);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment('Experiments_PPO_L', gamma=GAMMA, learning_rate=LEARNING_RATE, lam=0.25, policy=MlpPolicy);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment('Experiments_PPO_L', gamma=GAMMA, learning_rate=LEARNING_RATE, lam=0.0, policy=MlpPolicy);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Parameter|L1|L2|L3|L4|L5|L6|\n",
    "|-|-|-|-|-|-|-|\n",
    "|GAMMA|0.99|0.99|0.99|0.99|0.99|0.99|\n",
    "|LEARNING_RATE|0.00025|0.00025|0.00025|0.00025|0.00025|0.00025|\n",
    "|LAM|0.95|1.00|0.99|0.5|0.25|0.0|\n",
    "|Smoothed Reward|21.62|23.71|22.51|21.88|24.28|20.39|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|![](./img/lam_complete.png)|\n",
    "|-|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|LAM = 0.95|LAM = 1.00|LAM = 0.99|\n",
    "|-|-|-|\n",
    "|![](./img/lam_0.95.png)|![](./img/lam_1.0.png)|![](./img/lam_0.99.png)|\n",
    "\n",
    "|LAM = 0.5|LAM = 0.25|LAM = 0.0|\n",
    "|-|-|-|\n",
    "|![](./img/lam_0.5.png)|![](./img/lam_0.25.png)|![](./img/lam_0.0.png)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the graphs above we can see that the LAM parameter doesn't seem to have an impactful relationship with the overall performance of the algorithm.\n",
    "\n",
    "Although the 0.25 lam ended up with a smoothed score of 24.28, it seems to be really unstable, and no apparent relationship with this parameter could be noted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Influence of the total timesteps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All previous experiments had a duration of 400K timesteps. In order to evaluate the influence of this parameter on the agents' performance, we run new experiments using the baseline and 1M timesteps. The results are shown on the figure bellow, for two executions of the baseline setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.00025\n",
    "LAM = 0.95\n",
    "GAMMA = 0.99\n",
    "N_ENV = 4\n",
    "TIMESTEPS = 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment('Experiments_PPO_baseline_1M', gamma=GAMMA, learning_rate=LEARNING_RATE, lam=LAM, policy=CnnPolicy,\n",
    "           n_stack=N_STACK, n_env=N_ENV, timesteps=TIMESTEPS);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| |\n",
    "|-|\n",
    "|<img src=https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/bae41b89189519f64aa78d12693f800f5d62e51c/marianna/project02-rl/01-figures-experiments/05-timesteps/timestep-baseline-1-2.svg width=\"400\">|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, after an initial growth, the reward stabilizes for a few thousand timesteps and starts growing again, allowing us to reach values much higher than those obtained previously. This indicates that we can still exploit a lot of the PPO's capacity for our problem, if we train it for a sufficient number of timesteps. Additionally, applying a smoothing of 0.6 on the results, we achieve a final smoothed reward of 28.73, and a peak reward of 31."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find bellow a gif showing how our trained agent perfoms after these 1M timesteps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| |\n",
    "|-|\n",
    "|<img src=https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/01ad1553f86ea6038e4fa81bf338fc86bc1769b8/marianna/project02-rl/gif/ppo_baseline_1M_gif.gif width=\"400\">|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Notes\n",
    "\n",
    "After the experiments with the discount factor, the forms of representation of the input, the number of frames of the stacks and the amount of training timesteps, we observed some important characteristics that should be highlighted.\n",
    "\n",
    "\n",
    "The first is the big variability of the results, so that two experiments performed with the same configurations sometimes generate considerably different performances. \n",
    "\n",
    "Another important observation is that, as we realized for the experiments with the baseline and 1M timesteps, we would probably get better results for all other configurations if we trained them for longer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Deep-Q-Network](https://arxiv.org/pdf/1312.5602.pdf) is a deep learning model that learns to control policies directly from high dimensional sensory using reinforcement learning.   \n",
    "\n",
    "The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating the future rewards.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Deep-Q-Network algorithm observes the image $x_t$ from the emulator which is a vector of raw pixel values representing the current screen. In addition it receives a reward $r_t$ representing the change in game score.  \n",
    "\n",
    "It considers sequences of actions and observations,  \n",
    "\n",
    "$s_t = x_1, a_1, x_2, ... a_{t-1}x_t$,  \n",
    "\n",
    "and learn game strategies that depend upon these sequences.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal action-value function obeys an important identity known as the Bellman equation. This is based on the following intuition: if the optimal value $Q*(s', a')$ of the sequence $s'$ at the next time-step was known for all possible actions $a'$, then the optimal strategy is to select the action $a'$\n",
    "maximising the expected value of $r + \\gamma Q*(s', a')$, where $\\gamma$ is the reward discount factor per time-step,  \n",
    "  \n",
    "$Q*(s, a) = E_{s' ~ \\epsilon}[r + \\gamma max_{a'}Q*(s', a')|s, a]$  \n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this project we applied the [algorithm implemented by Stable Baselines](https://stable-baselines.readthedocs.io/en/master/modules/dqn.html) to the Atari Freeway game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discount Factor $\\gamma$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The discount factor $\\gamma$ determines how much the agent cares about rewards in the distant future relative to those in the immediate future.  \n",
    "  \n",
    "If $\\gamma$=0, the agent will be completelly myopic and only learn about actions that produce an immediate reward.  \n",
    "\n",
    "If $\\gamma$=1, the agent will evaluate each of its actions based on the sum of total of all futures rewards.  \n",
    "  \n",
    "We used a $\\gamma$ value of 0.99 in order to make our agent care about distant future and we also decreased this value to 0.90 and 0.75 to see how they can impact the agent behavior.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we will be experimenting with 3 different parameters set:\n",
    "\n",
    "| Parameter | G1 | G2 | G3 |\n",
    "|------|----|----|----|\n",
    "| **`GAMMA`** | 0.99 | 0.90 | 0.75 |\n",
    "| `LEARNING_RATE` | 0.0005 | 0.0005 | 0.0005 |\n",
    "| `EXPLORATION_RATE` | 0.1 | 0.1 | 0.1 |\n",
    "|`Smoothed Reward` |20.73|23.25|21.72|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| |  \n",
    "|------|  \n",
    "|<img src=https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/main/aline.almeida/DQN/plots/dqn_gamma.png width=\"400\">|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| $\\gamma$=0.99 | $\\gamma$=0.90 | $\\gamma$=0.75 |  \n",
    "|---|---|---|  \n",
    "| <img src=https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/main/aline.almeida/DQN/plots/vermelho.png width=\"250\"> | <img src =https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/main/aline.almeida/DQN/plots/pink.png width=\"250\"> | <img src=https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/main/aline.almeida/DQN/plots/azul_claro.png width=\"250\"> |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plots above, we can see that the three values of $\\gamma$ can lead the agents to the similar score values, but some have delayed success achieving them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning rate deetermines to what extent newly acquired information overrides old information.  \n",
    "\n",
    "If the learning rate is 0, the agent will learn nothing (exclusively exploiting prior knowledge).  \n",
    "If the learning rate is 1, the agent consider only the most recent information (ignoring prior knowledge to explore possibilities).  \n",
    "\n",
    "We will be experimenting with 3 different parameters set:\n",
    "\n",
    "| Parameter | G1 | G2 | G3 |\n",
    "|------|----|----|----|\n",
    "| `GAMMA` | 0.99 | 0.99 | 0.99 |\n",
    "| **`LEARNING_RATE`** | 0.0005 | 0.0010 | 0.0050 |\n",
    "| `EXPLORATION_RATE` | 0.1 | 0.1 | 0.1 |\n",
    "|`Smoothed Reward` |20.73|21.13|2.616e-19 (approx. 0)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| |  \n",
    "|------|  \n",
    "|<img src=https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/main/aline.almeida/DQN/plots/dqn_lr.png width=\"400\">|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| `LEARNING_RATE`=0.0005 | `LEARNING_RATE`=0.0010 | `LEARNING_RATE`=0.0050 |  \n",
    "|---|---|---|  \n",
    "| <img src=https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/main/aline.almeida/DQN/plots/vermelho.png width=\"250\"> | <img src=https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/main/aline.almeida/DQN/plots/cinza.png width=\"250\"> | <img src=https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/main/aline.almeida/DQN/plots/verde.png width=\"250\"> |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in the plots above, the learning rate of 0.0005 and 0.0010 achieved approximately the same score values.  \n",
    "On the other hand, the learning rate of 0.0050 performed poorly and did not learn at all. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exploration rate is the probability that our agent will explore the environment rather than exploit it.  \n",
    "\n",
    "We used 0.1 as our baseline exploration value. In order to see how the exploration rate impact the agent behavior, we also made experiments using the double of this value (0.1) and the half of it (0.05)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All in all, these are the parameters that we are going to use to execute this experiment:\n",
    "\n",
    "| Parameter | G1 | G2 | G3 |\n",
    "|------|----|----|----|\n",
    "| `GAMMA` | 0.99 | 0.99 | 0.99 |\n",
    "| `LEARNING_RATE` | 0.0005 | 0.0005 | 0.0005 |\n",
    "| **`EXPLORATION_RATE`** | 0.1 | 0.05 | 0.20 |\n",
    "|`Smoothed Reward` |20.73|22.02|21.48|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| |  \n",
    "|------|  \n",
    "|<img src=https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/main/aline.almeida/DQN/plots/dqn_exploration.png witdh=\"400\">|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| `EXPLORATION_RATE`=0.0020 | `EXPLORATION_RATE`=0.0010 | `EXPLORATION_RATE`=0.0005 |  \n",
    "|---|---|---|  \n",
    "| <img src=https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/main/aline.almeida/DQN/plots/laranja.png width=\"250\"> | <img src=https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/main/aline.almeida/DQN/plots/vermelho.png width=\"250\"> | <img src=https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/main/aline.almeida/DQN/plots/azul.png width=\"250\"> |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As presented above, the three values of exploration rate lead the agents to about the same score values, but they do not increase the score at the same time, as we saw for $\\gamma$ parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Influence of the total timesteps\n",
    "Using the combination of parameters that seem to have a slightly high reward compare to the other experiments. We train the DQN over 1000000 timesteps. The defined parameters to run this experiment are the following:\n",
    "\n",
    "| Parameter | Value |\n",
    "|------|----|\n",
    "| `GAMMA` | 0.90 |\n",
    "| `LEARNING_RATE` | 0.0010 |\n",
    "| `EXPLORATION_RATE` | 0.05 |\n",
    "| **`TIMESTEPS`** | 1000000 |\n",
    "| `Smoothed Reward` |21.00|\n",
    "\n",
    "Finally, the result is showed in the image bellow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| |\n",
    "|-|\n",
    "|<img src=https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/main/marianna/project02-rl/01-figures-experiments/06-DQN/dqn_1M.svg width=700>|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, in the graph above that DQN starts to increase after 200 thousand timesteps and got a peak value of about 25.\n",
    "However, it still has a problem getting higher values. This indicates that we found an optimal local value and the DQN needs to explore more in order to get higher scores. Additionally, applying a smoothing of 0.6 on the results, we got a smoothed reward of 21.0 at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Episode\n",
    "You can see below a GIF showing our DQN trained agent after these 1M timesteps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| |\n",
    "|-|\n",
    "|<img src=https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/main/marianna/project02-rl/gif/dqn_1M_gif.gif width=400>|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also compare the performance of the model  1000000 timestep and the best model of our experiment that use the following configuration:\n",
    "\n",
    "| Parameter | Value |\n",
    "|------|----|\n",
    "| `GAMMA` | 0.90 |\n",
    "| `LEARNING_RATE` | 0.0005 |\n",
    "| `EXPLORATION_RATE` | 0.05 |\n",
    "| **`TIMESTEPS`** | 400000 |\n",
    "| `Smoothed Reward` |23.25|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| |\n",
    "|-|\n",
    "|<img src=https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/main/marianna/project02-rl/gif/dqn_teste2_gamma0p90.gif width=400>|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We noticed that with few timesteps the best-tested configuration achieved the same score as using 1 million timesteps that behavior also can be seen in the graph of reward. This seems to indicate that the DQN had already converged with 400k iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN experiments discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the results we got from the DQN plots we can see that they have achieved approximatelly the same score values at the end of 400k steps, the difference between them is mostly about how faster them increased their scores.\n",
    "\n",
    "From the experiments we ran, we are not able to indicate precisely what are the best hyper parameters to use, because they seem to not have a strong linear behavior.\n",
    "\n",
    "To explain that, we are supported by the Hado van Hasselt et al that demostrated in the paper [Deep Reinforcement Learning with Double Q-learning](https://arxiv.org/abs/1509.06461), that the DQN algorithm suffers from substantial overestimations in some games in the Atari domain.  \n",
    "\n",
    "They demonstrated that estimation errors of any kind can induce an upward bias, regardless of whether these errors are due to environmental noise, function approximation, non-stationarity, or any other source. This is important, because in practice any method will incur some inaccuracies during learning, simply due to the fact that the true values are initially unknown.\n",
    "\n",
    "As they show in their experiments, which plots are presented below, the DQN algorithm can be consistently and sometimes vastly overoptimistic about the value of the current greedy policy, as can be seen by comparing the orange learning curves in the top row of plots to the straight orange lines, which represent the actual discounted value of the best learned policy.   \n",
    "\n",
    "| |  \n",
    "|------|  \n",
    "| <img src=https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/main/aline.almeida/DQN/plot%20from%20the%20paper%20Deep%20Reinforcement%20Learning%20with%20Double%20Q-learning.png width=\"800\"> |  \n",
    "| Image from the paper [Deep Reinforcement Learning with Double Q-learning](https://arxiv.org/abs/1509.06461) |  \n",
    "\n",
    "\n",
    "In the image above we can see the detrimental effect of the DQN overestimations on the score achieved by the agent as it is evaluated during training in comparison with Double-DQN.\n",
    "\n",
    "Also, according to Sebastian Thrun and Anton Schwartz in the paper [Issues in Using Function Approximation for Reinforcement Learning](https://www.ri.cmu.edu/pub_files/pub1/thrun_sebastian_1993_1/thrun_sebastian_1993_1.pdf), DQN can have a systematic overestimation effect of values which is due to function approximation when used\n",
    "in recursive value estimation scheme, that can leads to learning fails completely on some cases if the parameters exceed the upper or lower bound for expected failure of Q-learning. This effect of failure exceeding the upper bound is presented in the figure below:  \n",
    "\n",
    "| |  \n",
    "|------|  \n",
    "| <img src=https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/main/aline.almeida/DQN/plot%20from%20the%20paper%20Issues%20in%20Using%20Function%20Approx%20for%20RL.png width=\"600\"> |  \n",
    "| Image from the paper [Issues in Using Function Approximation for Reinforcement Learning](https://www.ri.cmu.edu/pub_files/pub1/thrun_sebastian_1993_1/thrun_sebastian_1993_1.pdf) |  \n",
    "\n",
    "\n",
    "In the figure we can see the learning curves as a function of $\\gamma$. Each diagram shows the\n",
    "performance (probability of reaching the goal state) as a function of the number of training episodes. Note that learning fails completely if  $\\gamma$ > 0.98.\n",
    "\n",
    "Additionally, according to Kamyar Azizzadenesheli et al in the paper [Efficient Exploration through Bayesian Deep Q-Networks](https://arxiv.org/abs/1802.04412), DQN are empirically sensitive to the learning rate and changing it can degrade the performance to even worse than random policy.\n",
    "\n",
    "As we could see in our learning rate experiments, the learning rate of 0.0005 possible entered in the failure region and because of that did not learn at all, or it may suffered from the fact that DQN are sensitive to the learning rate.\n",
    "\n",
    "For the discount factor and exploration rate parameters experiments, we found an arbitrairly behavior when determining which agent would achieve higher scores faster. This aparently lack of correlation between the hyper parameters changes can be caused by the DQN overestimation caracteristic and the average of more than one run could be necessary to see the expected correlation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Thoughts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the stability of the solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the biggest challenges of this work was how to deal with the instability of the Deep Reinforcement Learning methods.\n",
    "\n",
    "Many times, simply re-running the experiment would lead to different results, making it hard to compare and draw conclusions out of it.\n",
    "\n",
    "This can be seem when we look at the experiments regarding the Learning Rate for both the DQN and the PPO algorithm, where one of the tests almost didn't score any points at all, while using other parameters achieved some reasonable scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computational cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another issue was regarding the required time to train the algorithms.\n",
    "In order to run the 400k iterations, DQN would take about an hour and a half, and due to our time constraints, it was hard to run many longer experiments.\n",
    "\n",
    "The PPO was a lot faster to train than the DQN because it accepts a vectorized environment as input.\n",
    "This allows the training agent to train in multiple environments per step, allowing a much faster training.\n",
    "Also, PPO was already designed with performance in mind, aiming to be a faster algorithm than its peers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimality and convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we believe we would be able to achieve better results if we left our algorithms training longer, we are still satisfied with what we achieved here, principally with the PPO.\n",
    "We were able to achieve a smoothed average of 28.73 points using it, and it isn't that far from the state-of-the-art score of 34 points.\n",
    "It is worth noting here that this solution hadn't converged yet.\n",
    "\n",
    "The DQN algorithm didn't perform as good as PPO.\n",
    "Training it for 1M timesteps, the model converged to a 0.6 smoothed average of 21.0 points.\n",
    "Interestingly, the DQN converged a lot faster (in timesteps) than PPO, which is characteristic of off-policy methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing with classical methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the first assignment we used classical tabular methods to tackle this problem.\n",
    "Using the SARSA($\\lambda$) algorithm, we were able to achieve ~31 points on average, and a peak of 34.\n",
    "\n",
    "The classical methods still provided a better solution for our problem, but we believe that training the deep learning algorithms longer could lead to better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All in all, we were satisfied with what we achieved, exploring and experimenting a lot on two different deep reinforcement learning methods, in a challenging problem!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find a short video of our work [here](https://drive.google.com/file/d/1DZ2QEzZsy_rGJPoFqY9wUiT0Tj1hMYXu/view?usp=sharing)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
