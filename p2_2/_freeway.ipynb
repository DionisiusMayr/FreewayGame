{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Freeway"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the second project for the MC935A/MO436A - Reinforcement Learning course, taught by Prof. Esther Colombini.\n",
    "\n",
    "In this project we propose to apply Reinforcement Learning methods to teach an agent how to play the Freeway Atari game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Group members:**\n",
    "- Aline Gabriel de Almeida\n",
    "- Dionisius Oliveira Mayr (229060)\n",
    "- Marianna de Pinho Severo (264960)\n",
    "- Victor Jesús Sotelo Chico (265173)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Freeway game\n",
    "\n",
    "![Baseline 1](./img/Freeway_logo.png)\n",
    "\n",
    "Freeway is a video game written by David Crane for the Atari 2600 and published by Activision [[1]](https://en.wikipedia.org/wiki/Freeway_(video_game)).\n",
    "\n",
    "In the game, two players compete against each other trying to make their chickens cross the street, while evading the cars passing by.\n",
    "There are three possible actions: staying still, moving forward or moving backward.\n",
    "Each time a chicken collides with a car, it is forced back some spaces and takes a while until the chicken regains its control.\n",
    "\n",
    "When a chicken is successfully guided across the freeway, it is awarded one point and moved to the initial space, where it will try to cross the street again.\n",
    "The game offers multiple scenarios with different vehicles configurations (varying the type, frequency and speed of them) and plays for 2 minutes and 16 seconds.\n",
    "During the 8 last seconds the scores will start blinking to indicate that the game is close to end.\n",
    "Whoever has the most points after this, wins the game!\n",
    "\n",
    "The image was extracted from the [manual of the game](https://www.gamesdatabase.org/Media/SYSTEM/Atari_2600/Manual/formated/Freeway_-_1981_-_Zellers.pdf).\n",
    "\n",
    "[1 - Wikipedia - Freeway](https://en.wikipedia.org/wiki/Freeway_(video_game))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the [OpenAI Gym](https://gym.openai.com/) toolkit.\n",
    "This toolkit uses the [Arcade Learning Environment](https://github.com/mgbellemare/Arcade-Learning-Environment) to simulate the game through the [Stella](https://stella-emu.github.io/) emulator.\n",
    "\n",
    "Although the game offers multiple scenarios, we are going to consider only the first one. Also, we will be controlling a *single chicken*, while we try to maximize its score.\n",
    "\n",
    "In this configuration, there are ten lanes and each lane contains exactly one car (with a different speed and direction).\n",
    "Whenever an action is chosen, it is repeated for $k$ frames, $k \\in \\{2, 3, 4\\}$.\n",
    "\n",
    "This means that our environment is **stochastic** and it is also **episodic**, with its terminal state being reached whenever 2 minutes and 16 seconds have passed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stable Baselines Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the [Stable Baselines](https://stable-baselines.readthedocs.io/en/master/) implementations of the Deep Reinforcement algorithms.\n",
    "\n",
    "It is a fork of the OpenAI Baselines, fully compatible with the OpenAI Gym environments with straightforward usage.\n",
    "\n",
    "Under the hood, it is using the Tensorflow library to implement the neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the dependencies:\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "pip install stable-baselines[mpi]\n",
    "pip install tensorflow==1.15.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can find a list of useful links and materials that were used during this project.\n",
    "\n",
    "* [Freeway-ram-v0 from OpenAI Gym](https://gym.openai.com/envs/Freeway-ram-v0/)\n",
    "* [Manual of the game](https://www.gamesdatabase.org/Media/SYSTEM/Atari_2600/Manual/formated/Freeway_-_1981_-_Zellers.pdf)\n",
    "* [Freeway Benchmarks](https://paperswithcode.com/sota/atari-games-on-atari-2600-freeway)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import gym\n",
    "\n",
    "from stable_baselines.common import make_vec_env\n",
    "from stable_baselines.common.cmd_util import make_atari_env\n",
    "from stable_baselines.common.vec_env import VecFrameStack\n",
    "from stable_baselines.common.atari_wrappers import make_atari\n",
    "from stable_baselines.common.policies import MlpPolicy, CnnPolicy\n",
    "\n",
    "from stable_baselines.deepq.policies import CnnPolicy\n",
    "\n",
    "from stable_baselines import DQN\n",
    "from stable_baselines import PPO2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Action space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we said above, the agent in this game has three possible actions at each frame, each represented by an integer:\n",
    "\n",
    "* 0: Stay\n",
    "* 1: Move forward\n",
    "* 2: Move backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In theory, a perfect chicken wouldn't ever need to move backward, since it is possible to know if moving forward would lead you into a collision (in the immediate frame or in the future frames)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State of the art benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image bellow (extracted from https://paperswithcode.com/sota/atari-games-on-atari-2600-freeway) shows the evolution of the scores over time using different techniques.\n",
    "\n",
    "Today, the state of the art approaches are making 34.0 points, using Deep Reinforcement Learning methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Benchmarks](./img/state_of_art_scores.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Previous Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the previous assignment, we explored how classical Reinforcement Learning tabular methods performed on this game.\n",
    "\n",
    "In some runs, we were able to achieve a peak score of 34 points in a single run using the SARSA($\\lambda$) algorithm, which is the state-of-the-art score for this game; but on average we were scoring about 31 points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reward Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the base environment we are awarded on point each time we successfully cross the freeway."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of parameters to be tuned in our algorithms.\n",
    "However, we don't have the required computational power to properly optimize them all.\n",
    "\n",
    "Since we still want to at least experiment them to observe their impact on the obtained reward, we will be training them for 400k iterations, and then we will select a few of them to train for more iterations.\n",
    "\n",
    "We are aware that this is far from perfect in the optimization point of view, but this is the best we can do with our available machines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be comparing two different algorithms here: DQN and PPO.\n",
    "\n",
    "The idea is to compare an off-policy method (DQN) and an on-policy method (PPO) regarding their stability and number of samples to convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorboard allows us to visualize and compare the evolution of the algorithms being tested.\n",
    "It shows the metrics like the reward over time, loss and advantage, in an easy way.\n",
    "\n",
    "We will be using it to present our results, understand the impact of different parameters and compare our solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frame Stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before diving into the experiments' results, it is worth understanding some nuances of our testing environment.\n",
    "\n",
    "We will be using image representations (frames) of the game as our state.\n",
    "But by doing so, we end up losing some relavant information of the problem, like the direction the cars are moving.\n",
    "In other words, the problem we are trying to solve becomes non-markovian.\n",
    "\n",
    "To solve this, we use the `VecFrameStack` function, that stacks $n$ frames of the game, making our environment become markovian once again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorized Environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Vectorized environments](https://stable-baselines.readthedocs.io/en/master/guide/vec_envs.html) allows us to stack multiple environments, thus permiting us to traing an agent in $n$ environments per step.\n",
    "\n",
    "It is controlled by the `num_env` parameter in the `make_atari_env` function.\n",
    "\n",
    "Although it isn't possible to use vectorized environments with DQN, we will be using it for the PPO2 algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Deep-Q-Network](https://arxiv.org/pdf/1312.5602.pdf) is a deep learning model that learns to control policies directly from high dimensional sensory using reinforcement learning.   \n",
    "\n",
    "The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating the future rewards.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Deep-Q-Network algorithm observes the image $x_t$ from the emulator which is a vector of raw pixel values representing the current screen. In addition it receives a reward $r_t$ representing the change in game score.  \n",
    "\n",
    "It considers sequences of actions and observations,  \n",
    "\n",
    "$s_t = x_1, a_1, x_2, ... a_{t-1}x_t$,  \n",
    "\n",
    "and learn game strategies that depend upon these sequences.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal action-value function obeys an important identity known as the Bellman equation. This is based on the following intuition: if the optimal value $Q*(s', a')$ of the sequence $s'$ at the next time-step was known for all possible actions $a'$, then the optimal strategy is to select the action $a'$\n",
    "maximising the expected value of $r + \\gamma Q*(s', a')$, where $\\gamma$ is the reward discount factor per time-step,  \n",
    "  \n",
    "$Q*(s, a) = E_{s' ~ \\epsilon}[r + \\gamma max_{a'}Q*(s', a')|s, a]$  \n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this project we applied the [algorithm implemented by Stable Baselines](https://stable-baselines.readthedocs.io/en/master/modules/dqn.html) to the Atari Freeway game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Influence of the discount factor $\\gamma$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The discount factor $\\gamma$ determines how much the agent cares about rewards in the distant future relative to those in the immediate future.  \n",
    "  \n",
    "If $\\gamma$=0, the agent will be completelly myopic and only learn about actions that produce an immediate reward.  \n",
    "\n",
    "If $\\gamma$=1, the agent will evaluate each of its actions based on the sum of total of all futures rewards.  \n",
    "  \n",
    "We used a $\\gamma$ value of 0.99 in order to make our agent care about distant future and we also decreased this value to 0.90 and 0.75 to see how they can impact the agent behavior.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we will be experimenting with 3 different parameters set:\n",
    "\n",
    "| Parameter | G1 | G2 | G3 |\n",
    "|------|----|----|----|\n",
    "| **`GAMMA`** | 0.99 | 0.90 | 0.75 |\n",
    "| `LEARNING_RATE` | 0.0005 | 0.0005 | 0.0005 |\n",
    "| `EXPLORATION_RATE` | 0.1 | 0.1 | 0.1 |\n",
    "|`Smoothed Reward` |20.73|23.25|21.72|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/main/aline.almeida/DQN/plots/dqn_gamma.png width=\"400\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| $\\gamma$=0.99 | $\\gamma$=0.90 | $\\gamma$=0.75 |  \n",
    "|---|---|---|  \n",
    "| <img src=https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/main/aline.almeida/DQN/plots/vermelho.png width=\"250\"> | <img src =https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/main/aline.almeida/DQN/plots/pink.png width=\"250\"> | <img src=https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/main/aline.almeida/DQN/plots/azul_claro.png width=\"250\"> |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be experimenting with 3 different parameters set:\n",
    "\n",
    "| Parameter | G1 | G2 | G3 |\n",
    "|------|----|----|----|\n",
    "| `GAMMA` | 0.99 | 0.99 | 0.99 |\n",
    "| **`LEARNING_RATE`** | 0.0005 | 0.0010 | 0.0050 |\n",
    "| `EXPLORATION_RATE` | 0.1 | 0.1 | 0.1 |\n",
    "|`Smoothed Reward` |20.73|21.13|2.616e-19 (approx. 0)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/main/aline.almeida/DQN/plots/dqn_lr.png width=\"400\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| `LEARNING_RATE`=0.0005 | `LEARNING_RATE`=0.0010 | `LEARNING_RATE`=0.0050 |  \n",
    "|---|---|---|  \n",
    "| <img src=https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/main/aline.almeida/DQN/plots/vermelho.png width=\"250\"> | <img src=https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/main/aline.almeida/DQN/plots/cinza.png width=\"250\"> | <img src=https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/main/aline.almeida/DQN/plots/verde.png width=\"250\"> |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Influence of the agent's exploration rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exploration rate is the probability that our agent will explore the environment rather than exploit it.  \n",
    "\n",
    "We used 0.1 as our baseline exploration value. In order to see how the exploration rate impact the agent behavior, we also made experiments using the double of this value (0.1) and the half of it (0.05)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All in all, these are the parameters that we are going to use to execute this experiment:\n",
    "\n",
    "| Parameter | G1 | G2 | G3 |\n",
    "|------|----|----|----|\n",
    "| `GAMMA` | 0.99 | 0.99 | 0.99 |\n",
    "| `LEARNING_RATE` | 0.0005 | 0.0005 | 0.0005 |\n",
    "| **`EXPLORATION_RATE`** | 0.1 | 0.05 | 0.20 |\n",
    "|`Smoothed Reward` |20.73|22.02|21.48|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/main/aline.almeida/DQN/plots/dqn_exploration.png witdh=\"400\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| `EXPLORATION_RATE`=0.0020 | `EXPLORATION_RATE`=0.0010 | `EXPLORATION_RATE`=0.0005 |  \n",
    "|---|---|---|  \n",
    "| <img src=https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/main/aline.almeida/DQN/plots/laranja.png width=\"250\"> | <img src=https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/main/aline.almeida/DQN/plots/vermelho.png width=\"250\"> | <img src=https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/main/aline.almeida/DQN/plots/azul.png width=\"250\"> |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_TIMESTEPS = 400000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(tb_lob, gamma, learning_rate, lam, policy, timesteps=TOTAL_TIMESTEPS):\n",
    "    # multiprocess environment\n",
    "    env = make_atari_env('FreewayNoFrameskip-v0', num_env=8, seed=0)\n",
    "    # Frame-stacking with 4 frames\n",
    "    env = VecFrameStack(env, n_stack=4)\n",
    "\n",
    "    model = PPO2(policy, env, verbose=0, tensorboard_log=tb_lob, gamma=gamma, learning_rate=learning_rate, lam=lam)\n",
    "    model.learn(total_timesteps=timesteps)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 0.00025\n",
    "LAM = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment('Experiments_PPO_A', gamma=GAMMA, learning_rate=LEARNING_RATE, lam=LAM, policy=MlpPolicy);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment('Experiments_PPO_A', gamma=GAMMA, learning_rate=0.00025 * 0.25, lam=LAM, policy=MlpPolicy);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment('Experiments_PPO_A', gamma=GAMMA, learning_rate=0.00025 * 0.5, lam=LAM, policy=MlpPolicy);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment('Experiments_PPO_A', gamma=GAMMA, learning_rate=0.00025 * 2, lam=LAM, policy=MlpPolicy);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment('Experiments_PPO_A', gamma=GAMMA, learning_rate=0.00025 * 4, lam=LAM, policy=MlpPolicy);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Parameter|A1|A2|A3|A4|A5|\n",
    "|-|-|-|-|-|-|\n",
    "|GAMMA|0.99|0.99|0.99|0.99|0.99|\n",
    "|LEARNING_RATE|$$2.5 \\cdot 10^{-4}$$|$$6.25 \\cdot 10^{-5}$$|$$1.25 \\cdot 10^{-4}$$|$$5 \\cdot 10^{-4}$$|$$1 \\cdot 10^{-3}$$|\n",
    "|LAM|0.95|0.95|0.95|0.95|0.95|\n",
    "|Smoothed Reward|21.49|21.41|0.0|21.21|21.17|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|![](./img/alpha_complete.png)|\n",
    "|-| "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|LEARNING_RATE = $$2.5 \\cdot 10^{-4}$$|LEARNING_RATE = $$6.25 \\cdot 10^{-5}$$|LEARNING_RATE = $$1.25 \\cdot 10^{-4}$$|\n",
    "|-|-|-|\n",
    "|![](./img/alpha_2.5-4.png)|![](./img/alpha_6.25-5.png)|![](./img/alpha_1.25-4.png)|\n",
    "\n",
    "|LEARNING_RATE = $$5 \\cdot 10^{-4}$$|LEARNING_RATE = $$1 \\cdot 10^{-3}$$|\n",
    "|-|-|\n",
    "|![](./img/alpha_5-4.png)|![](./img/alpha_1-3.png)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `lam` is a factor that controls the trade-off of bias vs variance for Generalized Advantage Estimator [(PPO2 doc)](https://stable-baselines.readthedocs.io/en/master/modules/ppo2.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 0.00025\n",
    "LAM = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /root/venv/lib/python3.5/site-packages/stable_baselines/common/misc_util.py:26: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/venv/lib/python3.5/site-packages/stable_baselines/common/tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/venv/lib/python3.5/site-packages/stable_baselines/common/tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/venv/lib/python3.5/site-packages/stable_baselines/common/policies.py:116: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/venv/lib/python3.5/site-packages/stable_baselines/common/input.py:25: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/venv/lib/python3.5/site-packages/stable_baselines/common/policies.py:561: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From /root/venv/lib/python3.5/site-packages/tensorflow_core/python/layers/core.py:332: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From /root/venv/lib/python3.5/site-packages/stable_baselines/common/tf_layers.py:123: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/venv/lib/python3.5/site-packages/stable_baselines/common/distributions.py:326: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/venv/lib/python3.5/site-packages/stable_baselines/common/distributions.py:327: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/venv/lib/python3.5/site-packages/stable_baselines/ppo2/ppo2.py:190: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/venv/lib/python3.5/site-packages/stable_baselines/ppo2/ppo2.py:198: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/venv/lib/python3.5/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /root/venv/lib/python3.5/site-packages/stable_baselines/ppo2/ppo2.py:206: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/venv/lib/python3.5/site-packages/stable_baselines/ppo2/ppo2.py:240: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/venv/lib/python3.5/site-packages/stable_baselines/ppo2/ppo2.py:242: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/venv/lib/python3.5/site-packages/stable_baselines/common/base_class.py:1169: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/venv/lib/python3.5/site-packages/stable_baselines/common/tf_util.py:502: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "experiment('Experiments_PPO_L', gamma=GAMMA, learning_rate=LEARNING_RATE, lam=LAM, policy=MlpPolicy);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment('Experiments_PPO_L', gamma=GAMMA, learning_rate=LEARNING_RATE, lam=1.00, policy=MlpPolicy);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment('Experiments_PPO_L', gamma=GAMMA, learning_rate=LEARNING_RATE, lam=0.99, policy=MlpPolicy);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment('Experiments_PPO_L', gamma=GAMMA, learning_rate=LEARNING_RATE, lam=0.5, policy=MlpPolicy);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment('Experiments_PPO_L', gamma=GAMMA, learning_rate=LEARNING_RATE, lam=0.25, policy=MlpPolicy);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment('Experiments_PPO_L', gamma=GAMMA, learning_rate=LEARNING_RATE, lam=0.0, policy=MlpPolicy);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Parameter|L1|L2|L3|L4|L5|L6|\n",
    "|-|-|-|-|-|-|-|\n",
    "|GAMMA|0.99|0.99|0.99|0.99|0.99|0.99|\n",
    "|LEARNING_RATE|0.00025|0.00025|0.00025|0.00025|0.00025|0.00025|\n",
    "|LAM|0.95|1.00|0.99|0.5|0.25|0.0|\n",
    "|Smoothed Reward|21.62|23.71|22.51|21.88|24.28|20.39|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|![](./img/lam_complete.png)|\n",
    "|-|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|LAM = 0.95|LAM = 1.00|LAM = 0.99|\n",
    "|-|-|-|\n",
    "|![](./img/lam_0.95.png)|![](./img/lam_1.0.png)|![](./img/lam_0.99.png)|\n",
    "\n",
    "|LAM = 0.5|LAM = 0.25|LAM = 0.0|\n",
    "|-|-|-|\n",
    "|![](./img/lam_0.5.png)|![](./img/lam_0.25.png)|![](./img/lam_0.0.png)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the graphs above we can see that the LAM parameter doesn't seem to have an impactful relationship with the overall performance of the algorithm.\n",
    "\n",
    "Although the 0.25 lam ended up with a smoothed score of 24.28, it seems to be really unstable, and no apparent relationship with this parameter could be noted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variability of experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will be exploring how unstable a run of the algorithm is by repeating it multiple times with the same parameter settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 0.00025\n",
    "LAM = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment('Experiments_PPO_Var', gamma=GAMMA, learning_rate=LEARNING_RATE, lam=LAM, policy=MlpPolicy);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment('Experiments_PPO_Var', gamma=GAMMA, learning_rate=LEARNING_RATE, lam=LAM, policy=MlpPolicy);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment('Experiments_PPO_Var', gamma=GAMMA, learning_rate=LEARNING_RATE, lam=LAM, policy=MlpPolicy);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Parameter|R1|R2|R3|\n",
    "|-|-|-|-|\n",
    "|GAMMA|0.99|0.99|0.99|\n",
    "|LEARNING_RATE|0.00025|0.00025|0.00025|\n",
    "|LAM|0.95|0.95|0.95|\n",
    "|Smoothed Reward|22.68|21.75|21.75|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|![](./img/var_complete.png)|\n",
    "|-|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|R1|R2|R3|\n",
    "|-|-|-|\n",
    "|![](./img/var_1.png)|![](./img/var_2.png)|![](./img/var_3.png)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Thoughts ------------- TODO TODO TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computational cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the biggest problems on this project was the computational cost of running an episode.\n",
    "\n",
    "Since each run plays for 2 minutes and 16 seconds in the original game, there are quite a lot of frames that need to be computed for each episode.\n",
    "Even though the frame-sync is deactivated in our environment, each time we execute on episode it takes about 2 seconds to compute it for Q-Learn and Monte Carlo, but it takes around 21s seconds for SARSA($\\lambda$)! This means that in order to run 4k episodes, one must wait 23 hours. And since we want to experiment with 6 different values of $\\lambda$, it would take about a week to compute this all using a single core processor.\n",
    "\n",
    "The memory usage is fairly low compared to the time that it takes to run the algorithms, even with a decent amount of unique states in our problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding the optimality of our solution, we were able to achive the state-of-the-art score (34.0) some times with the SARSA($\\lambda$) algorithm, but on average we are closer to 31 points (which is also good, having said that our baseline was 21.8.\n",
    "Q-Learning also showed really good results, achieving about 28 points on average.\n",
    "On the other hand, Monte Carlo didn't perform well in our problem, achieving only 13 points on average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regarding the Linear Function Approximators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the experiments realized with function approximation for Monte Carlo, Q Learning and Sarsa Lambda control algorithms, we achieved some meaningful results: \n",
    "\n",
    "* We were not able to improve the agents compared to the algorithms without function approximation. Some factors may have contributed to this, such as the simplicity of the linear approximator for a problem that, probably, have many nonlinear relationships between the environment variables. Also, the features that we created may not be good enough to provide meaningful information about the environment to the agents.\n",
    "\n",
    "* The feature approximation of Monte Carlo said that we have to take a sample for our model in order to update the weight values. However, for our problems, this sample is the entire game, so each new generated sample follows a past policy that makes the convergence slow.\n",
    "\n",
    "* Linear approximators can be faster than its original counterpart as we saw with SARSA, but it’s not always the case.\n",
    "\n",
    "* To add randomness to underfitted models by using a bigger N0 can be beneficial to the solution.\n",
    "\n",
    "* Each algorithm has its own specificities, that depends on how it works. For example, for the Monte Carlo approximator, using a less sparse feature vector contributed for better results. On the other hand, for the Q-Learning and Sarsa Lambda approximators, providing a bigger exploration capacity for the agent was very important to achieve better results.\n",
    "\n",
    "* In all the experiments, the agents converged to a behavior similar to that of the baseline, learning that moving up the most part of the time is the best policy. However, as we saw from the algorithms without approximation, that is not the truth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All in all, we were satisfied with what we achieved, being able to apply multiple tabular methods and experiment a lot on Reinforcement Learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
