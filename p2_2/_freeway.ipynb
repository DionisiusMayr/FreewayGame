{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Deep-Q-Network](https://arxiv.org/pdf/1312.5602.pdf) is a deep learning model that learns to control policies directly from high dimensional sensory using reinforcement learning.   \n",
    "\n",
    "The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating the future rewards.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Deep-Q-Network algorithm observes the image $x_t$ from the emulator which is a vector of raw pixel values representing the current screen. In addition it receives a reward $r_t$ representing the change in game score.  \n",
    "\n",
    "It considers sequences of actions and observations,  \n",
    "\n",
    "$s_t = x_1, a_1, x_2, ... a_{t-1}x_t$,  \n",
    "\n",
    "and learn game strategies that depend upon these sequences.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal action-value function obeys an important identity known as the Bellman equation. This is based on the following intuition: if the optimal value $Q*(s', a')$ of the sequence $s'$ at the next time-step was known for all possible actions $a'$, then the optimal strategy is to select the action $a'$\n",
    "maximising the expected value of $r + \\gamma Q*(s', a')$, where $\\gamma$ is the reward discount factor per time-step,  \n",
    "  \n",
    "$Q*(s, a) = E_{s' ~ \\epsilon}[r + \\gamma max_{a'}Q*(s', a')|s, a]$  \n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this project we applied the [algorithm implemented by Stable Baselines](https://stable-baselines.readthedocs.io/en/master/modules/dqn.html) to the Atari Freeway game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discount Factor $\\gamma$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The discount factor $\\gamma$ determines how much the agent cares about rewards in the distant future relative to those in the immediate future.  \n",
    "  \n",
    "If $\\gamma$=0, the agent will be completelly myopic and only learn about actions that produce an immediate reward.  \n",
    "\n",
    "If $\\gamma$=1, the agent will evaluate each of its actions based on the sum of total of all futures rewards.  \n",
    "  \n",
    "We used a $\\gamma$ value of 0.99 in order to make our agent care about distant future and we also decreased this value to 0.90 and 0.75 to see how they can impact the agent behavior.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we will be experimenting with 3 different parameters set:\n",
    "\n",
    "| Parameter | G1 | G2 | G3 |\n",
    "|------|----|----|----|\n",
    "| **`GAMMA`** | 0.99 | 0.90 | 0.75 |\n",
    "| `LEARNING_RATE` | 0.0005 | 0.0005 | 0.0005 |\n",
    "| `EXPLORATION_RATE` | 0.1 | 0.1 | 0.1 |\n",
    "|`Smoothed Reward` |20.73|23.25|21.72|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| |  \n",
    "|------|  \n",
    "|<img src=https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/main/aline.almeida/DQN/plots/dqn_gamma.png width=\"400\">|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| $\\gamma$=0.99 | $\\gamma$=0.90 | $\\gamma$=0.75 |  \n",
    "|---|---|---|  \n",
    "| <img src=https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/main/aline.almeida/DQN/plots/vermelho.png width=\"250\"> | <img src =https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/main/aline.almeida/DQN/plots/pink.png width=\"250\"> | <img src=https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/main/aline.almeida/DQN/plots/azul_claro.png width=\"250\"> |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plots above, we can see that the three values of $\\gamma$ can lead the agents to the similar score values, but some have delayed success achieving them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning rate deetermines to what extent newly acquired information overrides old information.  \n",
    "\n",
    "If the learning rate is 0, the agent will learn nothing (exclusively exploiting prior knowledge).  \n",
    "If the learning rate is 1, the agent consider only the most recent information (ignoring prior knowledge to explore possibilities).  \n",
    "\n",
    "We will be experimenting with 3 different parameters set:\n",
    "\n",
    "| Parameter | G1 | G2 | G3 |\n",
    "|------|----|----|----|\n",
    "| `GAMMA` | 0.99 | 0.99 | 0.99 |\n",
    "| **`LEARNING_RATE`** | 0.0005 | 0.0010 | 0.0050 |\n",
    "| `EXPLORATION_RATE` | 0.1 | 0.1 | 0.1 |\n",
    "|`Smoothed Reward` |20.73|21.13|2.616e-19 (approx. 0)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| |  \n",
    "|------|  \n",
    "|<img src=https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/main/aline.almeida/DQN/plots/dqn_lr.png width=\"400\">|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| `LEARNING_RATE`=0.0005 | `LEARNING_RATE`=0.0010 | `LEARNING_RATE`=0.0050 |  \n",
    "|---|---|---|  \n",
    "| <img src=https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/main/aline.almeida/DQN/plots/vermelho.png width=\"250\"> | <img src=https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/main/aline.almeida/DQN/plots/cinza.png width=\"250\"> | <img src=https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/main/aline.almeida/DQN/plots/verde.png width=\"250\"> |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in the plots above, the learning rate of 0.0005 and 0.0010 achieved approximately the same score values.  \n",
    "On the other hand, the learning rate of 0.0050 performed poorly and did not learn at all. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exploration rate is the probability that our agent will explore the environment rather than exploit it.  \n",
    "\n",
    "We used 0.1 as our baseline exploration value. In order to see how the exploration rate impact the agent behavior, we also made experiments using the double of this value (0.1) and the half of it (0.05)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All in all, these are the parameters that we are going to use to execute this experiment:\n",
    "\n",
    "| Parameter | G1 | G2 | G3 |\n",
    "|------|----|----|----|\n",
    "| `GAMMA` | 0.99 | 0.99 | 0.99 |\n",
    "| `LEARNING_RATE` | 0.0005 | 0.0005 | 0.0005 |\n",
    "| **`EXPLORATION_RATE`** | 0.1 | 0.05 | 0.20 |\n",
    "|`Smoothed Reward` |20.73|22.02|21.48|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| |  \n",
    "|------|  \n",
    "|<img src=https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/main/aline.almeida/DQN/plots/dqn_exploration.png witdh=\"400\">|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| `EXPLORATION_RATE`=0.0020 | `EXPLORATION_RATE`=0.0010 | `EXPLORATION_RATE`=0.0005 |  \n",
    "|---|---|---|  \n",
    "| <img src=https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/main/aline.almeida/DQN/plots/laranja.png width=\"250\"> | <img src=https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/main/aline.almeida/DQN/plots/vermelho.png width=\"250\"> | <img src=https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/main/aline.almeida/DQN/plots/azul.png width=\"250\"> |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As presented above, the three values of exploration rate lead the agents to about the same score values, but they do not increase the score at the same time, as we saw for $\\gamma$ parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new text below -----------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN experiments discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the results we got from the DQN plots we can see that they have achieved approximatelly the same score values at the end of 400k steps, the difference between them is mostly about how faster them increased their scores.\n",
    "\n",
    "From the experiments we ran, we are not able to indicate precisely what are the best hyper parameters to use, because they seem to not have a strong linear behavior.\n",
    "\n",
    "To explain that, we are supported by the Hado van Hasselt et al that demostrated in the paper [Deep Reinforcement Learning with Double Q-learning](https://arxiv.org/abs/1509.06461), that the DQN algorithm suffers from substantial overestimations in some games in the Atari domain.  \n",
    "\n",
    "They demonstrated that estimation errors of any kind can induce an upward bias, regardless of whether these errors are due to environmental noise, function approximation, non-stationarity, or any other source. This is important, because in practice any method will incur some inaccuracies during learning, simply due to the fact that the true values are initially unknown.\n",
    "\n",
    "As they show in their experiments, which plots are presented below, the DQN algorithm can be consistently and sometimes vastly overoptimistic about the value of the current greedy policy, as can be seen by comparing the orange learning curves in the top row of plots to the straight orange lines, which represent the actual discounted value of the best learned policy.   \n",
    "\n",
    "| |  \n",
    "|------|  \n",
    "| <img src=https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/main/aline.almeida/DQN/plot%20from%20the%20paper%20Deep%20Reinforcement%20Learning%20with%20Double%20Q-learning.png width=\"800\"> |  \n",
    "| Image from the paper [Deep Reinforcement Learning with Double Q-learning](https://arxiv.org/abs/1509.06461) |  \n",
    "\n",
    "\n",
    "In the image above we can see the detrimental effect of the DQN overestimations on the score achieved by the agent as it is evaluated during training in comparison with Double-DQN.\n",
    "\n",
    "Also, according to Sebastian Thrun and Anton Schwartz in the paper [Issues in Using Function Approximation for Reinforcement Learning](https://www.ri.cmu.edu/pub_files/pub1/thrun_sebastian_1993_1/thrun_sebastian_1993_1.pdf), DQN can have a systematic overestimation effect of values which is due to function approximation when used\n",
    "in recursive value estimation scheme, that can leads to learning fails completely on some cases if the parameters exceed the upper or lower bound for expected failure of Q-learning. This effect of failure exceeding the upper bound is presented in the figure below:  \n",
    "\n",
    "| |  \n",
    "|------|  \n",
    "| <img src=https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/main/aline.almeida/DQN/plot%20from%20the%20paper%20Issues%20in%20Using%20Function%20Approx%20for%20RL.png width=\"600\"> |  \n",
    "| Image from the paper [Issues in Using Function Approximation for Reinforcement Learning](https://www.ri.cmu.edu/pub_files/pub1/thrun_sebastian_1993_1/thrun_sebastian_1993_1.pdf) |  \n",
    "\n",
    "\n",
    "In the figure we can see the learning curves as a function of $\\gamma$. Each diagram shows the\n",
    "performance (probability of reaching the goal state) as a function of the number of training episodes. The performance is evaluated on an independent testing set of twenty initial robot positions. Note that learning fails\n",
    "completely if  $\\gamma$ > 0.98.\n",
    "\n",
    "Additionally, according to Kamyar Azizzadenesheli et al in the paper [Efficient Exploration through Bayesian Deep Q-Networks](https://arxiv.org/abs/1802.04412), DQN are sensitive to the learning rate and changing it can degrade the performance to even worse than random policy.\n",
    "\n",
    "As we could see in our learning rate experiments, the learning rate of 0.0005 possible entered in the failure region and could not learn at all or it may suffered from the fact that DQN are sensitive to the learning rate.\n",
    "\n",
    "For the discount factor and exploration rate parameters experiments, we found an arbitrairly behavior when determining which agent would learn faster. This aparently lack of correlation between the hyper parameters changes can be caused by the DQN overestimation caracteristic and the average of more than one run could be necessary to see the expected correlation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Thoughts ------------- TODO TODO TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computational cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the biggest problems on this project was the computational cost of running an episode.\n",
    "\n",
    "Since each run plays for 2 minutes and 16 seconds in the original game, there are quite a lot of frames that need to be computed for each episode.\n",
    "Even though the frame-sync is deactivated in our environment, each time we execute on episode it takes about 2 seconds to compute it for Q-Learn and Monte Carlo, but it takes around 21s seconds for SARSA($\\lambda$)! This means that in order to run 4k episodes, one must wait 23 hours. And since we want to experiment with 6 different values of $\\lambda$, it would take about a week to compute this all using a single core processor.\n",
    "\n",
    "The memory usage is fairly low compared to the time that it takes to run the algorithms, even with a decent amount of unique states in our problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding the optimality of our solution, we were able to achive the state-of-the-art score (34.0) some times with the SARSA($\\lambda$) algorithm, but on average we are closer to 31 points (which is also good, having said that our baseline was 21.8.\n",
    "Q-Learning also showed really good results, achieving about 28 points on average.\n",
    "On the other hand, Monte Carlo didn't perform well in our problem, achieving only 13 points on average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regarding the Linear Function Approximators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the experiments realized with function approximation for Monte Carlo, Q Learning and Sarsa Lambda control algorithms, we achieved some meaningful results: \n",
    "\n",
    "* We were not able to improve the agents compared to the algorithms without function approximation. Some factors may have contributed to this, such as the simplicity of the linear approximator for a problem that, probably, have many nonlinear relationships between the environment variables. Also, the features that we created may not be good enough to provide meaningful information about the environment to the agents.\n",
    "\n",
    "* The feature approximation of Monte Carlo said that we have to take a sample for our model in order to update the weight values. However, for our problems, this sample is the entire game, so each new generated sample follows a past policy that makes the convergence slow.\n",
    "\n",
    "* Linear approximators can be faster than its original counterpart as we saw with SARSA, but it’s not always the case.\n",
    "\n",
    "* To add randomness to underfitted models by using a bigger N0 can be beneficial to the solution.\n",
    "\n",
    "* Each algorithm has its own specificities, that depends on how it works. For example, for the Monte Carlo approximator, using a less sparse feature vector contributed for better results. On the other hand, for the Q-Learning and Sarsa Lambda approximators, providing a bigger exploration capacity for the agent was very important to achieve better results.\n",
    "\n",
    "* In all the experiments, the agents converged to a behavior similar to that of the baseline, learning that moving up the most part of the time is the best policy. However, as we saw from the algorithms without approximation, that is not the truth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All in all, we were satisfied with what we achieved, being able to apply multiple tabular methods and experiment a lot on Reinforcement Learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
