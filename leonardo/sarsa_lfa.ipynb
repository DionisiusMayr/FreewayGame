{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "d8aNOyql5UUN"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/content/FreewayGame/leonardo/')\n",
    "%matplotlib inline\n",
    "from collections import defaultdict\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import gym\n",
    "\n",
    "import src.agents as agents\n",
    "import src.episode as episode\n",
    "import src.environment as environment\n",
    "import src.aux_plots as aux_plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "PuE-rwxu5c_y"
   },
   "outputs": [],
   "source": [
    "def print_result(i, scores, total_reward, score):\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Run [{i:4}] - Total reward: {total_reward:7.2f} Mean scores: {sum(scores) / len(scores):.2f} Means Scores[:-10]: {sum(scores[-10:]) / len(scores[-10:]):5.2f} Score: {score:2} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "r8WTU2ji50K_"
   },
   "outputs": [],
   "source": [
    "RAM_mask = [\n",
    "      14  # Chicken Y\n",
    "    , 16  # Chicken Lane Collide\n",
    "    , 108, 109, 110, 111, 112, 113, 114, 115, 116, 117  # Car X Coords\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "vEYk4StW54vp"
   },
   "outputs": [],
   "source": [
    "def reduce_state(ob):\n",
    "    # Doesn't matter where we were hit\n",
    "    ob[16] = 1 if ob[16] != 255 else 0\n",
    "\n",
    "    # Reduce chicken y-position\n",
    "    ob[14] = ob[14] // 3\n",
    "\n",
    "    for b in range(108, 118):\n",
    "        # The chicken is in the x-posistion ~49\n",
    "        if ob[b] < 20 or ob[b] > 80:\n",
    "            # We don't need to represent cars far from the chicken\n",
    "            ob[b] = 0\n",
    "        else:\n",
    "            # Reduce the cars x-positions sample space\n",
    "            ob[b] = ob[b] // 3\n",
    "\n",
    "    return ob\n",
    "    \n",
    "def reward_policy(reward, ob, action):\n",
    "    if reward == 1:\n",
    "        reward = reward_policy.REWARD_IF_CROSS\n",
    "    elif ob[16] == 1:  # Collision!\n",
    "        reward = reward_policy.REWARD_IF_COLISION\n",
    "    elif action != 1:  # Don't incentivate staying still\n",
    "        reward = reward_policy.REWARD_IF_STILL\n",
    "\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "s5xgUrUa6aVe"
   },
   "outputs": [],
   "source": [
    "env, initial_state = environment.get_env()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "dSd6PE756dV8"
   },
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "\n",
    "AVAILABLE_ACTIONS = 2\n",
    "N0 = 2.5\n",
    "ALPHA = 0.001\n",
    "\n",
    "reward_policy.REWARD_IF_CROSS = 1\n",
    "reward_policy.REWARD_IF_COLISION = -1\n",
    "reward_policy.REWARD_IF_STILL = -0.0001\n",
    "\n",
    "# reward_policy.REWARD_IF_CROSS = 500\n",
    "# reward_policy.REWARD_IF_COLISION = -10\n",
    "# reward_policy.REWARD_IF_STILL = -1\n",
    "\n",
    "feat_type = 'all' #mean\n",
    "agent = agents.SarsaLFAADAM(gamma=GAMMA, state_size=len(RAM_mask), available_actions=AVAILABLE_ACTIONS, N0=N0, discount_factor=1, alpha=ALPHA)\n",
    "agent.trainScaler(env, RAM_mask, feat_type=feat_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "O_LAwEiw6vXf"
   },
   "outputs": [],
   "source": [
    "scores = []\n",
    "total_rewards = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eArXBcPQ6zbM",
    "outputId": "3c2a32ba-11b9-4f11-8b63-0ba4149d4ab1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run [   0] - Total reward:  -44.12 Mean scores: 13.00 Means Scores[:-10]: 13.00 Score: 13 \n",
      "Run [  10] - Total reward:  -51.12 Mean scores: 12.27 Means Scores[:-10]: 12.20 Score: 12 \n",
      "Run [  20] - Total reward:  -76.10 Mean scores: 12.33 Means Scores[:-10]: 12.40 Score: 12 \n",
      "Run [  30] - Total reward:  -63.10 Mean scores: 13.03 Means Scores[:-10]: 14.50 Score: 15 \n",
      "Run [  40] - Total reward:  -55.09 Mean scores: 13.63 Means Scores[:-10]: 15.50 Score: 15 \n",
      "Run [  50] - Total reward:  -57.09 Mean scores: 14.18 Means Scores[:-10]: 16.40 Score: 16 \n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "n_runs = 500\n",
    "\n",
    "for i in range(n_runs):\n",
    "    render = i % 200 == 199\n",
    "\n",
    "    E = defaultdict(lambda: np.zeros(2)) # TODO available actions\n",
    "\n",
    "    game_over = False\n",
    "    state = env.reset()\n",
    "    state = reduce_state(state)[RAM_mask].data.tobytes()  # Select useful bytes\n",
    "    action = agent.act(state)\n",
    "    \n",
    "    score = 0\n",
    "    total_reward = 0\n",
    "\n",
    "    while not game_over:\n",
    "        if render:\n",
    "            time.sleep(0.005)\n",
    "            env.render()\n",
    "\n",
    "        old_state = state\n",
    "        old_action = action\n",
    "        ob, reward, game_over, _ = env.step(action)\n",
    "\n",
    "        ob = reduce_state(ob)\n",
    "        reward = reward_policy(reward, ob, action)\n",
    "\n",
    "        total_reward += reward\n",
    "\n",
    "        if reward == reward_policy.REWARD_IF_CROSS:\n",
    "            score += 1\n",
    "\n",
    "        state = ob[RAM_mask].data.tobytes()\n",
    "\n",
    "        action = agent.act(state)  # Next action\n",
    "        \n",
    "        E[old_state][old_action] += 1\n",
    "\n",
    "        agent.update_Q(old_s=old_state, new_s=state, old_a=old_action, new_a=action, reward=reward, E=i+1)\n",
    "\n",
    "    scores.append(score)\n",
    "    total_rewards.append(total_reward)\n",
    "\n",
    "    print_result(i, scores, total_reward, score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_plots.plot_scores(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "sarsa_lfa.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
