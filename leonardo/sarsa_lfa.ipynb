{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sarsa_lfa.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhrLTmre3ml1",
        "outputId": "d1035f2c-40de-4893-84b8-923d9703f9a0"
      },
      "source": [
        "!git clone https://oramleo@github.com/DionisiusMayr/FreewayGame.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'FreewayGame' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjpaGNMU4Hwt"
      },
      "source": [
        "!cp -r FreewayGame/dionisius.mayr/* FreewayGame/leonardo/\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tSSWXd1b4dYU",
        "outputId": "468c9f98-6584-4a93-88c3-b19b93970b76"
      },
      "source": [
        "!pip install gym\n",
        "!pip install gym[atari]\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.17.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.19.4)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n",
            "Requirement already satisfied: gym[atari] in /usr/local/lib/python3.6/dist-packages (0.17.3)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.4.1)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.19.4)\n",
            "Requirement already satisfied: Pillow; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (7.0.0)\n",
            "Requirement already satisfied: atari-py~=0.2.0; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (0.2.6)\n",
            "Requirement already satisfied: opencv-python; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (4.1.2.30)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari]) (0.16.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from atari-py~=0.2.0; extra == \"atari\"->gym[atari]) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8aNOyql5UUN"
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/FreewayGame/leonardo/')\n",
        "%matplotlib inline\n",
        "from collections import defaultdict\n",
        "from typing import List\n",
        "\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import gym\n",
        "\n",
        "import src.agents as agents\n",
        "import src.episode as episode\n",
        "import src.environment as environment\n",
        "import src.aux_plots as aux_plots"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuE-rwxu5c_y"
      },
      "source": [
        "def print_result(i, scores, total_reward, score):\n",
        "    if i % 10 == 0:\n",
        "        print(f\"Run [{i:4}] - Total reward: {total_reward:7.2f} Mean scores: {sum(scores) / len(scores):.2f} Means Scores[:-10]: {sum(scores[-10:]) / len(scores[-10:]):5.2f} Score: {score:2} \")"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8WTU2ji50K_"
      },
      "source": [
        "RAM_mask = [\n",
        "      14  # Chicken Y\n",
        "    , 16  # Chicken Lane Collide\n",
        "    , 108, 109, 110, 111, 112, 113, 114, 115, 116, 117  # Car X Coords\n",
        "]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEYk4StW54vp"
      },
      "source": [
        "def reduce_state(ob):\n",
        "    # Doesn't matter where we were hit\n",
        "    ob[16] = 1 if ob[16] != 255 else 0\n",
        "\n",
        "    # Reduce chicken y-position\n",
        "    ob[14] = ob[14] // 3\n",
        "\n",
        "    for b in range(108, 118):\n",
        "        # The chicken is in the x-posistion ~49\n",
        "        if ob[b] < 20 or ob[b] > 80:\n",
        "            # We don't need to represent cars far from the chicken\n",
        "            ob[b] = 0\n",
        "        else:\n",
        "            # Reduce the cars x-positions sample space\n",
        "            ob[b] = ob[b] // 3\n",
        "\n",
        "    return ob\n",
        "    \n",
        "def reward_policy(reward, ob, action):\n",
        "    if reward == 1:\n",
        "        reward = reward_policy.REWARD_IF_CROSS\n",
        "    elif ob[16] == 1:  # Collision!\n",
        "        reward = reward_policy.REWARD_IF_COLISION\n",
        "    elif action != 1:  # Don't incentivate staying still\n",
        "        reward = reward_policy.REWARD_IF_STILL\n",
        "\n",
        "    return reward"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8O8wi2q6DX3"
      },
      "source": [
        "class SarsaLFA():\n",
        "    def __init__(self, gamma: float, available_actions: int, N0: float, discount_factor: int, alpha: float):\n",
        "        self.gamma = gamma\n",
        "        self.available_actions = available_actions\n",
        "        self.N0 = N0\n",
        "        self.alpha = alpha\n",
        "\n",
        "        self.weights = np.random.rand(2+len(RAM_mask))\n",
        "\n",
        "        self.state_visits = defaultdict(lambda: 0)\n",
        "\n",
        "    def qw(self, state, action):\n",
        "        return np.dot(self.get_features(state, action), self.weights)\n",
        "\n",
        "    def get_features(self, state, action):\n",
        "        return np.append(np.frombuffer(state, dtype=np.uint8, count=-1), [action, 1])\n",
        "\n",
        "    def act(self, state):\n",
        "        epsilon = self.N0 / (self.N0 + self.state_visits[state])\n",
        "\n",
        "        if np.random.choice(np.arange(self.available_actions), p=[1 - epsilon, epsilon]):\n",
        "            action = np.random.choice(self.available_actions)  # Explore!\n",
        "        elif self.state_visits[state] == 0:\n",
        "            action = 1  # Bias toward going forward\n",
        "        else:\n",
        "            action = np.argmax([self.qw(state, act) for act in range(self.available_actions)])\n",
        "\n",
        "        self.state_visits[state] += 1\n",
        "\n",
        "        return action\n",
        "\n",
        "    def update_Q(self, old_s, new_s, old_a, new_a, reward, E):\n",
        "        delta = reward + self.gamma * self.qw(new_s, new_a) - self.qw(old_s, old_a)\n",
        "        self.weights += self.alpha * delta * self.get_features(new_s, action)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5xgUrUa6aVe"
      },
      "source": [
        "env, initial_state = environment.get_env()\n"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSd6PE756dV8"
      },
      "source": [
        "GAMMA = 0.99\n",
        "\n",
        "AVAILABLE_STATES = len(RAM_mask)\n",
        "AVAILABLE_ACTIONS = 2\n",
        "N0 = 2.5\n",
        "ALPHA = 0.000001\n",
        "\n",
        "reward_policy.REWARD_IF_CROSS = 1\n",
        "reward_policy.REWARD_IF_COLISION = -0.5\n",
        "reward_policy.REWARD_IF_STILL = -0.01\n",
        "\n",
        "agent = SarsaLFA(gamma=GAMMA, available_actions=AVAILABLE_ACTIONS, N0=N0, discount_factor=1, alpha=ALPHA)"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_LAwEiw6vXf"
      },
      "source": [
        "\n",
        "scores = []\n",
        "total_rewards = []"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eArXBcPQ6zbM",
        "outputId": "3c2a32ba-11b9-4f11-8b63-0ba4149d4ab1"
      },
      "source": [
        "%%time\n",
        "n_runs = 1000\n",
        "\n",
        "for i in range(n_runs):\n",
        "    render = i % 200 == 199\n",
        "\n",
        "    E = defaultdict(lambda: np.zeros(2)) # TODO available actions\n",
        "\n",
        "    game_over = False\n",
        "    state = env.reset()\n",
        "    state = reduce_state(state)[RAM_mask].data.tobytes()  # Select useful bytes\n",
        "    action = agent.act(state)\n",
        "    \n",
        "    score = 0\n",
        "    total_reward = 0\n",
        "\n",
        "    while not game_over:\n",
        "        if render:\n",
        "            time.sleep(0.005)\n",
        "            env.render()\n",
        "\n",
        "        old_state = state\n",
        "        old_action = action\n",
        "        ob, reward, game_over, _ = env.step(action)\n",
        "\n",
        "        ob = reduce_state(ob)\n",
        "        reward = reward_policy(reward, ob, action)\n",
        "\n",
        "        total_reward += reward\n",
        "\n",
        "        if reward == reward_policy.REWARD_IF_CROSS:\n",
        "            score += 1\n",
        "\n",
        "        state = ob[RAM_mask].data.tobytes()\n",
        "\n",
        "        action = agent.act(state)  # Next action\n",
        "        \n",
        "        E[old_state][old_action] += 1\n",
        "\n",
        "        agent.update_Q(old_s=old_state, new_s=state, old_a=old_action, new_a=action, reward=reward, E=E)\n",
        "\n",
        "    scores.append(score)\n",
        "    total_rewards.append(total_reward)\n",
        "\n",
        "    print_result(i, scores, total_reward, score)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Run [   0] - Total reward:  -32.79 Mean scores: 11.00 Means Scores[:-10]: 11.00 Score: 11 \n",
            "Run [   1] - Total reward:  -33.50 Mean scores: 11.50 Means Scores[:-10]: 11.50 Score: 12 \n",
            "Run [   2] - Total reward:  -37.87 Mean scores: 12.00 Means Scores[:-10]: 12.00 Score: 13 \n",
            "Run [   3] - Total reward:  -32.27 Mean scores: 12.50 Means Scores[:-10]: 12.50 Score: 14 \n",
            "Run [   4] - Total reward:  -28.32 Mean scores: 12.40 Means Scores[:-10]: 12.40 Score: 12 \n",
            "Run [   5] - Total reward:  -31.14 Mean scores: 12.67 Means Scores[:-10]: 12.67 Score: 14 \n",
            "Run [   6] - Total reward:  -36.92 Mean scores: 12.71 Means Scores[:-10]: 12.71 Score: 13 \n",
            "Run [   7] - Total reward:  -40.57 Mean scores: 12.75 Means Scores[:-10]: 12.75 Score: 13 \n",
            "Run [   8] - Total reward:  -31.79 Mean scores: 12.78 Means Scores[:-10]: 12.78 Score: 13 \n",
            "Run [   9] - Total reward:  -34.49 Mean scores: 12.70 Means Scores[:-10]: 12.70 Score: 12 \n",
            "Run [  10] - Total reward:  -25.81 Mean scores: 12.73 Means Scores[:-10]: 12.90 Score: 13 \n",
            "Run [  11] - Total reward:  -36.86 Mean scores: 12.67 Means Scores[:-10]: 12.90 Score: 12 \n",
            "Run [  12] - Total reward:  -26.14 Mean scores: 12.85 Means Scores[:-10]: 13.10 Score: 15 \n",
            "Run [  13] - Total reward:  -26.64 Mean scores: 12.86 Means Scores[:-10]: 13.00 Score: 13 \n",
            "Run [  14] - Total reward:  -33.92 Mean scores: 12.87 Means Scores[:-10]: 13.10 Score: 13 \n",
            "Run [  15] - Total reward:  -23.96 Mean scores: 12.94 Means Scores[:-10]: 13.10 Score: 14 \n",
            "Run [  16] - Total reward:  -40.13 Mean scores: 12.82 Means Scores[:-10]: 12.90 Score: 11 \n",
            "Run [  17] - Total reward:  -26.79 Mean scores: 12.89 Means Scores[:-10]: 13.00 Score: 14 \n",
            "Run [  18] - Total reward:  -33.67 Mean scores: 12.89 Means Scores[:-10]: 13.00 Score: 13 \n",
            "Run [  19] - Total reward:  -31.34 Mean scores: 12.95 Means Scores[:-10]: 13.20 Score: 14 \n",
            "Run [  20] - Total reward:  -35.20 Mean scores: 13.00 Means Scores[:-10]: 13.30 Score: 14 \n",
            "Run [  21] - Total reward:  -15.03 Mean scores: 13.18 Means Scores[:-10]: 13.80 Score: 17 \n",
            "Run [  22] - Total reward:  -33.43 Mean scores: 13.17 Means Scores[:-10]: 13.60 Score: 13 \n",
            "Run [  23] - Total reward:  -29.90 Mean scores: 13.25 Means Scores[:-10]: 13.80 Score: 15 \n",
            "Run [  24] - Total reward:  -21.12 Mean scores: 13.32 Means Scores[:-10]: 14.00 Score: 15 \n",
            "Run [  25] - Total reward:  -16.98 Mean scores: 13.38 Means Scores[:-10]: 14.10 Score: 15 \n",
            "Run [  26] - Total reward:  -14.12 Mean scores: 13.52 Means Scores[:-10]: 14.70 Score: 17 \n",
            "Run [  27] - Total reward:  -22.19 Mean scores: 13.61 Means Scores[:-10]: 14.90 Score: 16 \n",
            "Run [  28] - Total reward:  -19.91 Mean scores: 13.66 Means Scores[:-10]: 15.10 Score: 15 \n",
            "Run [  29] - Total reward:  -28.96 Mean scores: 13.67 Means Scores[:-10]: 15.10 Score: 14 \n",
            "Run [  30] - Total reward:  -16.29 Mean scores: 13.77 Means Scores[:-10]: 15.40 Score: 17 \n",
            "Run [  31] - Total reward:  -24.57 Mean scores: 13.78 Means Scores[:-10]: 15.10 Score: 14 \n",
            "Run [  32] - Total reward:  -25.13 Mean scores: 13.85 Means Scores[:-10]: 15.40 Score: 16 \n",
            "Run [  33] - Total reward:  -30.61 Mean scores: 13.88 Means Scores[:-10]: 15.40 Score: 15 \n",
            "Run [  34] - Total reward:  -38.80 Mean scores: 13.89 Means Scores[:-10]: 15.30 Score: 14 \n",
            "Run [  35] - Total reward:  -27.64 Mean scores: 13.92 Means Scores[:-10]: 15.30 Score: 15 \n",
            "Run [  36] - Total reward:  -22.41 Mean scores: 13.97 Means Scores[:-10]: 15.20 Score: 16 \n",
            "Run [  37] - Total reward:  -24.70 Mean scores: 14.03 Means Scores[:-10]: 15.20 Score: 16 \n",
            "Run [  38] - Total reward:  -25.50 Mean scores: 14.05 Means Scores[:-10]: 15.20 Score: 15 \n",
            "Run [  39] - Total reward:  -27.96 Mean scores: 14.12 Means Scores[:-10]: 15.50 Score: 17 \n",
            "Run [  40] - Total reward:  -18.71 Mean scores: 14.20 Means Scores[:-10]: 15.50 Score: 17 \n",
            "Run [  41] - Total reward:  -22.37 Mean scores: 14.29 Means Scores[:-10]: 15.90 Score: 18 \n",
            "Run [  42] - Total reward:  -24.85 Mean scores: 14.30 Means Scores[:-10]: 15.80 Score: 15 \n",
            "Run [  43] - Total reward:  -26.05 Mean scores: 14.34 Means Scores[:-10]: 15.90 Score: 16 \n",
            "Run [  44] - Total reward:  -27.76 Mean scores: 14.40 Means Scores[:-10]: 16.20 Score: 17 \n",
            "Run [  45] - Total reward:  -22.55 Mean scores: 14.43 Means Scores[:-10]: 16.30 Score: 16 \n",
            "Run [  46] - Total reward:  -26.37 Mean scores: 14.47 Means Scores[:-10]: 16.30 Score: 16 \n",
            "Run [  47] - Total reward:  -14.79 Mean scores: 14.56 Means Scores[:-10]: 16.60 Score: 19 \n",
            "Run [  48] - Total reward:  -28.00 Mean scores: 14.61 Means Scores[:-10]: 16.80 Score: 17 \n",
            "Run [  49] - Total reward:  -18.74 Mean scores: 14.68 Means Scores[:-10]: 16.90 Score: 18 \n",
            "Run [  50] - Total reward:  -18.46 Mean scores: 14.73 Means Scores[:-10]: 16.90 Score: 17 \n",
            "Run [  51] - Total reward:  -11.50 Mean scores: 14.83 Means Scores[:-10]: 17.10 Score: 20 \n",
            "Run [  52] - Total reward:  -38.87 Mean scores: 14.81 Means Scores[:-10]: 17.00 Score: 14 \n",
            "Run [  53] - Total reward:  -11.48 Mean scores: 14.89 Means Scores[:-10]: 17.30 Score: 19 \n",
            "Run [  54] - Total reward:  -21.56 Mean scores: 14.93 Means Scores[:-10]: 17.30 Score: 17 \n",
            "Run [  55] - Total reward:  -16.53 Mean scores: 15.00 Means Scores[:-10]: 17.60 Score: 19 \n",
            "Run [  56] - Total reward:  -21.78 Mean scores: 15.07 Means Scores[:-10]: 17.90 Score: 19 \n",
            "Run [  57] - Total reward:  -13.37 Mean scores: 15.16 Means Scores[:-10]: 18.00 Score: 20 \n",
            "Run [  58] - Total reward:  -31.87 Mean scores: 15.19 Means Scores[:-10]: 18.00 Score: 17 \n",
            "Run [  59] - Total reward:  -26.25 Mean scores: 15.22 Means Scores[:-10]: 17.90 Score: 17 \n",
            "Run [  60] - Total reward:  -16.07 Mean scores: 15.28 Means Scores[:-10]: 18.10 Score: 19 \n",
            "Run [  61] - Total reward:  -30.78 Mean scores: 15.27 Means Scores[:-10]: 17.60 Score: 15 \n",
            "Run [  62] - Total reward:  -28.45 Mean scores: 15.29 Means Scores[:-10]: 17.80 Score: 16 \n",
            "Run [  63] - Total reward:  -10.12 Mean scores: 15.36 Means Scores[:-10]: 17.90 Score: 20 \n",
            "Run [  64] - Total reward:  -15.74 Mean scores: 15.40 Means Scores[:-10]: 18.00 Score: 18 \n",
            "Run [  65] - Total reward:   -6.38 Mean scores: 15.50 Means Scores[:-10]: 18.30 Score: 22 \n",
            "Run [  66] - Total reward:  -32.47 Mean scores: 15.51 Means Scores[:-10]: 18.00 Score: 16 \n",
            "Run [  67] - Total reward:  -14.20 Mean scores: 15.57 Means Scores[:-10]: 18.00 Score: 20 \n",
            "Run [  68] - Total reward:  -21.69 Mean scores: 15.61 Means Scores[:-10]: 18.10 Score: 18 \n",
            "Run [  69] - Total reward:  -19.76 Mean scores: 15.64 Means Scores[:-10]: 18.20 Score: 18 \n",
            "Run [  70] - Total reward:  -18.97 Mean scores: 15.68 Means Scores[:-10]: 18.10 Score: 18 \n",
            "Run [  71] - Total reward:  -23.30 Mean scores: 15.68 Means Scores[:-10]: 18.20 Score: 16 \n",
            "Run [  72] - Total reward:  -17.87 Mean scores: 15.70 Means Scores[:-10]: 18.30 Score: 17 \n",
            "Run [  73] - Total reward:  -21.98 Mean scores: 15.73 Means Scores[:-10]: 18.10 Score: 18 \n",
            "Run [  74] - Total reward:  -20.24 Mean scores: 15.76 Means Scores[:-10]: 18.10 Score: 18 \n",
            "Run [  75] - Total reward:  -23.62 Mean scores: 15.79 Means Scores[:-10]: 17.70 Score: 18 \n",
            "Run [  76] - Total reward:  -21.39 Mean scores: 15.82 Means Scores[:-10]: 17.90 Score: 18 \n",
            "Run [  77] - Total reward:  -21.44 Mean scores: 15.83 Means Scores[:-10]: 17.60 Score: 17 \n",
            "Run [  78] - Total reward:  -25.01 Mean scores: 15.86 Means Scores[:-10]: 17.60 Score: 18 \n",
            "Run [  79] - Total reward:  -17.33 Mean scores: 15.90 Means Scores[:-10]: 17.70 Score: 19 \n",
            "Run [  80] - Total reward:  -11.56 Mean scores: 15.95 Means Scores[:-10]: 17.90 Score: 20 \n",
            "Run [  81] - Total reward:  -17.50 Mean scores: 15.98 Means Scores[:-10]: 18.10 Score: 18 \n",
            "Run [  82] - Total reward:  -20.32 Mean scores: 15.99 Means Scores[:-10]: 18.10 Score: 17 \n",
            "Run [  83] - Total reward:  -18.17 Mean scores: 16.02 Means Scores[:-10]: 18.20 Score: 19 \n",
            "Run [  84] - Total reward:  -25.79 Mean scores: 16.02 Means Scores[:-10]: 18.00 Score: 16 \n",
            "Run [  85] - Total reward:  -20.72 Mean scores: 16.06 Means Scores[:-10]: 18.10 Score: 19 \n",
            "Run [  86] - Total reward:  -18.43 Mean scores: 16.10 Means Scores[:-10]: 18.30 Score: 20 \n",
            "Run [  87] - Total reward:  -24.71 Mean scores: 16.11 Means Scores[:-10]: 18.30 Score: 17 \n",
            "Run [  88] - Total reward:  -25.82 Mean scores: 16.12 Means Scores[:-10]: 18.20 Score: 17 \n",
            "Run [  89] - Total reward:  -25.89 Mean scores: 16.14 Means Scores[:-10]: 18.10 Score: 18 \n",
            "Run [  90] - Total reward:  -20.24 Mean scores: 16.14 Means Scores[:-10]: 17.70 Score: 16 \n",
            "Run [  91] - Total reward:  -29.50 Mean scores: 16.16 Means Scores[:-10]: 17.70 Score: 18 \n",
            "Run [  92] - Total reward:  -32.11 Mean scores: 16.15 Means Scores[:-10]: 17.50 Score: 15 \n",
            "Run [  93] - Total reward:  -26.73 Mean scores: 16.16 Means Scores[:-10]: 17.30 Score: 17 \n",
            "Run [  94] - Total reward:  -13.21 Mean scores: 16.20 Means Scores[:-10]: 17.70 Score: 20 \n",
            "Run [  95] - Total reward:  -23.46 Mean scores: 16.23 Means Scores[:-10]: 17.70 Score: 19 \n",
            "Run [  96] - Total reward:  -11.16 Mean scores: 16.28 Means Scores[:-10]: 17.80 Score: 21 \n",
            "Run [  97] - Total reward:  -20.14 Mean scores: 16.30 Means Scores[:-10]: 17.90 Score: 18 \n",
            "Run [  98] - Total reward:  -11.87 Mean scores: 16.33 Means Scores[:-10]: 18.20 Score: 20 \n",
            "Run [  99] - Total reward:  -10.91 Mean scores: 16.37 Means Scores[:-10]: 18.40 Score: 20 \n",
            "CPU times: user 4min 46s, sys: 164 ms, total: 4min 46s\n",
            "Wall time: 4min 47s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OGuOiYf6z1V"
      },
      "source": [
        "!git commit -"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}