{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01-q-learning-approximator.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "-PsCgC_nCmcm",
        "TS9P2qo0CyR_"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBtuBSIP5xCf"
      },
      "source": [
        "## 1. Import Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGGjwvDI3eIc",
        "outputId": "15b8529f-de00-4007-e0fb-eadec5a04f7e"
      },
      "source": [
        "# !rm -rf FreewayGame/\n",
        "!git clone https://oramleo@github.com/DionisiusMayr/FreewayGame.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'FreewayGame'...\n",
            "remote: Enumerating objects: 86, done.\u001b[K\n",
            "remote: Counting objects: 100% (86/86), done.\u001b[K\n",
            "remote: Compressing objects: 100% (56/56), done.\u001b[K\n",
            "remote: Total 367 (delta 41), reused 68 (delta 29), pack-reused 281\u001b[K\n",
            "Receiving objects: 100% (367/367), 70.79 MiB | 21.66 MiB/s, done.\n",
            "Resolving deltas: 100% (166/166), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzH1qNXp6C9n"
      },
      "source": [
        "## 2. Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpsIFliN54ks"
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/FreewayGame/marianna/')\n",
        "\n",
        "%matplotlib inline\n",
        "from collections import defaultdict\n",
        "from typing import List\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import time\n",
        "from google.colab import drive\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import gym\n",
        "\n",
        "import src.agents as agents\n",
        "import src.episode as episode\n",
        "import src.environment as environment\n",
        "import src.aux_plots as aux_plots\n",
        "import src.serializer as serializer"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufcmlsSxVsdw",
        "outputId": "da2ac15d-c7c4-44d0-8f94-2004cb6d21e1"
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2crDDCeiV14H"
      },
      "source": [
        "modelSavePath = '/content/drive/MyDrive/02-second-semester-unicamp/02-reinforcement-learning/01-project01-rl/models/'\n",
        "scoresSavePath = '/content/drive/MyDrive/02-second-semester-unicamp/02-reinforcement-learning/01-project01-rl/scores/'"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-zxVnd2CKZu"
      },
      "source": [
        "## 3. Auxiliary functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ufXSAy8CNSD"
      },
      "source": [
        "### 3.1. Reduce state space"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GhdhphDCGIj"
      },
      "source": [
        "def reduce_state(ob):\n",
        "    # Doesn't matter where we were hit\n",
        "    ob[16] = 1 if ob[16] != 255 else 0\n",
        "\n",
        "    # Reduce chicken y-position\n",
        "    ob[14] = ob[14] // 3\n",
        "\n",
        "    for b in range(108, 118):\n",
        "        # The chicken is in the x-posistion ~49\n",
        "        if ob[b] < 20 or ob[b] > 80:\n",
        "            # We don't need to represent cars far from the chicken\n",
        "            ob[b] = 0\n",
        "        else:\n",
        "            # Reduce the cars x-positions sample space\n",
        "            ob[b] = ob[b] // 3\n",
        "\n",
        "    return ob"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwXkr2pLCRtf"
      },
      "source": [
        "### 3.2. Reward policy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuHXaUZNCPl4"
      },
      "source": [
        "def reward_policy(reward, ob, action):\n",
        "    if reward == 1:\n",
        "        reward = reward_policy.REWARD_IF_CROSS\n",
        "    elif ob[16] == 1:  # Collision!\n",
        "        reward = reward_policy.REWARD_IF_COLISION\n",
        "    elif action != 1:  # Don't incentivate staying still\n",
        "        reward = reward_policy.REWARD_IF_STILL\n",
        "\n",
        "    return reward"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzcLPXN_CYuS"
      },
      "source": [
        "### 3.3. Print results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUkE1NRYCVYA"
      },
      "source": [
        "def print_result(i, scores, total_reward, score):\n",
        "#     if i % 10 == 0:\n",
        "        print(f\"Run [{i:4}] - Total reward: {total_reward:7.2f} Mean scores: {sum(scores) / len(scores):.2f} Means Scores[:-10]: {sum(scores[-10:]) / len(scores[-10:]):5.2f} Score: {score:2} \")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12re5YhdCfEK"
      },
      "source": [
        "## 4. Running algorithms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGvaDtfACh_d"
      },
      "source": [
        "### 4.1. Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCbmenOXCbFK"
      },
      "source": [
        "RAM_mask = [\n",
        "      14  # Chicken Y\n",
        "    , 16  # Chicken Lane Collide\n",
        "    , 108, 109, 110, 111, 112, 113, 114, 115, 116, 117  # Car X Coords\n",
        "]\n",
        "\n",
        "ALPHA = 0.00001\n",
        "GAMMA = 0.99\n",
        "AVAILABLE_ACTIONS = 2\n",
        "N0 = 2.5\n",
        "\n",
        "reward_policy.REWARD_IF_CROSS = 50\n",
        "reward_policy.REWARD_IF_COLISION = -1\n",
        "reward_policy.REWARD_IF_STILL = -0.1"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PsCgC_nCmcm"
      },
      "source": [
        "### 4.2. Monte Carlo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1R30OXJFCkQt"
      },
      "source": [
        "env, initial_state = environment.get_env()\n",
        "\n",
        "mc_agent = agents.MonteCarloControl(gamma=GAMMA, available_actions=AVAILABLE_ACTIONS, N0=N0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atMkoiI5Co4e"
      },
      "source": [
        "def MonteCarloES(agent, reduce_s, reward_p, RAM_mask: List[int], render: bool=False):\n",
        "    epi = episode.generate_episode(env, reduce_state=reduce_s, reward_policy=reward_p, agent=agent, RAM_mask=RAM_mask, render=render)\n",
        "    return agent.update_policy(epi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXY_N5hZCqwg",
        "outputId": "959e8017-4f88-4cf6-e055-eaffd90da611"
      },
      "source": [
        "%%time\n",
        "scores = []\n",
        "total_rewards = []\n",
        "\n",
        "\n",
        "n_runs = 5\n",
        "\n",
        "for i in range(n_runs):\n",
        "    render = i % 201 == 200\n",
        "\n",
        "    score, total_reward = MonteCarloES(agent=mc_agent,reduce_s=reduce_state, reward_p=reward_policy, RAM_mask=RAM_mask, render=render)\n",
        "\n",
        "    scores.append(score)\n",
        "    total_rewards.append(total_reward)\n",
        "\n",
        "    print_result(i, scores, total_reward, score)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Run [   0] - Total reward:  297.50 Mean scores: 10.00 Means Scores[:-10]: 10.00 Score: 10 \n",
            "Run [   1] - Total reward:  406.80 Mean scores: 11.00 Means Scores[:-10]: 11.00 Score: 12 \n",
            "Run [   2] - Total reward:  391.80 Mean scores: 11.33 Means Scores[:-10]: 11.33 Score: 12 \n",
            "Run [   3] - Total reward:  347.60 Mean scores: 11.25 Means Scores[:-10]: 11.25 Score: 11 \n",
            "Run [   4] - Total reward:  459.00 Mean scores: 11.60 Means Scores[:-10]: 11.60 Score: 13 \n",
            "CPU times: user 13.1 s, sys: 13.4 ms, total: 13.1 s\n",
            "Wall time: 13.1 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TS9P2qo0CyR_"
      },
      "source": [
        "### 4.3. Q Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfTcL2tFCsXY"
      },
      "source": [
        "env, initial_state = environment.get_env()\n",
        "ql_agent = agents.QLearning(gamma=GAMMA, available_actions=AVAILABLE_ACTIONS, N0=N0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSut91RuC0oN",
        "outputId": "6d683ebf-0bcc-45b1-e72d-75950f6d29bd"
      },
      "source": [
        "%%time\n",
        "scores = []\n",
        "total_rewards = []\n",
        "\n",
        "n_runs = 1\n",
        "render = False\n",
        "for i in range(n_runs):\n",
        "#     render = i % 200 == 0\n",
        "\n",
        "    game_over = False\n",
        "    state = env.reset()\n",
        "    state = reduce_state(state)[RAM_mask].data.tobytes()  # Select useful bytes\n",
        "    action = ql_agent.act(state)\n",
        "    \n",
        "    score = 0\n",
        "    total_reward = 0\n",
        "\n",
        "    while not game_over:\n",
        "        if render:\n",
        "            time.sleep(0.025)\n",
        "            env.render()\n",
        "\n",
        "        old_state = state\n",
        "        ob, reward, game_over, _ = env.step(action)\n",
        "\n",
        "        ob = reduce_state(ob)\n",
        "        reward = reward_policy(reward, ob, action)\n",
        "\n",
        "        total_reward += reward\n",
        "\n",
        "        if reward == reward_policy.REWARD_IF_CROSS:\n",
        "            score += 1\n",
        "\n",
        "        state = ob[RAM_mask].data.tobytes()\n",
        "\n",
        "        ql_agent.update_Q(old_state, state, action, reward)\n",
        "\n",
        "        action = ql_agent.act(state)  # Next action\n",
        "\n",
        "    scores.append(score)\n",
        "    total_rewards.append(total_reward)\n",
        "\n",
        "    print_result(i, scores, total_reward, score)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Run [   0] - Total reward:  392.20 Mean scores: 12.00 Means Scores[:-10]: 12.00 Score: 12 \n",
            "CPU times: user 2.61 s, sys: 2.66 ms, total: 2.61 s\n",
            "Wall time: 2.62 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0KGUM37C7G_"
      },
      "source": [
        "### 4.4. Q Learning Approximator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKFVWr2ZC2pl"
      },
      "source": [
        "feat_type = 'all' #mean\n",
        "fixed_alpha = True #False\n",
        "weights_length = len(RAM_mask) #4\n",
        "\n",
        "env, initial_state = environment.get_env()\n",
        "ql_agent_app = agents.QLearningLinearApprox(alpha=ALPHA, gamma=GAMMA, available_actions=AVAILABLE_ACTIONS, N0=N0, weights_length=weights_length)\n",
        "ql_agent_app.trainScaler(env, RAM_mask, feat_type=feat_type)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f110bhOAC_P0",
        "outputId": "411db745-110f-4d79-f68c-c712bdc48537"
      },
      "source": [
        "%%time\n",
        "scores = []\n",
        "total_rewards = []\n",
        "\n",
        "n_runs = 6000\n",
        "render = False\n",
        "for i in range(1,n_runs+1):\n",
        "#     render = i % 200 == 0\n",
        "\n",
        "    game_over = False\n",
        "    state = env.reset()\n",
        "    state = reduce_state(state)[RAM_mask].data.tobytes()  # Select useful bytes\n",
        "    action = ql_agent_app.act(state, feat_type=feat_type)\n",
        "    \n",
        "    score = 0\n",
        "    total_reward = 0\n",
        "\n",
        "    print(\"Episode \",i)\n",
        "    while not game_over:\n",
        "        if render:\n",
        "            time.sleep(0.025)\n",
        "            env.render()\n",
        "\n",
        "        old_state = state\n",
        "        ob, reward, game_over, _ = env.step(action)\n",
        "\n",
        "        ob = reduce_state(ob)\n",
        "        reward = reward_policy(reward, ob, action)\n",
        "\n",
        "        total_reward += reward\n",
        "\n",
        "        if reward == reward_policy.REWARD_IF_CROSS:\n",
        "            score += 1\n",
        "\n",
        "        state = ob[RAM_mask].data.tobytes()\n",
        "        ql_agent_app.update_W(old_state, state, action, reward, fixed_alpha=fixed_alpha, feat_type=feat_type)\n",
        "        action = ql_agent_app.act(state, feat_type=feat_type)  # Next action\n",
        "\n",
        "        # print('------------------------------------')\n",
        "    scores.append(score)\n",
        "    total_rewards.append(total_reward)\n",
        "    print_result(i, scores, total_reward, score)\n",
        "\n",
        "    if i % 500 == 0:\n",
        "      exp = serializer.Experiment(ql_agent_app, scores, total_rewards, reduce_state, reward_policy)\n",
        "      exp.save_experiment(modelSavePath,'model_ql_approximator_'+str(i))\n",
        "      print(\"Model saved!\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode  1\n",
            "Run [   1] - Total reward:  465.20 Mean scores: 13.00 Means Scores[:-10]: 13.00 Score: 13 \n",
            "Episode  2\n",
            "Run [   2] - Total reward:  473.00 Mean scores: 13.00 Means Scores[:-10]: 13.00 Score: 13 \n",
            "Model saved!\n",
            "Score Saved\n",
            "Episode  3\n",
            "Run [   3] - Total reward:  523.80 Mean scores: 13.33 Means Scores[:-10]: 13.33 Score: 14 \n",
            "Episode  4\n",
            "Run [   4] - Total reward:  407.20 Mean scores: 13.00 Means Scores[:-10]: 13.00 Score: 12 \n",
            "Model saved!\n",
            "Score Saved\n",
            "CPU times: user 18.9 s, sys: 38.7 ms, total: 19 s\n",
            "Wall time: 19.1 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUIhIYgZKefV",
        "outputId": "29fe8bdd-c302-4597-c602-9f441a14ab22"
      },
      "source": [
        "# with open('/content/q_learning_app_fix_2000.dill','wb') as f:\n",
        "#    dill.dump(obj=ql_agent_app, file=f)\n",
        "\n",
        "# with open('/content/q_learning_app_fix_2000.dill', \"rb\") as f:\n",
        "#   teste = dill.load(file=f)\n",
        "\n",
        "# teste.W == ql_agent_app.W"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "        True,  True,  True,  True,  True])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNa6KtwFa2iP"
      },
      "source": [
        "# aux_plots.plot_scores(scores)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgRZP6P-bWqM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}