{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/DionisiusMayr/FreewayGame/blob/main/aline.almeida/DQN/aa_DQN_freeway.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uRj5nWuM6PhK"
   },
   "source": [
    "# Freeway"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UKMotWp56PhY"
   },
   "source": [
    "This is the **second project** for the MC935rA/MO436A - Reinforcement Learning course, taught by Prof. Esther Colombini.\n",
    "\n",
    "In this project we propose to apply Deep Reinforcement Learning methods to teach an agent how to play the Freeway Atari game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "exKbCcw26Pha"
   },
   "source": [
    "**Group members:**\n",
    "- Aline Gabriel de Almeida\n",
    "- Dionisius Oliveira Mayr (229060)\n",
    "- Leonardo de Oliveira Ramos (171941)\n",
    "- Marianna de Pinho Severo (264960)\n",
    "- Victor Jesús Sotelo Chico (265173)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LmyFZKMI6Phb"
   },
   "source": [
    "## Freeway game\n",
    "\n",
    "<center><img src=https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/main/freeway/img/Freeway_logo.png>\n",
    "</center>\n",
    "\n",
    "Freeway is a video game written by David Crane for the Atari 2600 and published by Activision [[1]](https://en.wikipedia.org/wiki/Freeway_(video_game)).\n",
    "\n",
    "In the game, two players compete against each other trying to make their chickens cross the street, while evading the cars passing by.\n",
    "There are three possible actions: staying still, moving forward or moving backward.\n",
    "Each time a chicken collides with a car, it is forced back some spaces and takes a while until the chicken regains its control.\n",
    "\n",
    "When a chicken is successfully guided across the freeway, it is awarded one point and moved to the initial space, where it will try to cross the street again.\n",
    "The game offers multiple scenarios with different vehicles configurations (varying the type, frequency and speed of them) and plays for 2 minutes and 16 seconds.\n",
    "During the 8 last seconds the scores will start blinking to indicate that the game is close to end.\n",
    "Whoever has the most points after this, wins the game!\n",
    "\n",
    "The image was extracted from the [manual of the game](https://www.gamesdatabase.org/Media/SYSTEM/Atari_2600/Manual/formated/Freeway_-_1981_-_Zellers.pdf).\n",
    "\n",
    "[1 - Wikipedia - Freeway](https://en.wikipedia.org/wiki/Freeway_(video_game))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8oz1wIiA6Phc"
   },
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "okeePgwz6Phd"
   },
   "source": [
    "We will be using the [Stable Baselines](https://stable-baselines.readthedocs.io/en/master/index.html) toolkit.\n",
    "This toolkit is a set of improved implementations of Reinforcement LEarning algorithms based on [OpenAI Baselines](https://github.com/openai/baselines).\n",
    "\n",
    "Although the game offers multiple scenarios, we are going to consider only the first one. Also, we will be controlling a *single chicken*, while we try to maximize its score.\n",
    "\n",
    "In this configuration, there are ten lanes and each lane contains exactly one car (with a different speed and direction).\n",
    "Whenever an action is chosen, it is repeated for $k$ frames, $k \\in \\{2, 3, 4\\}$.\n",
    "\n",
    "This means that our environment is **stochastic** and it is also **episodic**, with its terminal state being reached whenever 2 minutes and 16 seconds have passed.\n",
    "\n",
    "You can find more information regarding the environment used at [Freeway-ram-v0](https://gym.openai.com/envs/Freeway-v0/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "abh0tSP06Phe"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EzsgJhjl6Phe"
   },
   "source": [
    "Install the dependencies:\n",
    "```sh\n",
    "pip install -r requirements.txt\n",
    "pip install stable-baselines\n",
    "pip install tensorflow==1.15.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qRlNBUOE6Phf"
   },
   "source": [
    "# Useful Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GUeKMfnq6Phf"
   },
   "source": [
    "Here you can find a list of useful links and materials that were used during this project.\n",
    "\n",
    "* [Freeway-ram-v0 from OpenAI Gym](https://gym.openai.com/envs/Freeway-ram-v0/)\n",
    "* [Manual of the game](https://www.gamesdatabase.org/Media/SYSTEM/Atari_2600/Manual/formated/Freeway_-_1981_-_Zellers.pdf)\n",
    "* [Freeway Disassembly](http://www.bjars.com/disassemblies.html)\n",
    "* [Atari Ram Annotations](https://github.com/mila-iqia/atari-representation-learning/blob/master/atariari/benchmark/ram_annotations.py)\n",
    "* [Freeway Benchmarks](https://paperswithcode.com/sota/atari-games-on-atari-2600-freeway)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5iSwfr-M6Phg"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ayz1Hcu6Phg"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')  # Enable importing from `src` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1cjd1pUS6Phh"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import statistics\n",
    "from collections import defaultdict\n",
    "from functools import lru_cache\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import src.agents as agents\n",
    "import src.episode as episode\n",
    "import src.environment as environment\n",
    "import src.aux_plots as aux_plots\n",
    "import src.serializer as serializer\n",
    "import src.gif as gif\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import gym\n",
    "from stable_baselines.common import make_vec_env\n",
    "from stable_baselines.common.cmd_util import make_atari_env\n",
    "from stable_baselines.common.vec_env import VecFrameStack\n",
    "from stable_baselines.common.atari_wrappers import make_atari\n",
    "\n",
    "from stable_baselines.deepq.policies import CnnPolicy\n",
    "from stable_baselines import DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6sa_1jGr6Phi"
   },
   "outputs": [],
   "source": [
    "def print_result(i, scores, total_reward, score):\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Run [{i:4}] - Total reward: {total_reward:7.2f} Mean scores: {sum(scores) / len(scores):.2f} Means Scores[:-10]: {sum(scores[-10:]) / len(scores[-10:]):5.2f} Score: {score:2} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "595kmf0y6Phj"
   },
   "outputs": [],
   "source": [
    "def read_int_array_from_file(fn: str):\n",
    "    with open(f\"./experiments/{fn}\") as f:\n",
    "        return [int(x) for x in f.read().splitlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_by9we2w6Phj"
   },
   "source": [
    "# Action space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BA0HBFbt6Phj"
   },
   "source": [
    "As we said above, the agent in this game has three possible actions at each frame, each represented by an integer:\n",
    "\n",
    "* 0: Stay\n",
    "* 1: Move forward\n",
    "* 2: Move backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3tkfPTiC6Phk"
   },
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clJkwoAG6Phk"
   },
   "source": [
    "## State of the art benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6-wLm9gz6Phk"
   },
   "source": [
    "The image bellow (extracted from https://paperswithcode.com/sota/atari-games-on-atari-2600-freeway) shows the evolution of the scores over time using different techniques.\n",
    "\n",
    "Today, the state of the art approaches are making 34.0 points, using Deep Reinforcement Learning methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e-p9lBOP6Phl"
   },
   "source": [
    "<center><img src=https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/47d8d1fd3b921471738b30b5f9ae447593705b09/freeway/img/state_of_art_scores.png>\n",
    "</center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wskzQX8N6Phl"
   },
   "source": [
    "## Simple baseline agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VgO5OARX6Phm"
   },
   "source": [
    "As a simple baseline, we are using an agent that always moves **up**, regardless of the rewards received or the current state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZZNYJOaO6Phn"
   },
   "outputs": [],
   "source": [
    "env, initial_state = environment.get_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bM3XH1kp6Pho"
   },
   "outputs": [],
   "source": [
    "agent = agents.Baseline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R1CFZpBp6Pho"
   },
   "outputs": [],
   "source": [
    "total_rewards = []\n",
    "n_runs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u9N0N3Qb6Pho",
    "outputId": "418e1f45-3b7e-4260-a862-f7936c993c51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.5 s, sys: 544 ms, total: 22.1 s\n",
      "Wall time: 50 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in range(n_runs):\n",
    "    render = i % 10 == 0\n",
    "\n",
    "    game_over = False\n",
    "    state = env.reset()\n",
    "    action = agent.act(state)\n",
    "\n",
    "    total_reward = 0\n",
    "\n",
    "    while not game_over:\n",
    "        if render:\n",
    "            time.sleep(0.01)\n",
    "            env.render()\n",
    "\n",
    "        ob, reward, game_over, _ = env.step(action)\n",
    "\n",
    "        total_reward += reward\n",
    "        action = agent.act(state)  # Next action\n",
    "\n",
    "    total_rewards.append(total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YlGDKdlX6Phr",
    "outputId": "7f5f0335-342a-4a81-e9b0-0b9a4eb93a35"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[23.0, 21.0, 23.0, 21.0, 21.0, 21.0, 23.0, 21.0, 23.0, 21.0]"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0AYe3QAg6Phs",
    "outputId": "22e23242-3e5a-4245-fa37-61d5f0fa58ff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.8"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_mean_score = np.mean(total_rewards)\n",
    "baseline_mean_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4tkJKCMe6Pht"
   },
   "source": [
    "As we can see, this agent usually scores 21 or 23 points (as shown in the images bellow). It depends on the the values of $k$ sampled, and on average it scores about 21.8 points per run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T2kdIiIK6Phu"
   },
   "source": [
    "<center><img src=https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/47d8d1fd3b921471738b30b5f9ae447593705b09/freeway/img/baseline_1.png>\n",
    "</center>\n",
    "\n",
    "<center><img src=https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/47d8d1fd3b921471738b30b5f9ae447593705b09/freeway/img/baseline_2.png>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OEY3MfGB6Phu"
   },
   "source": [
    "# State Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iMBFgeor6Phv"
   },
   "source": [
    "Since the tabular methods we are going to use work with some representation of the actual environment state, we will need to understand it better in order to effectively approach this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "86j84qlA6Phv"
   },
   "source": [
    "## Atari 2600"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vMyZP5rt6Phv"
   },
   "source": [
    "Before talking about the state representation, it is important to understand how the Atari 2600 works.\n",
    "\n",
    "Atari 2600 is a video game released in 1977 by the American Atari, Inc.\n",
    "Its **8-bit** microprocessor was of the MOS **6502** family and it had **128 bytes** of RAM.\n",
    "\n",
    "And these 128 bytes are what really matters here.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bpZ-PQ-16Phw"
   },
   "source": [
    "Recall that Gym gives us the RAM memory of the Atari as the state representation.\n",
    "In other words, it gives us an 128-element `np.array`, where each element of the array is an `uint8` (*integer values ranging from 0 to 255*).\n",
    "\n",
    "That said, we have (in theory) $256^{128} \\approx 1.8 \\cdot 10^{308}$ possible game states!\n",
    "\n",
    "This is *far* from being manageable, and thus we need to come up with a different approch to represent our state if we want our algorithms to converge.\n",
    "\n",
    "One might argue that the RAM state is *sparse* and although that is true, it is still not sparse enough to apply tabular methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ji7ogEuc6Ph3"
   },
   "source": [
    "# Reward Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ljfENGRz6Ph3"
   },
   "source": [
    "In the base environment we are awarded on point each time we successfully cross the freeway."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ucF4x5xK6Ph5"
   },
   "source": [
    "# Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3VQBJpkZ6Ph5"
   },
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 0.0005\n",
    "EXPLORATION_RATE = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e6BzspnJ6Ph6"
   },
   "source": [
    "# Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t36obrBM6Ph6"
   },
   "source": [
    "Since it takes a lot of time to train the models, we won't train them all in this report.\n",
    "Instead, we will be load the results of our simulations and specifying the parameters used to obtain those results.\n",
    "Of course, it is possible to reproduce our results simply by running the algorithms here using the same hyper parameters as specified.\n",
    "\n",
    "Whenever possible, we will be adding plots comparing different approaches and parameters, as well as adding gifs in this notebook so that we can visualize the development of the agent and unique strategies that they learned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_rZEku736Ph6"
   },
   "source": [
    "Also, we focused a lot of our experiments on Q-Learning, since it was showing the most promissor results.\n",
    "Monte Carlo methods didn't really work out, and SARSA($\\lambda$) methods took way too much time to run (roughly 12 hours per 2k iterations!).\n",
    "Since QLearning and SARSA aren't really that different, we applied most of the knowledge we acquired from the QLearning experiments on SARSA, varying only its unique parameter, $\\lambda$, in steps of 0.2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "poesED2Q6Ph6"
   },
   "source": [
    "Also, it is worth mentioning that we left the code used by each agent inside `./src/agents.py` and provided a model of implementing the environment along the notebook, with the `n_runs` parameter (that controls the number of episodes used in to train the algorithm) set to `1`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9uuS4JS46Ph7"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review of Project 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tabular Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Function Approximation\n",
    "\n",
    "In Project 1, we also worked with linear function approximators, which were based on the Monte Carlo, Q-Learning and Sarsa($\\lambda$) algorithms. For each of them, we experimented with different sets of features to represent the state-action pairs, varied reward functions, different exploration rates, and in all of them we used only two actions (move backward and move forward), which were the best actions found by the tabular methods.\n",
    "\n",
    "All function approximators obtained results close to those of the baseline, being slightly better. Of them, the Monte Carlo based approximator was the only one that managed to improve in relation to its tabular version, reaching an average score of 22.2, compared to the 13 points obtained by the latter.\n",
    "\n",
    "Throughout the experiments, we observed that some linear approximators can be faster than the original algorithms, as happened with Sarsa($\\lambda$). In addition, we saw that the Q-Learning and Sarsa($\\lambda$) approximators achieved better results when they were given greater exploration capacity, and that the Monte Carlo based approximator benefited when less sparse feature vectors were adopted.\n",
    "\n",
    "Given that, although linear function approximators are powerful tools, they were not good enough to improve the performance of our solutions, either due to the nature of the problem, the possible poor quality of the features we created or other factors that were not addressed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zfA13AxhB0zJ"
   },
   "source": [
    "# DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hQHaobajDecf"
   },
   "source": [
    "The [Deep-Q-Network](https://arxiv.org/pdf/1312.5602.pdf) is a deep learning model that learns to control policies directly from high dimensional sensory using reinforcement learning.  \n",
    "\n",
    "The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating the future rewards.  \n",
    "\n",
    "We applied the [algorithm implemented by Stable Baselines](https://stable-baselines.readthedocs.io/en/master/modules/dqn.html) to the Atari Freeway game.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-m6j7vatDej1"
   },
   "source": [
    "The Deep-Q-Network algorithm observes the image $x_t$ from the emulator which is a vector of raw pixel values representing the current screen. In addition it receives a reward $r_t$ representing the change in game score.\n",
    "\n",
    "It considers sequences of actions and observations, $s_t = x_1, a_1, x_2, ... a_{t-1}, x_t$, and learn game strategies that depend upon these sequences.  \n",
    "\n",
    "All sequences in the emulator are assumed to terminate in a finite number of time-steps. This formalism gives rise to a large but finite Markov decision process (MDP) in which each sequence is a distinct state.  \n",
    "\n",
    "As a result, we can apply standard reinforcement learning methods for MDPs, simply by using the complete sequence $s_t$ as the state representation at time $t$.  \n",
    "\n",
    "The future reward are discounted by a factor of $\\gamma$ per time-step.  \n",
    "\n",
    "\n",
    "The optimal action-value function obeys an important identity known as the Bellman equation. This is based on the following intuition: if the optimal value $Q*(s', a')$ of the sequence $s'$ at the next time-step was known for all possible actions $a'$, then the optimal strategy is to select the action $a'$\n",
    "maximising the expected value of $r + \\gamma Q*(s', a')$:  \n",
    "  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f51teduuFGgG"
   },
   "source": [
    "$Q*(s, a) = E_{s' ~ \\epsilon}[r + \\gamma max_{a'}Q*(s', a')|s, a]$  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OqMEYj6VFGnZ"
   },
   "source": [
    "A Q-network can be trained by minimising a sequence of loss functions $L_i(\\theta_i)$ that changes at each iteration $i$:  \n",
    "\n",
    "$L_i(\\theta_i) = E_{s, a ~p(.)}[(y_i - Q(s, a; \\theta_i))^2$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cfBSvtxDFGtW"
   },
   "source": [
    "where  \n",
    "\n",
    "$y_i = E_{s' ~ \\epsilon}[r + \\gamma max_{a'}Q*(s', a'; \\theta_i)|s,a] $   \n",
    "\n",
    "is the target for iteration $i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rh05DhCt6Ph-"
   },
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U4zV0AFhwfKR"
   },
   "source": [
    "### Influence of the discount factor $\\gamma$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "stvc_pWT6PiH"
   },
   "source": [
    "The discount factor $\\gamma$ determines how much the agent cares about rewards in the distant future relative to those in the immediate future.  \n",
    "\n",
    "If $\\gamma$=0, the agent will be completelly myopic and only learn about actions that produce an immediate reward.If $\\gamma$=1, the agent will evaluate each of its actions based on the sum of total of all futures rewards.\n",
    "\n",
    "We used a $\\gamma$ value of 0.99 in order to make our agent care about distant future and we also decreased this value to 0.90 and 0.75 to see how they can impact the agent behavior. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tP2-TKkX6PiI"
   },
   "source": [
    "Thus, we will be experimenting with 3 different parameters set:\n",
    "\n",
    "| Parameter | G1 | G2 | G3 |\n",
    "|------|----|----|----|\n",
    "| **`GAMMA`** | 0.99 | 0.90 | 0.75 |\n",
    "| `LEARNING_RATE` | 0.0005 | 0.0005 | 0.0005 |\n",
    "| `EXPLORATION_RATE` | 0.1 | 0.1 | 0.1 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6bp79q2EdGls"
   },
   "outputs": [],
   "source": [
    "#### IMAGEM DO GRAFICO AQUI ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LhUBXEXudXUn"
   },
   "outputs": [],
   "source": [
    "# From the plots above we can see that $\\gamma = 0.75$ led to poor results, but $\\gamma = 0.9$ and $\\gamma = 0.99$ seems to be equivalent.\n",
    "# An explanation is that when we make our agent short-sighted, it doesn't try to cross all the lanes and receive that huge reward we are offering as much as the far-sighted agents try.\n",
    "\n",
    "# That being said, we will be focusing on the $\\gamma = 0.99$, arbitrarily.\n",
    "# We could be using the $0.9$ too, since it appears to have the same performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lqRy84cAdjUI"
   },
   "source": [
    "### Influence of the learning rate parameter\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0E9TEWNpdsx2"
   },
   "source": [
    "We will be experimenting with 3 different parameters set:\n",
    "\n",
    "| Parameter | G1 | G2 | G3 |\n",
    "|------|----|----|----|\n",
    "| `GAMMA` | 0.99 | 0.99 | 0.99 |\n",
    "| **`LEARNING_RATE`** | 0.0005 | 0.0010 | 0.0050 |\n",
    "| `EXPLORATION_RATE` | 0.1 | 0.1 | 0.1 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JHJ26scHdoud"
   },
   "outputs": [],
   "source": [
    "#### IMAGEM DO GRAFICO AQUI ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cv_3dIkDdpWS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xiogqiuFwfKN"
   },
   "source": [
    "### Influence of the agent's exploration rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wDD_PMkQ6PiL"
   },
   "source": [
    "The exploration rate is the probability that our agent will explore the environment rather than exploit it.  \n",
    "\n",
    "We used 0.1 as our baseline exploration value. In order to see how the exploration rate impact the agent behavior, we also made experiments using the double of this value (0.1) and the half of it (0.05)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "padSZjle6PiL"
   },
   "source": [
    "All in all, these are the parameters that we are going to use to execute this experiment.\n",
    "\n",
    "| Parameter | G1 | G2 | G3 |\n",
    "|------|----|----|----|\n",
    "| `GAMMA` | 0.99 | 0.99 | 0.99 |\n",
    "| `LEARNING_RATE` | 0.0005 | 0.0005 | 0.0005 |\n",
    "| **`EXPLORATION_RATE`** | 0.1 | 0.05 | 0.20 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jp93T-TjeHWy"
   },
   "outputs": [],
   "source": [
    "#### IMAGEM DO GRAFICO AQUI ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Imn2a7GKeHbI"
   },
   "outputs": [],
   "source": [
    "# The exploration rate is the probability that our agent will explore the environment rather than exploit it.  \n",
    "\n",
    "# As we can see from the results show in the plots above, the lower is the $N0$ value, the better is the performance of the agent.\n",
    "# Although this migth seem counterintuitive at first, in fact, it stands to reason.\n",
    "# When we explore more (higher $N0$), we exploit less, leading to worst results in the beginning.\n",
    "# From the graphs above, we can see that all three lines are looking up, still increasing their values, and the gap between them is closing.\n",
    "# We expect to achive better results with higher $N0$s, but it would take too much time for it to happen (we even tested some of them overnight and it still wasn't enough).\n",
    "\n",
    "# Based on our reward function, it is fairly simple to detect which action should be taken in most of the states.\n",
    "# We want to move up always, unless it is leading to a collision.\n",
    "# Thus, frequently it is easy to detect the best action, and for most of the states we don't need to explore a lot to find it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proximal Policy Optimization (PPO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an on-policy algorithm for solving our problem, we chose [Proximal Policy Optimization (PPO)](https://arxiv.org/abs/1707.06347). PPO is a policy optimization method that can be used in environments that work with continuous or discrete action spaces.\n",
    "\n",
    "As with trust region based approaches, such as the TRPO algorithm, it tries to reduce the size of the update that must be applied to a policy, since major updates tend to generate agents with worse performance. However, it seeks to overcome some of the limitations of TRPO  and other methods, being easier to implement and adjust parameters, and having a better sample complexity.\n",
    "\n",
    "Finally, this algorithm has some variants. One of them, which is used in this project, adopts a specialized clipping function to calculate the objective function, as shown in the equation below. For the experiments, we used the [PPO2](https://stable-baselines.readthedocs.io/en/master/modules/ppo2.html) implementation, from the Stable Baselines library.\n",
    "\n",
    "$$\n",
    "L^{CLIP}(\\theta) = \\hat{\\mathbb{E}}_t[min(r_t(\\theta)\\hat{A}_t, clip(r_t(\\theta), 1-\\epsilon, 1+\\epsilon)\\hat{A}_t)]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "\n",
    "To assess the performance of the PPO in solving our problem, we decided to vary some of its parameters, which were: the discount factor ($\\gamma$), the learning rate, the trade-off factor between bias and variance (lam), the number of frames in the input stack of the algorithm, the type of representation of the states (image or RAM), the number of environments and the total timesteps of training.\n",
    "\n",
    "For this, we created a baseline for comparison, with the following configurations:\n",
    "\n",
    "- Policy network: CnnPolicy\n",
    "- Discount factor ($\\gamma$): 0.99.\n",
    "- Learning rate: 0.00025.\n",
    "- Trade-off factor (lam): 0.95.\n",
    "- Number of frames in the stack: 4.\n",
    "- Type of representation: image.\n",
    "- Number of environments: 4.\n",
    "- Training timesteps: 400K.\n",
    "\n",
    "Below, we describe the experiments and results obtained.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Influence of discount factor with image representation\n",
    "\n",
    "In the table below, we can see the settings of the experiments carried out in this section, as well as the results achieved after being smoothed by a factor of 0,999. In addition, in the following graphs, we observe how the reward obtained by agents varies according to the value of the discount factor ($\\gamma$) and the number of timesteps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Parameter**  | **Exp 1** | **Exp 2** | **Exp 3** |\n",
    "|----------------|-----------|-----------|-----------|\n",
    "| **Gamma**      | 0,75      | 0,90      | 0,99      |\n",
    "| Learning Rate  | 0,00025   | 0,00025   | 0,00025   |\n",
    "| Lam            | 0,95      | 0,95      | 0,95      |\n",
    "| Stacks         | 4         | 4         | 4         |\n",
    "| Representation | image     | image     | image     |\n",
    "| Environments   | 4         | 4         | 4         |\n",
    "| Policy         | CnnPolicy | CnnPolicy | CnnPolicy |\n",
    "| Smothed reward | 9,55      | 15,74     | **20,04**     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| |\n",
    "|-|\n",
    "|<center><img src=https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/bae41b89189519f64aa78d12693f800f5d62e51c/marianna/project02-rl/01-figures-experiments/01-images-gamma/svg/image_gamma_all.svg width=\"400\"></center>|\n",
    "\n",
    "| $\\gamma$=0.75 | $\\gamma$=0.90 | $\\gamma$=0.99 |  \n",
    "|---|---|---|  \n",
    "| <img src=https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/bae41b89189519f64aa78d12693f800f5d62e51c/marianna/project02-rl/01-figures-experiments/01-images-gamma/svg/image_gamma_75.svg width=\"250\"> | <img src =https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/bae41b89189519f64aa78d12693f800f5d62e51c/marianna/project02-rl/01-figures-experiments/01-images-gamma/svg/image_gamma_90.svg width=\"250\"> | <img src=https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/bae41b89189519f64aa78d12693f800f5d62e51c/marianna/project02-rl/01-figures-experiments/01-images-gamma/svg/image_gamma_baseline.svg width=\"250\"> |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that in all the graphics, there is a considerable variation in the reward and that the curve that proves to be more stable is the one constructed with $ \\gamma = 0,99 $ (baseline). On the other hand, we noticed that the other curves decrease a lot from certain timesteps, being the one constructed with $ \\gamma = 0,75 $  the first to decrease.\n",
    "\n",
    "This indicates that, for the problem addressed, a PPO agent with a far-sight view is able to achieve better performances and that, the more limited this view is, the worse their results will be. Finally, although the highest value of the best smoothed curve is the one shown in the table, the values of this curve vary around $22$ points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Influence of the number of frames in the stack with image representation\n",
    "\n",
    "Once the learning algorithms receive images as input, to model the problem as a MDP, we need to group a set of frames (images) in structures called stacks, which are given as input to these algorithms. This allows models to access time information about the game.\n",
    "\n",
    "In order to analyze the influence of the size of the frame stacks on the performance of the created PPO agents, we experimented with the baseline with stacks of size 1, 4, 32 and 64, as described in the table below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Parameter**  | **Exp 1** | **Exp 2** | **Exp 3** | **Exp 4** |\n",
    "|----------------|-----------|-----------|-----------|-----------|\n",
    "| Gamma          | 0,99      | 0,99      | 0,99      | 0,99      |\n",
    "| Learning Rate  | 0,00025   | 0,00025   | 0,00025   | 0,00025   |\n",
    "| Lam            | 0,95      | 0,95      | 0,95      | 0,95      |\n",
    "| **Stacks**     | 1         | 4         | 32        | 64        |\n",
    "| Representation | image     | image     | image     | image     |\n",
    "| Environments   | 4         | 4         | 4         | 4         |\n",
    "| Policy         | CnnPolicy | CnnPolicy | CnnPolicy | CnnPolicy |\n",
    "| Smothed reward | **21,16** | 20,04     | 18,23     | 9,72      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/bae41b89189519f64aa78d12693f800f5d62e51c/marianna/project02-rl/01-figures-experiments/02-images-stacks-gamma/images-stacks-gamma-all.svg width=\"400\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| stack = 1 | stack = 4 | stack = 32 | stack = 64 |  \n",
    "|---|---|---|---|  \n",
    "| <img src=https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/bae41b89189519f64aa78d12693f800f5d62e51c/marianna/project02-rl/01-figures-experiments/02-images-stacks-gamma/images-stacks-gamma-stack-1.svg width=\"200\"> | <img src =https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/bae41b89189519f64aa78d12693f800f5d62e51c/marianna/project02-rl/01-figures-experiments/02-images-stacks-gamma/images-stacks-gamma-baseline.svg width=\"200\"> | <img src=https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/bae41b89189519f64aa78d12693f800f5d62e51c/marianna/project02-rl/01-figures-experiments/02-images-stacks-gamma/images-stacks-gamma-stack-32.svg width=\"200\"> |<img src=https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/bae41b89189519f64aa78d12693f800f5d62e51c/marianna/project02-rl/01-figures-experiments/02-images-stacks-gamma/images-stacks-gamma-stack-64.svg width=\"200\"> |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the graphs, we observed that, as we increase the number of frames in the stacks, the agent needs more timesteps to learn a policy that allows it to earn good rewards. This behavior may indicate that knowledge about a more distant past doesn’t necessarily bring improvements to agents for the problem addressed.\n",
    "\n",
    "Despite this, all curves tend to average reward values that are close and are around $21$ to $22$ points. In addition, the training time for each agent also increases by adding more frames, which is expected, since more data will be processed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Influence of the discount factor with RAM representation\n",
    "\n",
    "In order to evaluate how the PPO behaves when receiving the RAM values of the game as input, instead of the image, we performed experiments using all the RAM, an MLP policy network and varied the discount factor. The results are shown on the graphics bellow and the smoothed rewards for $\\gamma = 0.75$, $\\gamma = 0.90$ and $\\gamma = 0.99$ are $20.92$, $20.84$ and $19.57$, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/bae41b89189519f64aa78d12693f800f5d62e51c/marianna/project02-rl/01-figures-experiments/03-ram-gamma/ram-gamma-all.svg width=\"400\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| $\\gamma$=0.75 | $\\gamma$=0.90 | $\\gamma$=0.99 |  \n",
    "|---|---|---|  \n",
    "| <img src=https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/bae41b89189519f64aa78d12693f800f5d62e51c/marianna/project02-rl/01-figures-experiments/03-ram-gamma/ram-gamma-75.svg width=\"250\"> | <img src =https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/bae41b89189519f64aa78d12693f800f5d62e51c/marianna/project02-rl/01-figures-experiments/03-ram-gamma/ram-gamma-90.svg width=\"250\"> | <img src=https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/bae41b89189519f64aa78d12693f800f5d62e51c/marianna/project02-rl/01-figures-experiments/03-ram-gamma/ram-gamma-baseline.svg width=\"250\"> |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observing the curves, we see that they are quite noisy, indicating difficulties during the agent's learning process. Furthermore, unlike what we saw when using images, now a lower value of $ \\gamma $ leads to better results. This indicates that, for this agent, seeing values closer to the present time is more significant. This time, the average rewards were between 21 and 23 points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Influence of input representation\n",
    "\n",
    "Another important comparison, still in the context of the previous section, is between the rewards obtained by using RAM or images as input. In the graphics bellow, we can see the results for images and RAM representations and for the three discount factors experimented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <center> Discount factor equal to 0.75\n",
    "\n",
    "<center><img src=https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/bae41b89189519f64aa78d12693f800f5d62e51c/marianna/project02-rl/01-figures-experiments/04-images-ram/images-ram-75-all.svg width=\"300\"></center>\n",
    "    \n",
    "\n",
    "| Image | RAM |  \n",
    "|---|---|\n",
    "| <img src=https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/bae41b89189519f64aa78d12693f800f5d62e51c/marianna/project02-rl/01-figures-experiments/04-images-ram/images-ram-75-image.svg width=\"200\"> | <img src =https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/bae41b89189519f64aa78d12693f800f5d62e51c/marianna/project02-rl/01-figures-experiments/04-images-ram/images-ram-75-ram.svg width=\"200\"> | \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <center> Discount factor equal to 0.90\n",
    "\n",
    "<center><img src=https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/bae41b89189519f64aa78d12693f800f5d62e51c/marianna/project02-rl/01-figures-experiments/04-images-ram/images-ram-90-all.svg width=\"300\"></center>\n",
    "    \n",
    "| Image | RAM |  \n",
    "|---|---|\n",
    "| <img src=https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/bae41b89189519f64aa78d12693f800f5d62e51c/marianna/project02-rl/01-figures-experiments/04-images-ram/images-ram-90-image.svg width=\"200\"> | <img src =https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/bae41b89189519f64aa78d12693f800f5d62e51c/marianna/project02-rl/01-figures-experiments/04-images-ram/images-ram-90-ram.svg width=\"200\"> |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <center> Discount factor equal to 0.99\n",
    "\n",
    "<center><img src=https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/bae41b89189519f64aa78d12693f800f5d62e51c/marianna/project02-rl/01-figures-experiments/04-images-ram/images-ram-baseline-all.svg width=\"300\"></center>\n",
    "    \n",
    "| Image | RAM |  \n",
    "|---|---|\n",
    "| <img src=https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/bae41b89189519f64aa78d12693f800f5d62e51c/marianna/project02-rl/01-figures-experiments/04-images-ram/images-ram-baseline-image.svg width=\"200\"> | <img src =https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/bae41b89189519f64aa78d12693f800f5d62e51c/marianna/project02-rl/01-figures-experiments/04-images-ram/images-ram-baseline-ram.svg width=\"200\"> |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the rewards obtained are similar, except for the regions of the curves for $ \\gamma = 0,75 $ and $ \\gamma = 0,90 $ that start to decrease. This indicates that RAM can also provide a good representation for our problem.\n",
    "\n",
    "Despite this, given the big variability of the experiments, more tests need to be carried out to verify this hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Influence of the total timesteps\n",
    "\n",
    "All previous experiments had a duration of 400K timesteps. In order to evaluate the influence of this parameter on the agents' performance, we run new experiments using the baseline and 1M timesteps. The results are shown on the figure bellow, for two executions of the baseline setting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=https://raw.githubusercontent.com/DionisiusMayr/FreewayGame/bae41b89189519f64aa78d12693f800f5d62e51c/marianna/project02-rl/01-figures-experiments/05-timesteps/timestep-baseline-1-2.svg width=\"400\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, after an initial growth, the reward stabilizes for a few thousand timesteps and starts growing again, allowing us to reach values much higher than those obtained previously. This indicates that we can still exploit a lot of the PPO's capacity for our problem, if we train it for a sufficient number of timesteps. Additionally, applying a smoothing of 0,999 on the results, we achieve a maximum reward of 23,76.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Notes\n",
    "\n",
    "After the experiments with the discount factor, the forms of representation of the input, the number of frames of the stacks and the amount of training timesteps, we observed some important characteristics that should be highlighted.\n",
    "\n",
    "\n",
    "The first is the big variability of the results, so that two experiments performed with the same configurations sometimes generate considerably different performances. \n",
    "\n",
    "Another important observation is that, as we realized for the experiments with the baseline and 1M timesteps, we would probably get better results for all other configurations if we trained them for longer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "clJkwoAG6Phk",
    "wskzQX8N6Phl",
    "86j84qlA6Phv"
   ],
   "include_colab_link": true,
   "name": "aa_DQN_freeway.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
