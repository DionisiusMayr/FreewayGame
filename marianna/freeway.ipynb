{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO:\n",
    "- [ ] Write the MDP formulation -> Aline\n",
    "- [X] The discretization model adopted\n",
    "  - Será que tá pronto? Falei bastante no State Representation.\n",
    "\n",
    "Aspectos que devem ser abordados:\n",
    "- [ ] computational cost\n",
    "  - [ ] Falar que o gargalo aqui é processamento e não uso de memória\n",
    "- [ ] optimality\n",
    "- [ ] influence of reward function\n",
    "- [X] state and action space sizes\n",
    "\n",
    "Também precisamos definir\n",
    "- [ ] How the problem was modeled\n",
    "  - [ ] Precisa falar alguma coisa aqui?\n",
    "- [ ] Implementation specifics and restrictions\n",
    "  - [ ] Precisa falar alguma coisa aqui?\n",
    "\n",
    "Problem\n",
    "- [X] The nature of your environment (episodic/not episodic, deterministic/stochastic)\n",
    "- [X] What are your terminal states (when they exist)\n",
    "- [X] How is your reward function defined\n",
    "- [X] All parameters employed in your methods (discount factor, step size, etc.)\n",
    "\n",
    "Outros:\n",
    "- [ ] Gerar um requirements.txt completo e instruções confiáveis para instalação.\n",
    "- [ ] Computational cost of SARSA-Lambda (Final Thoughts)\n",
    "- [ ] Plot the optimal value function V ∗ (s) = maxa Q∗(s, a).\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_mean_score = 21.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Freeway"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the first project for the MC935rA/MO436A - Reinforcement Learning course, taught by Prof. Esther Colombini.\n",
    "\n",
    "In this project we propose to apply Reinforcement Learning methods to teach an agent how to play the Freeway Atari game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Group members:**\n",
    "- Aline Gabriel de Almeida\n",
    "- Dionisius Oliveira Mayr (229060)\n",
    "- Leonardo de Oliveira Ramos (171941)\n",
    "- Marianna de Pinho Severo (264960)\n",
    "- Victor Jesús Sotelo Chico (265173)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Freeway game\n",
    "\n",
    "![Baseline 1](./img/Freeway_logo.png)\n",
    "\n",
    "Freeway is a video game written by David Crane for the Atari 2600 and published by Activision [[1]](https://en.wikipedia.org/wiki/Freeway_(video_game)).\n",
    "\n",
    "In the game, two players compete against each other trying to make their chikens cross the street, while evading the cars passing by.\n",
    "There are three possible actions: staying still, moving forward or moving backward.\n",
    "Each time a chicken collides with a car, it is forced back some spaces and takes a while until the chiken regains its control.\n",
    "\n",
    "When a chicken is successfully guided across the freeway, it is awarded one point and moved to the initial space, where it will try to cross the street again.\n",
    "The game offers multiple scenarios with different vehicles configurations (varying the type, frequency and speed of them) and plays for 2 minutes and 16 seconds.\n",
    "During the 8 last seconds the scores will start blinking to indicate that the game is close to end.\n",
    "Whoever has the most points after this, wins the game!\n",
    "\n",
    "The image was extracted from the [manual of the game](https://www.gamesdatabase.org/Media/SYSTEM/Atari_2600/Manual/formated/Freeway_-_1981_-_Zellers.pdf).\n",
    "\n",
    "[1 - Wikipedia - Freeway](https://en.wikipedia.org/wiki/Freeway_(video_game))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the [OpenAI Gym](https://gym.openai.com/) toolkit.\n",
    "This toolkit uses the [Arcade Learning Environment](https://github.com/mgbellemare/Arcade-Learning-Environment) to simulate the game through the [Stella](https://stella-emu.github.io/) emulator.\n",
    "\n",
    "Although the game offers multiple scenarios, we are going to consider only the first one. Also, we will be controlling a *single chicken*, while we try to maximize its score.\n",
    "\n",
    "In this configuration, there are ten lanes and each lane contains exactly one car (with a different speed and direction).\n",
    "Whenever an action is chosen, it is repeated for $k$ frames, $k \\in \\{2, 3, 4\\}$.\n",
    "\n",
    "This means that our environment is **stochastic** and it is also **episodic**, with its terminal state being reached whenever 2 minutes and 16 seconds have passed.\n",
    "\n",
    "Our base state representation is given by the RAM of the Atari 2600 (more on this latter).\n",
    "\n",
    "You can find more information regarding the environment used at [Freeway-ram-v0](https://gym.openai.com/envs/Freeway-ram-v0/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the dependencies:\n",
    "```sh\n",
    "pip install gym\n",
    "pip install gym[atari]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can find a list of useful links and materials that were used during this project.\n",
    "\n",
    "* [Freeway-ram-v0 from OpenAI Gym](https://gym.openai.com/envs/Freeway-ram-v0/)\n",
    "* [Manual of the game](https://www.gamesdatabase.org/Media/SYSTEM/Atari_2600/Manual/formated/Freeway_-_1981_-_Zellers.pdf)\n",
    "* [Freeway Disassembly](http://www.bjars.com/disassemblies.html)\n",
    "* [Atari Ram Annotations](https://github.com/mila-iqia/atari-representation-learning/blob/master/atariari/benchmark/ram_annotations.py)\n",
    "* [Freeway Benchmarks](https://paperswithcode.com/sota/atari-games-on-atari-2600-freeway)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')  # Enable importing from `src` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import statistics\n",
    "from collections import defaultdict\n",
    "from functools import lru_cache\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import gym\n",
    "\n",
    "import src.agents as agents\n",
    "import src.episode as episode\n",
    "import src.environment as environment\n",
    "import src.aux_plots as aux_plots\n",
    "import src.serializer as serializer\n",
    "import src.gif as gif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_result(i, scores, total_reward, score):\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Run [{i:4}] - Total reward: {total_reward:7.2f} Mean scores: {sum(scores) / len(scores):.2f} Means Scores[:-10]: {sum(scores[-10:]) / len(scores[-10:]):5.2f} Score: {score:2} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_int_array_from_file(fn: str):\n",
    "    with open(f\"./experiments/{fn}\") as f:\n",
    "        return [int(x) for x in f.read().splitlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Action space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we said above, the agent in this game has three possible actions at each frame, each represented by an integer:\n",
    "\n",
    "* 0: Stay\n",
    "* 1: Move forward\n",
    "* 2: Move backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In theory, a perfect chicken wouldn't ever need to move backward, since it is possible to know if moving forward would lead you into a collision (in the immediate frame or in the future frames).\n",
    "\n",
    "In our project we will be experimenting with agents using only two possible actions (staying and moving forward) and also agents using all the three possible actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State of the art benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image bellow (extracted from https://paperswithcode.com/sota/atari-games-on-atari-2600-freeway) shows the evolution of the scores over time using different techniques.\n",
    "\n",
    "Today, the state of the art approaches are making 34.0 points, using Deep Reinforcement Learning methods.\n",
    "\n",
    "However, since we are using tabular methods, we don't think it will be possible to beat this benchmark.\n",
    "Instead, we will be looking at a different, simpler baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Benchmarks](./img/state_of_art_scores.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple baseline agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a simple baseline, we are using an agent that always moves **up**, regardless of the rewards received or the current state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env, initial_state = environment.get_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent = agents.Baseline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_rewards = []\n",
    "# n_runs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# for i in range(n_runs):\n",
    "#     render = i % 10 == 0\n",
    "\n",
    "#     game_over = False\n",
    "#     state = env.reset()\n",
    "#     action = agent.act(state)\n",
    "\n",
    "#     total_reward = 0\n",
    "\n",
    "#     while not game_over:\n",
    "#         if render:\n",
    "#             time.sleep(0.01)\n",
    "#             env.render()\n",
    "\n",
    "#         ob, reward, game_over, _ = env.step(action)\n",
    "\n",
    "#         total_reward += reward\n",
    "#         action = agent.act(state)  # Next action\n",
    "\n",
    "#     total_rewards.append(total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline_mean_score = np.mean(total_rewards)\n",
    "# baseline_mean_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, this agent usually scores 21 or 23 points (as shown in the images bellow). It depends on the the values of $k$ sampled, and on average it scores about 21.8 points per run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Baseline 1](./img/baseline_1.png)\n",
    "![Baseline 2](./img/baseline_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the tabular methods we are going to use work with some representation of the actual environment state, we will need to understand it better in order to effectively approach this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Atari 2600"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before talking about the state representation, it is important to understand how the Atari 2600 works.\n",
    "\n",
    "Atari 2600 is a video game released in 1977 by the American Atari, Inc.\n",
    "Its **8-bit** microprocessor was of the MOS **6502** family and it had **128 bytes** of RAM.\n",
    "\n",
    "And these 128 bytes are what really matters here.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that Gym gives us the RAM memory of the Atari as the state representation.\n",
    "In other words, it gives us an 128-element `np.array`, where each element of the array is an `uint8` (*integer values ranging from 0 to 255*).\n",
    "\n",
    "That said, we have (in theory) $256^{128} \\approx 1.8 \\cdot 10^{308}$ possible game states!\n",
    "\n",
    "This is *far* from being manageable, and thus we need to come up with a different approch to represent our state if we want our algorithms to converge.\n",
    "\n",
    "One might argue that the RAM state is *sparse* and although that is true, it is still not sparse enough to apply tabular methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting useful bytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try to select only the bytes that are useful to deal with our problem.\n",
    "\n",
    "To do so, we will be looking at a [fan made disassembly](http://www.bjars.com/disassemblies.html) by Glenn Saunders.\n",
    "\n",
    "From the 6502 assembly we can see the variables locations in the memory, their size and count the amount of bytes since the initial offset to determine which byte represents what.\n",
    "\n",
    "Simplifying it a bit, we would end up with a list of candidate bytes for our state representation like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Description| Bytes |\n",
    "|----|----|\n",
    "| Chicken Y | 14 |\n",
    "| Chicken Lane Collide | 16 |\n",
    "| Chicken Collision flag | 18 |\n",
    "| Car X Direction | 22 |\n",
    "| Z Car Patterns | 23, 24, 25, 26, 27, 28, 29, 30, 31, 32 |\n",
    "| Car Motion Timmers | 33, 34, 35, 36, 37, 38, 39, 40, 41, 42 |\n",
    "| Car Motions | 43, 44, 45, 46, 47, 48, 49, 50, 51, 52 |\n",
    "| Car Shape Ptr | 87, 88 |\n",
    "| Chicken Shape Ptr | 89, 90 |\n",
    "| Chicken Sounds | 106, 107 |\n",
    "| Car X Coords | 108, 109, 110, 111, 112, 113, 114, 115, 116, 117 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After observing them, we were able to filter bytes that wouldn't be useful for us (like auxiliary variables used during function calls, e.g. `Car Motion Timmers`, `Chicken Sounds`) and bytes that contain constant values, like `Car X Direction` and`Car Motions`.\n",
    "\n",
    "The final list of bytes being used is given here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Description| Bytes |\n",
    "|----|----|\n",
    "| Chicken Y | 14 |\n",
    "| Chicken Lane Collide | 16 |\n",
    "| Car X Coords | 108, 109, 110, 111, 112, 113, 114, 115, 116, 117 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAM_mask = [\n",
    "      14  # Chicken Y\n",
    "    , 16  # Chicken Lane Collided\n",
    "    , 108, 109, 110, 111, 112, 113, 114, 115, 116, 117  # Car X Coords\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we went from using 128 bytes to only 12, with $256^{12} \\approx 7.9 \\cdot 10^{28}$ theoritical possible states.\n",
    "\n",
    "But this is still a lot.\n",
    "We need to reduce it even more, and that is exactly what the function `reduce_state` bellow does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_state(ob):\n",
    "    # Doesn't matter where we were hit\n",
    "    ob[16] = 1 if ob[16] != 255 else 0\n",
    "\n",
    "    # Reduce chicken y-position\n",
    "    ob[14] = ob[14] // 3\n",
    "\n",
    "    for b in range(108, 118):\n",
    "        # The chicken is in the x-posistion ~49\n",
    "        if ob[b] < 20 or ob[b] > 80:\n",
    "            # We don't need to represent cars far from the chicken\n",
    "            ob[b] = 0\n",
    "        else:\n",
    "            # Reduce the cars x-positions sample space\n",
    "            ob[b] = ob[b] // 3\n",
    "\n",
    "    return ob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using 4 strategies to reduce our state.\n",
    "\n",
    "* The `byte[16]` represents the lane where a collision happened. Instead of using it like this, we will make it binary, 1 being a collision (on any lane) and 0 otherwise.\n",
    "* The `byte[14]` represents the y-position of the chicken (from 0 to ~170). We will be losing a bit of precision here in order to reduce the state space, dividing (truncating the fractional part) the `byte[14]` by 3.\n",
    "* The `bytes[108:118]` are used as the cars x-position. If they are far from the chicken (the chicken x-position is fixed at ~49), we won't care about them, setting it as 0. If they are near the chicken (between x-20 and x-80), we will represent them again dividing by 3. The idea here is that the cars that are near the chicken contain more valuable information than the cars far from it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have $2 \\cdot 57 \\cdot 21^{10} \\approx 1.9 \\cdot 10^{15}$ theoritical possible states.\n",
    "\n",
    "This might seen like it is still too much, but as we will soon see, empirically this number is far smaller.\n",
    "\n",
    "The biggest factor of the $10^{15}$ states is due to the cars x-positions.\n",
    "But recall that each car has a constant speed (which can be different from the other cars). Because of it, their positions are periodical, meaning that they don't use the entire range of possible values in their byte.\n",
    "\n",
    "For instance: a car with speed 4 will *never* be at the x-coordinate 3, nor will a car with speed 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reward Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the base environment we are awarded on point each time we successfully cross the freeway.\n",
    "\n",
    "However, it stands to reason experimenting with other reward strategies, like penalizing collisions or standing still and also changing the numerical value of these quantities.\n",
    "\n",
    "The method bellow allows us to experiment multiple reward policies.\n",
    "\n",
    "We defined some conditions were the agent get a reward: \n",
    "- `REWARD_IF_CROSS`: If the chicken cross the 10 lanes; this is our fundamental goal and should be encouraged;\n",
    "- `REWARD_IF_COLLISION`: If the chicken collides with a car; as the collisions can delay the chicken to cross the lanes, it can be discouraged;\n",
    "- Each one of the agent's three actions can be rewarded according to how much they can help the chicken to cross all the lanes and doing so we can hopefully accelerate the conversion of the algorithms. The `REWARD_IF_STILL` is the reward given if the chicken doesn't  move up, and *can* be discouraged in some cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_policy(reward, ob, action):\n",
    "    if reward == 1:\n",
    "        reward = reward_policy.REWARD_IF_CROSS\n",
    "    elif ob[16] == 1:  # Collision!\n",
    "        reward = reward_policy.REWARD_IF_COLLISION\n",
    "    elif action != 1:  # Don't incentivate staying still\n",
    "        reward = reward_policy.REWARD_IF_STILL\n",
    "\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "AVAILABLE_ACTIONS = 2\n",
    "N0 = 2.5\n",
    "LAMBD = 0.2\n",
    "\n",
    "reward_policy.REWARD_IF_CROSS = 1\n",
    "reward_policy.REWARD_IF_COLLISION = 0\n",
    "reward_policy.REWARD_IF_STILL = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it takes a lot of time to train the models, we won't train them all in this report.\n",
    "Instead, we will be load the results of our simulations and specifying the parameters used to obtain those results.\n",
    "Of course, it is possible to reproduce our results simply by running the algorithms here using the same hyper parameters as specified.\n",
    "\n",
    "You can find inside `./experiments/` the collection of results that we generated, and inside `./serialized_models/` one can find some serialized models.\n",
    "We don't advise trying to load the serialized models in machines different from the machine where it was generated beucase of compatibility issues.\n",
    "\n",
    "Whenever possible, we will be adding plots comparing different approaches and parameters, as well as adding gifs in this notebook so that we can visualize the development of the agent and unique strategies that they learned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-4f76a9dad686>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Q-learning algorithm receives the $\\gamma$ and the $N0$ parameters.  \n",
    "\n",
    "$\\gamma$ is the discount factor; This parameters determines the importance of future rewards. A value of 0 makes the agent short-sighned by only considering current rewards, while a factor approaching 1 will make it strive for a long term reward.  \n",
    "\n",
    "The $N0$ parameter is used to define the agent's exploration rate $\\epsilon$, where $\\epsilon = N0/(N0+N_{sa})$ and $N_{sa}$ is the number of visits in the state $s$ where the action $a$ was taken.\n",
    "\n",
    "In the algorithm, the action-value function is initialized to zero. Then, at each time $t$ the agent selects an action $a_t$, observes a reward $r_t$, enters a new state $s_{s+1}$, and $Q$ is updated:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Q_{new}(s_t, a_t) :=  Q(s_t, a_t)+\\alpha (r_t + \\gamma.max_aQ(s_{t+1},a)-Q(s_t, a_t))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning rate $\\alpha$ is defined as $\\alpha = 1/N_{sa}$, where $N_{sa}$ is the number of times that the specific state-action pair has already occured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env, initial_state = environment.get_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = agents.QLearning(gamma=GAMMA, available_actions=AVAILABLE_ACTIONS, N0=N0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "total_rewards = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "n_runs = 1\n",
    "\n",
    "for i in range(n_runs):\n",
    "    render = i % 200 == 201\n",
    "\n",
    "    game_over = False\n",
    "    state = env.reset()\n",
    "    state = reduce_state(state)[RAM_mask].data.tobytes()  # Select useful bytes\n",
    "    action = agent.act(state)\n",
    "    \n",
    "    score = 0\n",
    "    total_reward = 0\n",
    "\n",
    "    while not game_over:\n",
    "        if render:\n",
    "            time.sleep(0.025)\n",
    "            env.render()\n",
    "\n",
    "        old_state = state\n",
    "        ob, reward, game_over, _ = env.step(action)\n",
    "\n",
    "        ob = reduce_state(ob)\n",
    "        reward = reward_policy(reward, ob, action)\n",
    "\n",
    "        total_reward += reward\n",
    "\n",
    "        if reward == reward_policy.REWARD_IF_CROSS:\n",
    "            score += 1\n",
    "\n",
    "        state = ob[RAM_mask].data.tobytes()\n",
    "\n",
    "        agent.update_Q(old_state, state, action, reward)\n",
    "\n",
    "        action = agent.act(state)  # Next action\n",
    "\n",
    "    scores.append(score)\n",
    "    total_rewards.append(total_reward)\n",
    "\n",
    "    print_result(i, scores, total_reward, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp = serializer.Experiment(agent, scores, total_rewards, reduce_state, reward_policy)\n",
    "# exp.save_experiment('QL_EXPERIMENT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3rjTCHMkwfKE"
   },
   "source": [
    "### Influence of the number of actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will compare the impact that the number of possible actions (3 vs 2) has on our score.\n",
    "\n",
    "To do so, we will run the QLearn algorithm twice, with 3 actions and with 2 actions, and then compare the scores obtained. The complete set of parameters used is shown bellow:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters used:\n",
    "```\n",
    "GAMMA = 0.99\n",
    "AVAILABLE_ACTIONS = {2, 3}\n",
    "N0 = 2.5\n",
    "\n",
    "reward_policy.REWARD_IF_CROSS = 1\n",
    "reward_policy.REWARD_IF_COLLISION = 0\n",
    "reward_policy.REWARD_IF_STILL = 0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_2act = read_int_array_from_file(\"./QL/QL_scores_2act.txt\")\n",
    "total_rewards_2act = read_int_array_from_file(\"./QL/QL_total_rewards_2act.txt\")\n",
    "scores_3act = read_int_array_from_file(\"./QL/QL_scores_3act.txt\")\n",
    "total_rewards_3act = read_int_array_from_file(\"./QL/QL_total_rewards_3act.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = [baseline_mean_score for i in range(4000)]\n",
    "\n",
    "ax = aux_plots.plot_2scores(scores_3act[:4000], scores_2act[:4000], \"3 actions (up, down or stay)\", \"2 actions (up or stay)\")\n",
    "aux_plots.moving_average(scores_2act[:4000], ax, label='2 actions moving avg', color='darkmagenta')\n",
    "aux_plots.moving_average(scores_3act[:4000], ax, label='3 actions moving avg', color='darkcyan')\n",
    "aux_plots.moving_average(baseline[:4000], ax, label='Baseline', color='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the graph above, we can see that when using only two actions the agent learns faster than with three actions, thus leading to better results in less iterations.\n",
    "\n",
    "As we said before, this is expected and we will now be focusing only on testing with two actions.\n",
    "\n",
    "Since we are using the default reward policy, the reward graph is exactly the same as the score, and thus we won't be showing it here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eJ_JiwOewfKK"
   },
   "source": [
    "### Influence of the reward values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to investigate the influence of the reward values in the agent behavior, we ran the algorithm using three different sets of reward values.\n",
    "\n",
    "* **R1:** Positive rewards for crossing: it offered a sparse reward to the agent: +1 if the chicken cross all the lanes.  \n",
    "* **R2:** Positive rewards for crossing and negative reward for colliding: it given +1 for crossing all the lanes and -1 if the chicken collide. Thus, we expect the agent to be encouraged to cross the lanes (positive reward) and to avoid collisions (negative reward).  \n",
    "* **R3:** Huge positive reward for crossing and negative reward for colliding or staying in the same position: it increased the reward for crossing all the lanes to +500 and give -10 when the chicken collide. It also gives a negative reward of -1 when the agent decide to **still**. Thus we expect the chicken to cross all the lanes faster and increase the final score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Parameter | R1 | R2 | R3 |\n",
    "|------|----|----|----|\n",
    "| `GAMMA` | 0.99 | 0.99 | 0.99 |\n",
    "| `AVAILABLE_ACTIONS` | 2 | 2 | 2 |\n",
    "| `N0` | 2.5 | 2.5 | 2.5 |\n",
    "| `CROSS` | 1 | 1 | 500 |\n",
    "| `COLLISION` | 0 | -1 | -10 |\n",
    "| `STILL` | 0 | 0 | -1 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_2act_R1 = read_int_array_from_file('./QL/QL_scores_2act_R1.txt')\n",
    "scores_2act_R2 = read_int_array_from_file('./QL/QL_scores_2act_R2.txt')\n",
    "scores_2act_R3 = read_int_array_from_file('./QL/QL_scores_2act_R3.txt')\n",
    "\n",
    "total_rewards_2act_R1 = read_int_array_from_file('./QL/QL_total_rewards_2act_R1.txt')\n",
    "total_rewards_2act_R2 = read_int_array_from_file('./QL/QL_total_rewards_2act_R2.txt')\n",
    "total_rewards_2act_R3 = read_int_array_from_file('./QL/QL_total_rewards_2act_R3.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = [baseline_mean_score for i in range(4000)]\n",
    "\n",
    "ax = aux_plots.plot_3scores(scores_2act_R1[:4000], scores_2act_R2[:4000], scores_2act_R3[:4000], \"R1 (+1 if cross)\", \"R2 (+1 if cross, -1 if collide)\", \"R3 (+500 if cross, -10 if collide, -1 if stay)\")\n",
    "\n",
    "aux_plots.moving_average(scores_2act_R1[:4000], ax, label=\"R1 (+1 if cross)\", color='darkcyan')\n",
    "aux_plots.moving_average(scores_2act_R2[:4000], ax, label=\"R2 (+1 if cross, -1 if collide)\", color='darkmagenta')\n",
    "aux_plots.moving_average(scores_2act_R3[:4000], ax, label=\"R3 (+500 if cross, -10 if collide, -1 if stay)\", color='#FF3341')\n",
    "aux_plots.moving_average(baseline, ax, label='Baseline', color='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = aux_plots.plot_3rewards(total_rewards_2act_R1[:4000], total_rewards_2act_R2[:4000], total_rewards_2act_R3[:4000], \"R1 (+1 if cross)\", \"R2 (+1 if cross, -1 if collide)\", \"R3 (+500 if cross, -10 if collide, -1 if stay)\")\n",
    "\n",
    "aux_plots.moving_average(total_rewards_2act_R1[:4000], ax, label=\"R1 (+1 if cross)\", color='#1F8FFF')\n",
    "aux_plots.moving_average(total_rewards_2act_R2[:4000], ax, label=\"R2 (+1 if cross, -1 if collide)\", color='#9C4BE7')\n",
    "aux_plots.moving_average(total_rewards_2act_R3[:4000], ax, label=\"R3 (+500 if cross, -10 if collide, -1 if stay)\", color='#FF3341')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the scales of the rewards vary a lot, we are separating R3 from R1 and R2, so we can compare than easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = aux_plots.plot_2rewards(total_rewards_2act_R1[:4000], total_rewards_2act_R2[:4000], \"R1 (+1 if cross)\", \"R2 (+1 if cross, -1 if collide)\")\n",
    "\n",
    "aux_plots.moving_average(total_rewards_2act_R1[:4000], ax, label=\"R1 (+1 if cross)\", color='#1F8FFF')\n",
    "aux_plots.moving_average(total_rewards_2act_R2[:4000], ax, label=\"R2 (+1 if cross, -1 if collide)\", color='#9C4BE7')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that R1 is the default reward policy of our system, where the chicken receives 1 reward each time it crosses the freeway, and thus it is the same as the score.\n",
    "\n",
    "From these graphs we can see that for about 250 iterations the agents are scoring basically the same amount of points, but a few episodes latter, its behaviour change completely.\n",
    "R2 starts to show a very good performance for a while, but as it explores more and tries to maximize its reward, it starts getting more \"coward\", avoiding colliding with cars at all costs, even if it means staying still.\n",
    "The R2 agent prefers not to take risks, since one collision \"takes\" as much reward as it would get with a successful cross.\n",
    "That is why after 1000 episodes its performance starts to degrade a lot.\n",
    "This is the point where the agent starts to learn that in order to maximize its reward, it is better to stay still than to try to cross.\n",
    "Thus, even though the reward obtained is still good, the score is not.\n",
    "\n",
    "This behavior led us to the R3 approach, where we give a higher reward to crossing (500), while we still penalize collisions (-10) and also penalize staying still (-1).\n",
    "By doing so, we incentivate the agent to cross while discouraging it to collide or staying still.\n",
    "We give a higher penalty to collisions instead of staying still because the agent can avoid a collision by not moving.\n",
    "After a thound episodes, we can already see that this is the solution that performs better among these three, **scoring close to 30 points** in 4000 episodes!\n",
    "\n",
    "Notice that they haven't fully converged yet.\n",
    "However, the gain that we obtain from now on is marginal, compared to the time it takes for it to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U4zV0AFhwfKR"
   },
   "source": [
    "### Influence of the discount factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The discount factor $\\gamma$ determines how much the agent cares about rewards in the distant future relative to those in the immediate future.  \n",
    "\n",
    "If $\\gamma$=0, the agent will be completelly myopic and only learn about actions that produce an immediate reward.If $\\gamma$=1, the agent will evaluate each of its actions based on the sum of total of all futures rewards.\n",
    "\n",
    "We used a $\\gamma$ value of 0.99 in order to make our agent care about distant future and we also decreased this value to 0.90 and 0.75 to see how they can impact the agent behavior. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we will be experimenting with 3 different parameters set:\n",
    "\n",
    "| Parameter | G1 | G2 | G3 |\n",
    "|------|----|----|----|\n",
    "| `GAMMA` | 0.99 | 0.90 | 0.75 |\n",
    "| `AVAILABLE_ACTIONS` | 2 | 2 | 2 |\n",
    "| `N0` | 2.5 | 2.5 | 2.5 |\n",
    "| `CROSS` | 500 | 500 | 500 |\n",
    "| `COLLISION` | -10 | -10 | -10 |\n",
    "| `STILL` | -1 | -1 | -1 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_2act_gamma_0p99 = read_int_array_from_file('./QL/QL_scores_2act_R3.txt')\n",
    "scores_2act_gamma_0p9 = read_int_array_from_file('./QL/QL_scores_2act_gamma_0p9.txt')\n",
    "scores_2act_gamma_0p75 = read_int_array_from_file('./QL/QL_scores_2act_gamma_0p75.txt')\n",
    "\n",
    "total_rewards_2act_gamma_0p99 = read_int_array_from_file('./QL/QL_total_rewards_2act_R3.txt')\n",
    "total_rewards_2act_gamma_0p9 = read_int_array_from_file('./QL/QL_total_rewards_2act_gamma_0p9.txt')\n",
    "total_rewards_2act_gamma_0p75 = read_int_array_from_file('./QL/QL_total_rewards_2act_gamma_0p75.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = [baseline_mean_score for i in range(4000)]\n",
    "\n",
    "ax = aux_plots.plot_3scores(scores_2act_gamma_0p99[:4000], scores_2act_gamma_0p9[:4000], scores_2act_gamma_0p75[:4000], \"gamma=0.99\", \"gamma=0.90\", \"gamma=0.75\")\n",
    "\n",
    "aux_plots.moving_average(scores_2act_gamma_0p99[:4000], ax, label=\"gamma=0.99\", color='darkcyan')\n",
    "aux_plots.moving_average(scores_2act_gamma_0p9[:4000], ax, label=\"gamma=0.90\", color='darkmagenta')\n",
    "aux_plots.moving_average(scores_2act_gamma_0p75[:4000], ax, label=\"gamma=0.75\", color='#FF3341')\n",
    "aux_plots.moving_average(baseline, ax, label='Baseline', color='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = aux_plots.plot_3rewards(total_rewards_2act_gamma_0p99[:4000], total_rewards_2act_gamma_0p9[:4000], total_rewards_2act_gamma_0p75[:4000], \"gamma=0.99\", \"gamma=0.90\", \"gamma=0.75\")\n",
    "\n",
    "aux_plots.moving_average(total_rewards_2act_gamma_0p99[:4000], ax, label=\"gamma=0.99\", color='darkcyan')\n",
    "aux_plots.moving_average(total_rewards_2act_gamma_0p9[:4000], ax, label=\"gamma=0.90\", color='darkmagenta')\n",
    "aux_plots.moving_average(total_rewards_2act_gamma_0p75[:4000], ax, label=\"gamma=0.75\", color='#FF3341')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: JUSTIFICAR O 0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the results show in the plots above, the agent with higher values of $\\gamma$ got the best results.\n",
    "\n",
    "As our fundamental goal is to make the agent to cross all the lanes, we give it a huge reward when achieving this goal.\n",
    "If we make the agent far-sighned it keeps seeking the final reward and obtain the higher scores.\n",
    "\n",
    "When we make the agent more short-sighned, it considers the final reward as less valuable and starts to avoid to be penalized at the immediate future.\n",
    "\n",
    "Below the plots are showing the score and the total rewards got using the three agents. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xiogqiuFwfKN"
   },
   "source": [
    "### Influence of the $N0$ parameter that is used to define the agent's exploration rate $\\epsilon$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exploration rate is the probability that our agent will explore the environment rather than exploit it.  \n",
    "The $N0$ parameter is used to define the agent's exploration rate $\\epsilon$ , where  $\\epsilon = N0/(N0+N_{sa})$ and $N_{sa}$ is the number of times that state $s$ has been visited and taken the action $a$.\n",
    "\n",
    "We used a very small value of $N0$ (0.001) so we can see how well the agent performs if this constant $N0$ had almost no impact in the exploration rate done and we also used $N0$ = 2.5 and $N0$ = 5.0 so we can see how the exploration rate impact the agent behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All in all, these are the parameters that we are going to use to execute this experiment.\n",
    "\n",
    "| Parameter | N1 | N2 | N3 |\n",
    "|------|----|----|----|\n",
    "| `GAMMA` | 0.99 | 0.99 | 0.99 |\n",
    "| `AVAILABLE_ACTIONS` | 2 | 2 | 2 |\n",
    "| `N0` | 0.001 | 2.5 | 5.0 |\n",
    "| `CROSS` | 500 | 500 | 500 |\n",
    "| `COLLISION` | -10 | -10 | -10 |\n",
    "| `STILL` | -1 | -1 | -1 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_2act_N0_0 = read_int_array_from_file('./QL/QL_scores_2act_N0_0.txt')\n",
    "scores_2act_N0_25 = read_int_array_from_file('./QL/QL_scores_2act_R3.txt')\n",
    "scores_2act_N0_5 = read_int_array_from_file('./QL/QL_scores_2act_N0_5.txt')\n",
    "\n",
    "total_rewards_2act_N0_0 = read_int_array_from_file('./QL/QL_total_rewards_2act_N0_0.txt')\n",
    "total_rewards_2act_N0_25 = read_int_array_from_file('./QL/QL_total_rewards_2act_R3.txt')\n",
    "total_rewards_2act_N0_5 = read_int_array_from_file('./QL/QL_total_rewards_2act_N0_5.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = [baseline_mean_score for i in range(4000)]\n",
    "\n",
    "ax = aux_plots.plot_3scores(scores_2act_N0_0[:4000], scores_2act_N0_25[:4000], scores_2act_N0_5[:4000], \"N0=0.001\", \"N0=2.50\", \"N0=5.00\")\n",
    "\n",
    "aux_plots.moving_average(scores_2act_N0_0[:4000], ax, label=\"N0=0.001\", color='darkcyan')\n",
    "aux_plots.moving_average(scores_2act_N0_25[:4000], ax, label=\"N0=2.50\", color='darkmagenta')\n",
    "aux_plots.moving_average(scores_2act_N0_5[:4000], ax, label=\"N0=5.00\", color='#FF3341')\n",
    "aux_plots.moving_average(baseline, ax, label='Baseline', color='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = aux_plots.plot_3rewards(total_rewards_2act_N0_0[:4000], total_rewards_2act_N0_25[:4000], total_rewards_2act_N0_5[:4000], \"N0=0.001\", \"N0=2.50\", \"N0=5.00\")\n",
    "\n",
    "aux_plots.moving_average(total_rewards_2act_N0_0[:4000], ax, label=\"N0=0.001\", color='darkcyan')\n",
    "aux_plots.moving_average(total_rewards_2act_N0_25[:4000], ax, label=\"N0=2.50\", color='darkmagenta')\n",
    "aux_plots.moving_average(total_rewards_2act_N0_5[:4000], ax, label=\"N0=5.00\", color='#FF3341')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exploration rate is the probability that our agent will explore the environment rather than exploit it.  \n",
    "\n",
    "As we can see from the results show in the plots above, the lower is the $N0$ value, the better is the performance of the agent.\n",
    "Although this migth seem counterintuitive at first, in fact, it stands to reason.\n",
    "When we explore more (higher $N0$), we exploit less, leading to worst results in the beginning.\n",
    "From the graphs above, we can see that all three lines are looking up, still increasing their values, and the gap between them is closing.\n",
    "We expect to achive better results with higher $N0$s, but it would take too much time for it to happen (we even tested some of them overnight and it still wasn't enough).\n",
    "\n",
    "Based on our reward function, it is fairly simple to detect which action should be taken in most of the states.\n",
    "We want to move up always, unless it is leading to a collision.\n",
    "Thus, frequently it is easy to detect the best action, and for most of the states we don't need to explore a lot to find it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| #Episodes | 500 | 1000 | 2000 | 4000 |\n",
    "|------|----|---|---|---|\n",
    "| GIF - Coward | ![QL-R2_500](./gif/QL-R2_500.gif) | ![QL-R2_1000](./gif/QL-R2_1000.gif) | ![QL-R2_2000](./gif/QL-R2_2000.gif) | ![QL-R2_4000](./gif/QL-R2_4000.gif) |\n",
    "| GIF - Champion | ![QL-R3_500](./gif/QL-R3_500.gif) | ![QL-R3_1000](./gif/QL-R3_1000.gif) | ![QL-R3_2000](./gif/QL-R3_2000.gif) | ![QL-R3_4000](./gif/QL-R3_4000.gif) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: Explain the gifs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Monte Carlo algorithm receives the $\\gamma$ and the $N0$ parameters.\n",
    "\n",
    "$\\gamma$ is the discount factor; This parameters determines the importance of future rewards. A value of 0 makes the agent short-sighned by only considering current rewards, while a factor approaching 1 will make it strive for a long term reward.  \n",
    "\n",
    "The $N0$ parameter is used to define the agent's exploration rate $\\epsilon$, where $\\epsilon = N0/(N0+N_{sa})$ and $N_{sa}$ is the number of visits in the state $s$ where the action $a$ was taken.\n",
    "\n",
    "In the algorithm, the value function is initialized to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env, initial_state = environment.get_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = agents.MonteCarloControl(gamma=GAMMA, available_actions=AVAILABLE_ACTIONS, N0=N0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MonteCarloES(RAM_mask: List[int], render: bool=False):\n",
    "    epi = episode.generate_episode(env\n",
    "                                   , reduce_state=reduce_state\n",
    "                                   , reward_policy=reward_policy\n",
    "                                   , agent=agent\n",
    "                                   , RAM_mask=RAM_mask\n",
    "                                   , render=render)\n",
    "    return agent.update_policy(epi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "total_rewards = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "n_runs = 1000\n",
    "\n",
    "for i in range(n_runs):\n",
    "    render = i % 201 == 200\n",
    "\n",
    "    score, total_reward = MonteCarloES(RAM_mask=RAM_mask, render=render)\n",
    "\n",
    "    scores.append(score)\n",
    "    total_rewards.append(total_reward)\n",
    "\n",
    "    print_result(i, scores, total_reward, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BHaOs0-xx9La"
   },
   "source": [
    "### Influence of the number of actions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the goal of the game is to cross all the lanes, we propose that it would be enought if the chicken use just the two actions move **up** or **stay** and never move **down**.\n",
    "<br>\n",
    "As we did for the Q-learning agent, in order to test our hypothesis also for Monte Carlo control, we ran the agent using 3 and 2 actions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_2act = read_int_array_from_file(\"./MC/MC_scores_2act.txt\")\n",
    "scores_3act = read_int_array_from_file(\"./MC/MC_scores_3act.txt\")\n",
    "\n",
    "total_rewards_2act = read_int_array_from_file(\"./MC/MC_total_rewards_2act.txt\")\n",
    "total_rewards_3act = read_int_array_from_file(\"./MC/MC_total_rewards_3act.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = [baseline_mean_score for i in range(4000)]\n",
    "\n",
    "ax = aux_plots.plot_2scores(scores_3act[:4000], scores_2act[:4000], \"3 actions (up, down or stay)\", \"2 actions (up or stay)\")\n",
    "\n",
    "aux_plots.moving_average(scores_2act[:4000], ax, label='2 actions moving avg', color='darkmagenta')\n",
    "aux_plots.moving_average(scores_3act[:4000], ax, label='3 actions moving avg', color='darkcyan')\n",
    "aux_plots.moving_average(baseline[:4000], ax, label='Baseline', color='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KUX1Bnz1x9Ll"
   },
   "source": [
    "### Influence of the reward values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we did for the Q-learning agent, in order to investigate the influence of the reward values in the agent behavior also for Monte Carlo control, we ran the algorithm using three different set of reward values.\n",
    "\n",
    "**R1:** Positive rewards for crossing: it offered a sparse reward to the agent: +1 if the chicken cross all the lanes.\n",
    "\n",
    "**R2:** Positive rewards for crossing and negative reward for colliding: it given +1 for crossing all the lanes and -1 if the chicken collide. Thus, we expect the agent to be encouraged to cross the lanes (positive reward) and to avoid collisions (negative reward).\n",
    "\n",
    "**R3:** Huge positive reward for crossing and negative reward for colliding or staying in the same position: it increased the reward for crossing all the lanes to +500 and give -10 when the chicken collide. It also gives a negative reward of -1 when the agent decide to still. Thus we expect the chicken to be incentivate to cross the lanes more rapidilly and increase the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_2act_R1 = read_int_array_from_file('MC/MC_scores_2act_R1.txt')\n",
    "scores_2act_R2 = read_int_array_from_file('MC/MC_scores_2act_R2.txt')\n",
    "scores_2act_R3 = read_int_array_from_file('MC/MC_scores_2act_R3.txt')\n",
    "\n",
    "total_rewards_2act_R1 = read_int_array_from_file('MC/MC_total_rewards_2act_R1.txt')\n",
    "total_rewards_2act_R2 = read_int_array_from_file('MC/MC_total_rewards_2act_R2.txt')\n",
    "total_rewards_2act_R3 = read_int_array_from_file('MC/MC_total_rewards_2act_R3.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = [baseline_mean_score for i in range(4000)]\n",
    "\n",
    "ax = aux_plots.plot_3scores(scores_2act_R1[:4000], scores_2act_R2[:4000], scores_2act_R3[:4000], \"R1 (+1 if cross)\", \"R2 (+1 if cross, -1 if collide)\", \"R3 (+500 if cross, -10 if collide, -1 if stay)\")\n",
    "\n",
    "aux_plots.moving_average(scores_2act_R1[:4000], ax, label=\"R1 (+1 if cross)\", color='darkcyan')\n",
    "aux_plots.moving_average(scores_2act_R2[:4000], ax, label=\"R2 (+1 if cross, -1 if collide)\", color='darkmagenta')\n",
    "aux_plots.moving_average(scores_2act_R3[:4000], ax, label=\"R3 (+500 if cross, -10 if collide, -1 if stay)\", color='#FF3341')\n",
    "aux_plots.moving_average(baseline, ax, label='Baseline', color='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = aux_plots.plot_3rewards(total_rewards_2act_R1[:4000], total_rewards_2act_R2[:4000], total_rewards_2act_R3[:4000], \"R1 (+1 if cross)\", \"R2 (+1 if cross, -1 if collide)\", \"R3 (+500 if cross, -10 if collide, -1 if stay)\")\n",
    "\n",
    "aux_plots.moving_average(total_rewards_2act_R1[:4000], ax, label=\"R1 (+1 if cross)\", color='#1F8FFF')\n",
    "aux_plots.moving_average(total_rewards_2act_R2[:4000], ax, label=\"R2 (+1 if cross, -1 if collide)\", color='#9C4BE7')\n",
    "aux_plots.moving_average(total_rewards_2act_R3[:4000], ax, label=\"R3 (+500 if cross, -10 if collide, -1 if stay)\", color='#FF3341')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the scales of the rewards vary a lot, we are separating R3 from R1 and R2, so we can compare than easily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: valores negativos errados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = aux_plots.plot_2rewards(total_rewards_2act_R1[:4000], total_rewards_2act_R2[:4000], \"R1 (+1 if cross)\", \"R2 (+1 if cross, -1 if collide)\")\n",
    "\n",
    "aux_plots.moving_average(total_rewards_2act_R1[:4000], ax, label=\"R1 (+1 if cross)\", color='#1F8FFF')\n",
    "aux_plots.moving_average(total_rewards_2act_R2[:4000], ax, label=\"R2 (+1 if cross, -1 if collide)\", color='#9C4BE7')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xjXbOOO_x9Lt"
   },
   "source": [
    "### Influence of the discount factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The discount factor $\\gamma$ determines how much the agent cares about rewards in the distant future relative to those in the immediate future.  \n",
    "\n",
    "If $\\gamma$=0, the agent will be completelly myopic and only learn about actions that produce an immediate reward.If $\\gamma$=1, the agent will evaluate each of its actions based on the sum of total of all futures rewards.\n",
    "\n",
    "We used a $\\gamma$ value of 0.99 in order to make our agent care about distant future and we also decreased this value to 0.90 and 0.75 to see how they can impact the agent behavior. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we will be experimenting with 3 different parameters set:\n",
    "\n",
    "| Parameter | G1 | G2 | G3 |\n",
    "|------|----|----|----|\n",
    "| `GAMMA` | 0.99 | 0.90 | 0.75 |\n",
    "| `AVAILABLE_ACTIONS` | 2 | 2 | 2 |\n",
    "| `N0` | 2.5 | 2.5 | 2.5 |\n",
    "| `CROSS` | 500 | 500 | 500 |\n",
    "| `COLLISION` | -10 | -10 | -10 |\n",
    "| `STILL` | -1 | -1 | -1 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_2act_gamma_0p99 = read_int_array_from_file('./MC/MC_scores_2act_R3.txt')\n",
    "scores_2act_gamma_0p9 = read_int_array_from_file('./MC/MC_scores_2act_gamma_0p9.txt')\n",
    "scores_2act_gamma_0p75 = read_int_array_from_file('./MC/MC_scores_2act_gamma_0p75.txt')\n",
    "\n",
    "total_rewards_2act_gamma_0p99 = read_int_array_from_file('./MC/MC_total_rewards_2act_R3.txt')\n",
    "total_rewards_2act_gamma_0p9 = read_int_array_from_file('./MC/MC_total_rewards_2act_gamma_0p9.txt')\n",
    "total_rewards_2act_gamma_0p75 = read_int_array_from_file('./MC/MC_total_rewards_2act_gamma_0p75.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = [baseline_mean_score for i in range(4000)]\n",
    "\n",
    "ax = aux_plots.plot_3scores(scores_2act_gamma_0p99[:4000], scores_2act_gamma_0p9[:4000], scores_2act_gamma_0p75[:4000], \"gamma=0.99\", \"gamma=0.90\", \"gamma=0.75\")\n",
    "\n",
    "aux_plots.moving_average(scores_2act_gamma_0p99[:4000], ax, label=\"gamma=0.99\", color='darkcyan')\n",
    "aux_plots.moving_average(scores_2act_gamma_0p9[:4000], ax, label=\"gamma=0.90\", color='darkmagenta')\n",
    "aux_plots.moving_average(scores_2act_gamma_0p75[:4000], ax, label=\"gamma=0.75\", color='#FF3341')\n",
    "aux_plots.moving_average(baseline, ax, label='Baseline', color='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = aux_plots.plot_3rewards(total_rewards_2act_gamma_0p99[:4000], total_rewards_2act_gamma_0p9[:4000], total_rewards_2act_gamma_0p75[:4000], \"gamma=0.99\", \"gamma=0.90\", \"gamma=0.75\")\n",
    "\n",
    "aux_plots.moving_average(total_rewards_2act_gamma_0p99[:4000], ax, label=\"gamma=0.99\", color='darkcyan')\n",
    "aux_plots.moving_average(total_rewards_2act_gamma_0p9[:4000], ax, label=\"gamma=0.90\", color='darkmagenta')\n",
    "aux_plots.moving_average(total_rewards_2act_gamma_0p75[:4000], ax, label=\"gamma=0.75\", color='#FF3341')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wDjX3Uhqx9Lp"
   },
   "source": [
    "### Influence of the $N0$ parameter that is used to define the agent's exploration rate  $\\epsilon$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exploration rate is the probability that our agent will explore the environment rather than exploit it.  \n",
    "The $𝑁0$ parameter is used to define the agent's exploration rate $\\epsilon$ , where  $\\epsilon = N0/(N0+n)$ and $n$ is the number of times that state has been visited.  \n",
    "<br>\n",
    "As we did for the Q-learning agent, we used a $N0$ value very small (0.001) in order to make our agent short-sighned by only considering immediate rewards, and also used another two values of 2.5 and 5 to compare the results and see how they can impact the agent behavior. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_2act_N0_0 = read_int_array_from_file('./MC/MC_scores_2act_N0_0.txt')\n",
    "scores_2act_N0_25 = read_int_array_from_file('./MC/MC_scores_2act_R3.txt')\n",
    "scores_2act_N0_5 = read_int_array_from_file('./MC/MC_scores_2act_N0_5.txt')\n",
    "\n",
    "total_rewards_2act_N0_0 = read_int_array_from_file('./MC/MC_total_rewards_2act_N0_0.txt')\n",
    "total_rewards_2act_N0_25 = read_int_array_from_file('./MC/MC_total_rewards_2act_R3.txt')\n",
    "total_rewards_2act_N0_5 = read_int_array_from_file('./MC/MC_total_rewards_2act_N0_5.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = [baseline_mean_score for i in range(4000)]\n",
    "\n",
    "ax = aux_plots.plot_3scores(scores_2act_N0_0[:4000], scores_2act_N0_25[:4000], scores_2act_N0_5[:4000], \"N0=0.001\", \"N0=2.50\", \"N0=5.00\")\n",
    "\n",
    "aux_plots.moving_average(scores_2act_N0_0[:4000], ax, label=\"N0=0.001\", color='darkcyan')\n",
    "aux_plots.moving_average(scores_2act_N0_25[:4000], ax, label=\"N0=2.50\", color='darkmagenta')\n",
    "aux_plots.moving_average(scores_2act_N0_5[:4000], ax, label=\"N0=5.00\", color='#FF3341')\n",
    "aux_plots.moving_average(baseline, ax, label='Baseline', color='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = aux_plots.plot_3rewards(total_rewards_2act_N0_0[:4000], total_rewards_2act_N0_25[:4000], total_rewards_2act_N0_5[:4000], \"N0=0.001\", \"N0=2.50\", \"N0=5.00\")\n",
    "\n",
    "aux_plots.moving_average(total_rewards_2act_N0_0[:4000], ax, label=\"N0=0.001\", color='darkcyan')\n",
    "aux_plots.moving_average(total_rewards_2act_N0_25[:4000], ax, label=\"N0=2.50\", color='darkmagenta')\n",
    "aux_plots.moving_average(total_rewards_2act_N0_5[:4000], ax, label=\"N0=5.00\", color='#FF3341')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixed epsilon Monte Carlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: Explain why we are doing this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env, initial_state = environment.get_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = agents.MonteCarloControlFixedEpsilon(gamma=GAMMA, available_actions=AVAILABLE_ACTIONS, epsilon=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MonteCarloES(RAM_mask: List[int], render: bool=False):\n",
    "    epi = episode.generate_episode(env\n",
    "                                   , reduce_state=reduce_state\n",
    "                                   , reward_policy=reward_policy\n",
    "                                   , agent=agent\n",
    "                                   , RAM_mask=RAM_mask\n",
    "                                   , render=render)\n",
    "    return agent.update_policy(epi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "total_rewards = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "n_runs = 1000\n",
    "\n",
    "for i in range(n_runs):\n",
    "    render = i % 201 == 200\n",
    "\n",
    "    score, total_reward = MonteCarloES(RAM_mask=RAM_mask, render=render)\n",
    "\n",
    "    scores.append(score)\n",
    "    total_rewards.append(total_reward)\n",
    "\n",
    "    print_result(i, scores, total_reward, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_plots.plot_scores(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_plots.plot_rewards(total_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SARSA($\\lambda$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env, initial_state = environment.get_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = agents.SarsaLambda(gamma=GAMMA, available_actions=AVAILABLE_ACTIONS, N0=N0, lambd=LAMBD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "total_rewards = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "n_runs = 1\n",
    "\n",
    "for i in range(n_runs):\n",
    "    render = i % 200 == 199\n",
    "    \n",
    "    agent.reset_E()\n",
    "\n",
    "    game_over = False\n",
    "    state = env.reset()\n",
    "    state = reduce_state(state)[RAM_mask].data.tobytes()  # Select useful bytes\n",
    "    action = agent.act(state)\n",
    "    \n",
    "    score = 0\n",
    "    total_reward = 0\n",
    "\n",
    "    while not game_over:\n",
    "        if render:\n",
    "            time.sleep(0.005)\n",
    "            env.render()\n",
    "\n",
    "        old_state = state\n",
    "        old_action = action\n",
    "        ob, reward, game_over, _ = env.step(action)\n",
    "\n",
    "        ob = reduce_state(ob)\n",
    "        reward = reward_policy(reward, ob, action)\n",
    "\n",
    "        total_reward += reward\n",
    "\n",
    "        if reward == reward_policy.REWARD_IF_CROSS:\n",
    "            score += 1\n",
    "\n",
    "        state = ob[RAM_mask].data.tobytes()\n",
    "\n",
    "        action = agent.act(state)  # Next action\n",
    "\n",
    "        agent.update_Q(old_s=old_state, new_s=state, old_a=old_action, new_a=action, reward=reward)\n",
    "\n",
    "    scores.append(score)\n",
    "    total_rewards.append(total_reward)\n",
    "\n",
    "    print_result(i, scores, total_reward, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Influence of the $\\lambda$ parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear function approximators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous algorithms aim to solve control problems using a model free approach that depends on Q tables, which are structures that store the values associated with how good it is to take an action A in a state S, called Q values.\n",
    "\n",
    "Although using Q tables are good for solving small learning problems, they suffer with a trouble called the curse of dimensionality, in which even some small environments can generate a huge amount of possible states, requiring a large amount of memory that is not available. This drawback often prevents those techniques to be used in many tasks from real life that could benefit from it.\n",
    "\n",
    "One of the solutions developed to deal with this problem is Function Approximation. On it, instead of trying to find the optimal values for a very large table, we try to find the best parameters for a parameterized function whose objective is to approximate the optimal values that we would find on that table.\n",
    "\n",
    "Mathematically, we say that we have a family of parameterized functions $\\mathcal{Q}$ given by $Q_{\\theta}: S\\times A \\rightarrow \\mathbb{R}$, where $\\theta$ is an array of parameters in $\\mathbb{R}^d$, called weights, and $d << |S|$. Given that, the objective of a function approximator is to find the array of weights $\\theta^*$ that produces the $Q_{\\theta}^*$ that better approximates the optimal Q values ($Q^*$) for the problem addressed. Some of the greatest advantages of these algorithms is that they learn to generalize for unseen states and requires a much smaller set of values to be learned (d instead of |S|).\n",
    "\n",
    "For this project, we experimented with linear function approximators, which are given by the following equation, where $\\hat{q}(s,a)$ is the approximated value of choosing action **a** in state **s**, $\\theta_i$ is the i-th element of the array of parameters and $x_i$ is a function that turns the action-state pair into the i-th feature of a d-dimensional feature array.\n",
    "\n",
    "$\\hat{q}(s,a,\\theta) = \\sum_{i = 1}^d \\theta_i*x_i(s,a)$\n",
    "\n",
    "In order to discover the best parameters, we applied a Stochastic Gradient Descent algorithm to update the weights, given by the equation below:\n",
    "\n",
    "$\\theta = \\theta + \\alpha*(target - \\hat{q}(S_t, A_t, \\theta))*x(S_t, A_t)$\n",
    "\n",
    "The **target** is the value that our function approximator tries to achieve at each update and it changes for each algorithm approximated (Monte Carlo, Q Learning and Sarsa Lambda), and $\\alpha$ is a learning step size.\n",
    "\n",
    "One of the critical points of function approximators is the choice of the features to be used to represent the states. Depending on their choice, we can build good or bad function approximators. For this project, we experimented with a set of different features, as described in the next sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAM_full_mask=[14  # Chicken Y\n",
    "    , 16  # Chicken Lane Collide\n",
    "    , 18  # Chicken Collision flag (with the bottom car)\n",
    "    , 22  # Car X Direction\n",
    "    , 23, 24, 25, 26, 27, 28, 29, 30, 31, 32  # Z Car Patterns\n",
    "    , 33, 34, 35, 36, 37, 38, 39, 40, 41, 42  # Car Motion Timmers\n",
    "    , 43, 44, 45, 46, 47, 48, 49, 50, 51, 52  # Car Motions\n",
    "    , 87, 88  # Car Shape Ptr\n",
    "    # TODO: test if this makes any difference\n",
    "    , 89, 90  # Chicken Shape Ptr\n",
    "    # TODO: test if this makes any difference\n",
    "    #, 106, 107  # Chicken Sounds\n",
    "    , 108, 109, 110, 111, 112, 113, 114, 115, 116, 117]  # Car X Coords\n",
    "\n",
    "\n",
    "def reduce_state_abs_neg(ob):\n",
    "    # Doesn't matter where we were hit\n",
    "    ob[16] = 1 if ob[16] != 255 else 0\n",
    "\n",
    "    # Reduce chicken y-position\n",
    "    ob[14] = ob[14] // 3\n",
    "\n",
    "    for b in range(108, 118):\n",
    "        ob[b] = abs(ob[b] - 50)\n",
    "        \n",
    "        if b <= 112:\n",
    "            ob[b] = -ob[b]\n",
    "     \n",
    "        ob[b] = ob[b] // 3\n",
    "\n",
    "    return ob\n",
    "\n",
    "\n",
    "def runGame(env, agent, mask, reduce_state, reduce=True):\n",
    "    \n",
    "    actions = []\n",
    "    score = 0\n",
    "    total_reward = 0\n",
    "    \n",
    "    state = env.reset()\n",
    "    if reduce:\n",
    "        state = reduce_state(state)[mask].data.tobytes()  # Select useful bytes\n",
    "    else:\n",
    "        state = state[mask].data.tobytes()\n",
    "        \n",
    "    action = agent.act(state)\n",
    "    actions.append(action)\n",
    "    \n",
    "    game_over = False\n",
    "    render  = True\n",
    "    while not game_over:\n",
    "        if render:\n",
    "            time.sleep(0.0025)\n",
    "            env.render()\n",
    "\n",
    "        old_state = state\n",
    "        ob, reward, game_over, _ = env.step(action)\n",
    "        \n",
    "        if reward == 1:\n",
    "            score+=1\n",
    "        total_reward+=reward\n",
    "        \n",
    "        if reduce:\n",
    "            state = reduce_state(ob)[mask].data.tobytes()\n",
    "        else:\n",
    "            state = ob[mask].data.tobytes()\n",
    "            \n",
    "        action = agent.act(state)\n",
    "        actions.append(action)\n",
    "    \n",
    "    return score, total_reward, actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q Learning Function Approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Q learning function approximator, we decided to use the best parameters found for the Q learning algorithm shown before. They were: \n",
    "- $\\gamma = 0,99$\n",
    "- N0 = 0,001\n",
    "- 2 available actions (move up and stay)\n",
    "- A reward of 500 if the chicken crosses the street, a reward of -10 if it collides with a car and a reward of -1 if the chicken decides to stay.\n",
    "\n",
    "One important detail is that, for the function approximator we decided to use a fixed step size $\\alpha = 0,00001$. It happened because, when we were using the time varying step size used for the previous algorithms, its values were still much bigger than the necessary for the function approximator error converge, making the score and reward curves decrease instead of increase. With the smaller fixed value of $\\alpha$, the updates started to help the algorithm to learn.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment A (Original approximator)\n",
    "\n",
    "In the figures in this section, we can see the scores and the rewards achieved by the function approximator when it ran for 6000 episodes. As we can see, the scores and rewards were smaller than those achieved for the best Q learning algorithm, and the scores are close to those found for the baseline, which just moves up. \n",
    "\n",
    "We also can see that, in the first 500 episodes, the agent achieves bigger scores, with a mean of 23 and maximum value achieving 27 points. However, soon the agent starts to get worse scores and they seem to stabilize close to 21,5 after a few more episodes, varying just a little around it. Also, it is possible to see that the original curve spans a more behaved range of scores. \n",
    "\n",
    "The mean score and reward achieved by this agent were 21,49 and 10043,16, respectively. This results indicates that the agent is possibly learning to move up almost every time, what is close to the behavior of the baseline. To check this hypothesis, we watched a sample episode of this agent and saw that the agent chose to move up in all time steps. One possible explanation for it is the small value of N0, which implies a smaller exploration by the agent that seems it is not being enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './serialized_models/'\n",
    "model_exp08 = serializer.Experiment.load_experiment(path+'exp08_model_baseline_ql_approximator_6000_2020_12_26_10_07_09.dill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_score_exp08 = np.sum(model_exp08.scores)/len(model_exp08.scores)\n",
    "mean_reward_exp08 = np.sum(model_exp08.total_rewards)/len(model_exp08.total_rewards)\n",
    "mean_weights_exp08 = np.sum(model_exp08.agent.W)/len(model_exp08.agent.W)\n",
    "print('Mean score: {}, Mean reward: {}, Mean weights: {}'.format(mean_score_exp08, mean_reward_exp08, mean_weights_exp08))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_plots.plot_scores_mean(model_exp08.scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_plots.plot_rewards_mean(model_exp08.total_rewards, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env, initial_state = environment.get_env()\n",
    "model_exp08.agent.trainScaler(env, RAM_mask)\n",
    "\n",
    "score_exp08, reward_exp08, actions_exp08 = runGame(env,model_exp08.agent, RAM_mask, reduce_state, True)\n",
    "env.close()\n",
    "\n",
    "print('Score: {}, Reward: {}, Actions: {}'.format(score_exp08, reward_exp08, np.unique(actions_exp08, return_counts=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment B (Features without reduce)\n",
    "\n",
    "In order to analyse the influence of the features used to represent the state-action pairs, we decided to perform some experiments varying these features and keeping those hyperparameters. \n",
    "\n",
    "So, the next experiment we performed was the creation of an agent using no reduction to represent the states. We hypothesized that maybe if the agent had a wider view of the environment, it could help it to take better decisions.\n",
    "\n",
    "This time, the mean score and reward were 21,39 and 9942,23, respectively, which are smaller than those achieved by the previous agent. This indicates that the pre-processed states built with the reduced features provides much better information to the agent than using the raw values obtained from the RAM. However, we can see that the agent scores start to decrease a little later and that the score curve is less behaved than that of the previous agent. This can indicate that the learning process of this agent is slower than that of the original agent. Again, running an episode of the game, the agent always chose to move up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './serialized_models/'\n",
    "model_exp04 = serializer.Experiment.load_experiment(path+'exp04_model_no_reduce_ql_approximator_6000_2020_12_26_00_25_50.dill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_score_exp04 = np.sum(model_exp04.scores)/len(model_exp04.scores)\n",
    "mean_reward_exp04 = np.sum(model_exp04.total_rewards)/len(model_exp04.total_rewards)\n",
    "mean_weights_exp04 = np.sum(model_exp04.agent.W)/len(model_exp04.agent.W)\n",
    "print('Mean score: {}, Mean reward: {}, Mean weights: {}'.format(mean_score_exp04, mean_reward_exp04, mean_weights_exp04))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_plots.plot_scores_mean(model_exp04.scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_plots.plot_rewards_mean(model_exp04.total_rewards, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env, initial_state = environment.get_env()\n",
    "model_exp04.agent.trainScaler(env, RAM_mask)\n",
    "\n",
    "score_exp04, reward_exp04, actions_exp04 = runGame(env,model_exp04.agent, RAM_mask, reduce_state, False)\n",
    "env.close()\n",
    "\n",
    "print('Score: {}, Reward: {}, Actions: {}'.format(score_exp04, reward_exp04, np.unique(actions_exp04, return_counts=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment C (Distance features)\n",
    "\n",
    "Another idea of features that we tried to use with the function approximators was to show to the agent not the position, but the distance each car was from it. Also, we hypothesized that, once half of the cars walk in a direction while the other half walk in the opposite direction, show this to the agent, turning negative the first half of the distances, could also help the agent to better understand the environment and choose better actions.\n",
    "\n",
    "As we can see in the figures below, these features bring better results than using no pre-processing on the states, but achieved slightly worse results than the original agent, with a mean score and a mean reward of 21,47 and 10025,16. We also can see that the scores started to decrease earlier than the two others seen before and that the curve is a little less behaved than the original, especially in the latter episodes. Again, running an episode, the agent always chooses to move up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './serialized_models/'\n",
    "model_exp10 = serializer.Experiment.load_experiment(path+'exp10_model_abs_neg_div3_ql_approximator_6000_2020_12_26_10_16_00.dill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_score_exp10 = np.sum(model_exp10.scores)/len(model_exp10.scores)\n",
    "mean_reward_exp10 = np.sum(model_exp10.total_rewards)/len(model_exp10.total_rewards)\n",
    "mean_weights_exp10 = np.sum(model_exp10.agent.W)/len(model_exp10.agent.W)\n",
    "print('Mean score: {}, Mean reward: {}, Mean weights: {}'.format(mean_score_exp10, mean_reward_exp10, mean_weights_exp10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_plots.plot_scores_mean(model_exp10.scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_plots.plot_rewards_mean(model_exp10.total_rewards, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env, initial_state = environment.get_env()\n",
    "model_exp10.agent.trainScaler(env, RAM_mask)\n",
    "\n",
    "score_exp10, reward_exp10, actions_exp10 = runGame(env,model_exp10.agent, RAM_mask, reduce_state_abs_neg, True)\n",
    "env.close()\n",
    "\n",
    "print('Score: {}, Reward: {}, Actions: {}'.format(score_exp10, reward_exp10, np.unique(actions_exp10, return_counts=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment D (Distance features and more RAM)\n",
    "\n",
    "The next experiment we performed tried to see if passing more information about the environment to the agent, through more RAM bytes that encodes, for example, chicken shapes and car timers, could help in its learning process. For this, we kept the states preprocessing of the previous agent, but we gave more RAM positions.\n",
    "\n",
    "As can be seen in the figures below, the agent score still starts to decrease earlier, but the curve is less noisy for the latter episodes. The mean score and reward for this agent were 21,44 and 10007,21. Again, the agent decided to always move up when running an episode.\n",
    "\n",
    "\n",
    "As we could see for the four experiments discussed, the agent is not improving over the baseline. Despite that, the changes in the features generated different behaviors for the agents in the earlier episodes, even converging to a similar bad behavior in the last episodes. Moreover, the creation of more noisy or well behaved score and reward curves indicates the influence of the exploration policy adopted. More well behaved curves indicates that the agent is trying more the best action than exploring other possibilities. More noisy curves could indicate that the presented features could help the agent to achieve better results, probably, if it didn’t always choose the best action.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './serialized_models/'\n",
    "model_exp09 = serializer.Experiment.load_experiment(path+'exp09_model_all_ram_abs_neg_ql_approximator_6000_2020_12_26_10_02_43.dill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_score_exp09 = np.sum(model_exp09.scores)/len(model_exp09.scores)\n",
    "mean_reward_exp09 = np.sum(model_exp09.total_rewards)/len(model_exp09.total_rewards)\n",
    "mean_weights_exp09 = np.sum(model_exp09.agent.W)/len(model_exp09.agent.W)\n",
    "print('Mean score: {}, Mean reward: {}, Mean weights: {}'.format(mean_score_exp09, mean_reward_exp09, mean_weights_exp09))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_plots.plot_scores_mean(model_exp09.scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_plots.plot_rewards_mean(model_exp09.total_rewards, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env, initial_state = environment.get_env()\n",
    "model_exp09.agent.trainScaler(env, RAM_full_mask)\n",
    "\n",
    "score_exp09, reward_exp09, actions_exp09 = runGame(env,model_exp09.agent, RAM_full_mask, reduce_state_abs_neg, True)\n",
    "env.close()\n",
    "\n",
    "print('Score: {}, Reward: {}, Actions: {}'.format(score_exp09, reward_exp09, np.unique(actions_exp09, return_counts=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment E (Exploring more)\n",
    "\n",
    "Given the results obtained with the hyperparameters shown before, we decided to let the Q learning approximator to explore more, now giving N0 = 2,5. \n",
    "\n",
    "In the figures below, we can see that the score and reward curves changed their shapes, not presenting that big decrease in the first episodes. The bigger exploration manifests in the noisier curves, showing that sometimes the agent chooses actions that drive it to greater scores and rewards, and other times its decisions generate bad results, as expected from an agent that is exploring more.\n",
    "\n",
    "This time, the mean score and reward were 22,61 and 10560,47, which are better than the baseline and the previous results, but still worse than the best results achieved for the Q learning model without function approximation. Moreover, now the agent chooses to don’t move in some time steps. For a sample running, the agent chose to don’t move for 14 of the 2754 time steps. These results indicate that now we were able to build an agent that decides better when to take a given action in a given situation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './serialized_models/'\n",
    "model_exp02 = serializer.Experiment.load_experiment(path+'exp02_model_r3_ql_approximator_6000_2020_12_25_12_00_15.dill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_score_exp02 = np.sum(model_exp02.scores)/len(model_exp02.scores)\n",
    "mean_reward_exp02 = np.sum(model_exp02.total_rewards)/len(model_exp02.total_rewards)\n",
    "mean_weights_exp02 = np.sum(model_exp02.agent.W)/len(model_exp02.agent.W)\n",
    "print('Mean score: {}, Mean reward: {}, Mean weights: {}'.format(mean_score_exp02, mean_reward_exp02, mean_weights_exp02))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_plots.plot_scores_mean(model_exp02.scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_plots.plot_rewards_mean(model_exp02.total_rewards, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setattr(model_exp02.agent, 'feat_type', 'all')\n",
    "env, initial_state = environment.get_env()\n",
    "model_exp02.agent.trainScaler(env, RAM_mask)\n",
    "\n",
    "score_exp02, reward_exp02, actions_exp02 = runGame(env,model_exp02.agent, RAM_mask, reduce_state, True)\n",
    "env.close()\n",
    "\n",
    "print('Score: {}, Reward: {}, Actions: {}'.format(score_exp02, reward_exp02, np.unique(actions_exp02, return_counts=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment F (-50 for collision)\n",
    "\n",
    "Given that the results of the Q learning approximator were not good enough, compared with the Q learning without the approximation, we tried to perform other changes that could help the agent to learn. Watching some episodes of the created agent, we saw that it was colliding with cars a lot of times. So, we hypothesized that changing the reward function, given a bigger negative value for the collision, would help the agent to learn to collide less.\n",
    "\n",
    "To check it, we performed two more experiments, keeping N0 equal to 2,5. In one of them we gave a reward of -50 when the chicken collided with a car, and in the other we gave a reward of -100.\n",
    "\n",
    "The two figures below show the results for the reward of -50. As we can see, the curve of scores is very similar to that using the original rewards. Nevertheless, the reward curve shows much more variation in its values, as a consequence of the bigger negative reward. These results indicate that this change in the reward function was not able to decrease sufficiently the number of collisions.  The mean score and reward for this agent were 22,54 and 7876,18, which are smaller than that of the agent with the original reward policy. And, again, the agent learned to don’t move in some few situations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './serialized_models/'\n",
    "model_exp11 = serializer.Experiment.load_experiment(path+'exp11_model_r50_approximator_6000_2020_12_26_22_27_33.dill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_score_exp11 = np.sum(model_exp11.scores)/len(model_exp11.scores)\n",
    "mean_reward_exp11 = np.sum(model_exp11.total_rewards)/len(model_exp11.total_rewards)\n",
    "mean_weights_exp11 = np.sum(model_exp11.agent.W)/len(model_exp11.agent.W)\n",
    "print('Mean score: {}, Mean reward: {}, Mean weights: {}'.format(mean_score_exp11, mean_reward_exp11, mean_weights_exp11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_plots.plot_scores_mean(model_exp11.scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_plots.plot_rewards_mean(model_exp11.total_rewards, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env, initial_state = environment.get_env()\n",
    "model_exp11.agent.trainScaler(env, RAM_mask)\n",
    "\n",
    "score_exp11, reward_exp11, actions_exp11 = runGame(env,model_exp11.agent, RAM_mask, reduce_state, True)\n",
    "env.close()\n",
    "\n",
    "print('Score: {}, Reward: {}, Actions: {}'.format(score_exp11, reward_exp11, np.unique(actions_exp11, return_counts=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment G (-100 for collision)\n",
    "\n",
    "Using a reward of -100 when the chicken collides with a car gaves us a score curve that is slightly more stable than using a reward of -50. However, it is still very similar with the score curve built using the original reward policy. The reward curve, on the other hand, has much bigger amplitudes than the other two, as expected, but also has less variant mean values. The mean score and reward for this agent were 22,59 and 4617,35, and it also choses to don’t move in some time steps, but in less time steps than the other two.\n",
    "\n",
    "With these results we could see that changes in the reward policy was not good enough to prevent the chicken from colliding and, consequently, achieving better scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './serialized_models/'\n",
    "model_exp12 = serializer.Experiment.load_experiment(path+'exp12_model_r100_approximator_6000_2020_12_26_23_04_24.dill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_score_exp12 = np.sum(model_exp12.scores)/len(model_exp12.scores)\n",
    "mean_reward_exp12 = np.sum(model_exp12.total_rewards)/len(model_exp12.total_rewards)\n",
    "mean_weights_exp12 = np.sum(model_exp12.agent.W)/len(model_exp12.agent.W)\n",
    "print('Mean score: {}, Mean reward: {}, Mean weights: {}'.format(mean_score_exp12, mean_reward_exp12, mean_weights_exp12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_plots.plot_scores_mean(model_exp12.scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_plots.plot_rewards_mean(model_exp12.total_rewards, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env, initial_state = environment.get_env()\n",
    "model_exp12.agent.trainScaler(env, RAM_mask)\n",
    "\n",
    "score_exp12, reward_exp12, actions_exp12 = runGame(env,model_exp12.agent, RAM_mask, reduce_state, True)\n",
    "env.close()\n",
    "\n",
    "print('Score: {}, Reward: {}, Actions: {}'.format(score_exp12, reward_exp12, np.unique(actions_exp12, return_counts=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions for Q Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the table below we can see a summary of the mean scores and rewards for each experiment.\n",
    "\n",
    "| Experiment | Mean Score | Mean Reward |\n",
    "|:----------:|:----------:|:-----------:|\n",
    "|      A     |    21,49   |   10043,16  |\n",
    "|      B     |    21,39   |   9942,23   |\n",
    "|      C     |    21,47   |   10025,16  |\n",
    "|      D     |    21,44   |   10007,21  |\n",
    "|      E     |    22,61   |   10560,47  |\n",
    "|      F     |    22,54   |   7876,18   |\n",
    "|      G     |    22,59   |   4617,35   |\n",
    "\n",
    "\n",
    "As we could see from the results, the function approximation implemented for the Q learning algorithm didn’t improve the performance of the agents, compared to the Q learning without approximation, as we were expecting. Some factors could explain that, such as the choice of better hyperparameters, but one of the most important is the creation of the features.\n",
    "\n",
    "As discussed in the introduction about function approximation, the features are a key factor for the success of these algorithms, both because they decrease the dimension space of the values we are trying to optimize and because they are responsible for expressing the most important information that the agent will use to learn about the environment.\n",
    "\n",
    "Although the linear function approximators are powerful algorithms for a lot of reinforcement learning tasks, they have some disadvantages. One of them is its lack of capacity to express some kind of relationships between environment variables. For example, if a given variable represents a good event in the environment only in the absence of another, the linear approximator cannot parameterize that. In these cases, the features become responsible for representing the necessary information for the learning process, requiring good feature engineering.\n",
    "\n",
    "Given that and the results achieved with function approximators in this project, we assume that the features chosen by us, probably, were not very representative of our problem, for function approximation. So, for future works, we need to investigate better representations and try with more complex function approximators, such as the nonlinear ones. Despite that, we could see how the exploration capacity of the agent  and the rewards chosen can impact how good it becomes, as well as how the learning step size contributes to the convergence of the algorithms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the table below we can see the gifs generated for 3 out of the 8 experiments performed for the Q Learning approximator: **A**, which uses the original model; **E**, that is the agent with N0 = 2,5; and **G**, where the agent receives a reward of -100 when collides with a car. Each gif executes 1000 time steps and they were produced for agents trained during 500, 1000 and 6000 episodes.\n",
    "\n",
    "As we can see, there is not a big difference between the gifs of the same experiment, especially for those of experiments A and E. This reinforces the assumption that these agents are learning to act as the baseline, which is not the best behavior. However, in the gif for 500 episodes of experiment G, we can see that the agent suffers less collisions than the other examples (approximately 10 instead of 14, as the others), showing that for the first episodes that reward policy may be a little helpful.\n",
    "\n",
    "\n",
    "| Episodes | Experiment A | Experiment E | Experiment G |\n",
    "|:--------:|:------------:|:------------:|:------------:|\n",
    "|    500   |     ![SegmentLocal](./gif/ql_lfa_exp_A_500.gif \"segment\")     |     ![SegmentLocal](./gif/ql_lfa_exp_E_500.gif \"segment\")     |     ![SegmentLocal](./gif/ql_lfa_exp_G_500.gif \"segment\")     |\n",
    "|   1000   |     ![SegmentLocal](./gif/ql_lfa_exp_A_1000.gif \"segment\")     |     ![SegmentLocal](./gif/ql_lfa_exp_E_1000.gif \"segment\")    |     ![SegmentLocal](./gif/ql_lfa_exp_G_1000.gif \"segment\")    |\n",
    "|   6000   |     ![SegmentLocal](./gif/ql_lfa_exp_A_6000.gif \"segment\")     |     ![SegmentLocal](./gif/ql_lfa_exp_E_6000.gif \"segment\")     |     ![SegmentLocal](./gif/ql_lfa_exp_G_6000.gif \"segment\")     |\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SARSA($\\lambda$) Function Approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dionisius/Documents/academics/mestrado/2020_02/reinforcement_learning/venv/lib/python3.7/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator StandardScaler from version 0.23.1 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "exp_adam = serializer.Experiment.load_experiment('./serialized_models/adam_2020_12_27_15_27_35.dill')\n",
    "exp_adam_n25 = serializer.Experiment.load_experiment('./serialized_models/adam_n25_2020_12_27_15_18_08.dill')\n",
    "exp_lfa_n00 = serializer.Experiment.load_experiment('./serialized_models/sarsa_lfa_n00_2020_12_27_15_09_33.dill')\n",
    "exp_lfa_n25 = serializer.Experiment.load_experiment('./serialized_models/sarsa_lfa_n25_2020_12_27_14_19_24.dill')\n",
    "exp_adam_n25_r1 = serializer.Experiment.load_experiment('./serialized_models/adam_r1_2020_12_27_15_13_52.dill')\n",
    "num_eps = 2500\n",
    "blocks = num_eps//100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sarsa With Linear Function Approximation\n",
    "\n",
    "In this experiment we tested a linear function approximation with Sarsa. So for the values of Q(state, action) we substitute for a Q(w, state, action) where w are the weights for the following linear function:\n",
    "\n",
    "$w_1 \\cdot f_1 + w2 \\cdot f2 + ... + w_n \\cdot f_n + w_n + 1$\n",
    "\n",
    "And the weights are updated as follows:\n",
    "delta = reward + gamma * Q(w, new_state, new_action) - Q(w, old_state, old_action)\n",
    "Weights += alpha * delta * (Features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score Sarsa LFA\n",
    "\n",
    "In this first experiment, for the features we chose the y position of the chicken, if has collided and the x positions for the cars. The actions space is to go forward or be still. We chose Gamma=0.99, N0=2.5, alpha=0.001 and the rewords we chose were R3.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Episode', ylabel='Final Score'>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x480 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "s1 = exp_lfa_n00.scores[:num_eps]\n",
    "s2 = exp_lfa_n25.scores[:num_eps]\n",
    "s1_avg100 = []\n",
    "for b in range(blocks-1):\n",
    "    value = statistics.mean(s1[b*100:(b+1)*100])\n",
    "    s1_avg100.append(value)\n",
    "\n",
    "s2_avg100 = []\n",
    "for b in range(blocks-1):\n",
    "    value = statistics.mean(s2[b*100:(b+1)*100])\n",
    "    s2_avg100.append(value)\n",
    "    \n",
    "aux_plots.plot_2scores(s1_avg100, s2_avg100, \"N0=0.001\", \"N0=2.50\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the experiment with 2500 we already can see the convergence for the model. Analyzing the results based on the mean for the 100 last scores in each run, we can see that it passes the baseline, but not by much, the chicken learns a similar strategy as the baseline that is to go forward and not to worry about the collisions. We can observe that the linear model is too simple and it underfits the problem.\n",
    "\n",
    "\n",
    "In this second experiment we changed N0 to 0.001, and gave 2500 iterations. We can see that at first it rises very fast to the peak mean score of about 23, but as the weights keep being influenced to go forward we see that the weights explode to go only forward, going back to a mean score of 21.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scores Sarsa LSA with and without optimizer\n",
    "\n",
    "\n",
    "In this third experiment we used an optimization algorithm to change alpha over time and used a similar strategy as the l2 regularization. The Adam optimization algorithm is an algorithm that update alpha as the experiment runs, so in each iteration we update alpha as follows:\n",
    "self.m = self.beta_1 * self.m + (1 - self.beta_1) * g\n",
    "        self.v = self.beta_2 * self.v + (1 - self.beta_2) * np.power(g, 2)\n",
    "        m_hat = self.m / (1 - np.power(self.beta_1, t))\n",
    "        v_hat = self.v / (1 - np.power(self.beta_2, t))\n",
    "        return self.alpha * m_hat / (np.sqrt(v_hat) + self.epsilon)\n",
    "Where m and v are first and second moment estimators.\n",
    "\n",
    "Inspired by the l2 regularization we also tried to subtract lambda*Weights in the weights update, where lambda is a hyperparameter.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Episode', ylabel='Final Score'>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x480 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "s1 = exp_adam_n25.scores[:num_eps]\n",
    "s2 = exp_lfa_n25.scores[:num_eps]\n",
    "s1_avg100 = []\n",
    "for b in range(blocks-1):\n",
    "    value = statistics.mean(s1[b*100:(b+1)*100])\n",
    "    s1_avg100.append(value)\n",
    "\n",
    "s2_avg100 = []\n",
    "for b in range(blocks-1):\n",
    "    value = statistics.mean(s2[b*100:(b+1)*100])\n",
    "    s2_avg100.append(value)\n",
    "    \n",
    "aux_plots.plot_2scores(s1_avg100, s2_avg100, \"Adam\", \"No optimizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this experiment we ran many values for alpha, beta_1, beta_2 and lambda, and we concluded that Adam can speed up the convergence, but for these two ideas we coulnt see  an improvement of the solution. We ran 2500 iterations with alpha=0.001, beta1=0.9 and beta_2=0.99 and lambda=0.001, and N0 = 0.01, and we see the same conclusion as the previous experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score Adam N0=0.01\" vs \"N0=2.50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We changed N0=2.5 and we compare if Adam works better with N0=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Episode', ylabel='Final Score'>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x480 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "s1 = exp_adam.scores[:num_eps]\n",
    "s2 = exp_adam_n25.scores[:num_eps]\n",
    "s1_avg100 = []\n",
    "for b in range(blocks-1):\n",
    "    value = statistics.mean(s1[b*100:(b+1)*100])\n",
    "    s1_avg100.append(value)\n",
    "\n",
    "s2_avg100 = []\n",
    "for b in range(blocks-1):\n",
    "    value = statistics.mean(s2[b*100:(b+1)*100])\n",
    "    s2_avg100.append(value)\n",
    "    \n",
    "aux_plots.plot_2scores(s1_avg100, s2_avg100, \"N0=0.01\", \"N0=2.50\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward Adam N0=0.01\" vs \"N0=2.50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Episode', ylabel='Final Reward'>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x480 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "s1 = exp_adam.total_rewards[:num_eps]\n",
    "s2 = exp_adam_n25.total_rewards[:num_eps]\n",
    "s1_avg100 = []\n",
    "for b in range(blocks-1):\n",
    "    value = statistics.mean(s1[b*100:(b+1)*100])\n",
    "    s1_avg100.append(value)\n",
    "\n",
    "s2_avg100 = []\n",
    "for b in range(blocks-1):\n",
    "    value = statistics.mean(s2[b*100:(b+1)*100])\n",
    "    s2_avg100.append(value)\n",
    "    \n",
    "    \n",
    "aux_plots.plot_2rewards(s1_avg100, s2_avg100, \"N0=0.01\", \"N0=2.50\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately we see again results similar to the first experiment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score Adam Reward=r1\", \"Reward=r3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Episode', ylabel='Final Score'>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x480 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "s1 = exp_adam_n25_r1.scores[:num_eps]\n",
    "s2 = exp_adam_n25.scores[:num_eps]\n",
    "s1_avg100 = []\n",
    "for b in range(blocks-1):\n",
    "    value = statistics.mean(s1[b*100:(b+1)*100])\n",
    "    s1_avg100.append(value)\n",
    "\n",
    "\n",
    "s2_avg100 = []\n",
    "for b in range(blocks-1):\n",
    "    value = statistics.mean(s2[b*100:(b+1)*100])\n",
    "    s2_avg100.append(value)\n",
    "    \n",
    "aux_plots.plot_2scores(s1_avg100, s2_avg100, \"Reward=r1 modified\", \"Reward=r3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We changed the rewards to r1, in hopes that it could learn to avoid collisions, but it kept falling to very low scores since it was too scared and it was still most of the time. Changing only the reward when a collision happened to -0.5, then it got better but it just got as good as the other ones.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This game requires a more complex model to make an approximation. The linear approximation for Sarsa underfits, maybe with a better approximator it could work better. Most models are influenced to learn to go forward, and cannot serve well to evaluate the state for this game. This is due to the nature of the features and the fact that there is not many of them. Many other types of features were tested, as the distance to the cars, reorganizing to get the distance to the next car and others, still the pure ram worked as good as the others. Another problem is that Sarsa can sometimes lose knowledge since it is not a perfect gradient descent, as we saw in the experiments.\n",
    "\n",
    "An advantage seen was that compared to Sarsa Lambda, this performs very fast, and for simpler cases this approximation can be of big advantage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Thoughts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computational cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the biggest problems on this project was the computational cost of running an episode.\n",
    "\n",
    "Since each run plays for 2 minutes and 16 seconds in the original game, there are quite a lot of frames that need to be computed for each episode.\n",
    "Even though the frame-sync is deactivated in our environment, each time we execute on episode it takes about 2 seconds to compute it for Q-Learn and Monte Carlo, and XXXXXXXXXX seconds for SARSA(λ).\n",
    "\n",
    "The memory usage is fairly low compared to the time that it takes to run the algorithms, even with a decent amount of unique states in our problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..............."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
