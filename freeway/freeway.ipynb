{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO:\n",
    "- [ ] Write the MDP formulation -> Aline\n",
    "- [X] The discretization model adopted\n",
    "  - Será que tá pronto? Falei bastante no State Representation.\n",
    "\n",
    "Aspectos que devem ser abordados:\n",
    "- [ ] computational cost\n",
    "  - [ ] Falar que o gargalo aqui é processamento e não uso de memória\n",
    "- [ ] optimality\n",
    "- [ ] influence of reward function\n",
    "- [X] state and action space sizes\n",
    "\n",
    "Também precisamos definir\n",
    "- [ ] How the problem was modeled\n",
    "  - [ ] Precisa falar alguma coisa aqui?\n",
    "- [ ] Implementation specifics and restrictions\n",
    "  - [ ] Precisa falar alguma coisa aqui?\n",
    "\n",
    "Problem\n",
    "- [X] The nature of your environment (episodic/not episodic, deterministic/stochastic)\n",
    "- [X] What are your terminal states (when they exist)\n",
    "- [X] How is your reward function defined\n",
    "- [X] All parameters employed in your methods (discount factor, step size, etc.)\n",
    "\n",
    "Outros:\n",
    "- [ ] Gerar um requirements.txt completo e instruções confiáveis para instalação.\n",
    "- [ ] Computational cost of SARSA-Lambda (Final Thoughts)\n",
    "- [ ] Plot the optimal value function V ∗ (s) = maxa Q∗(s, a).\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_mean_score = 21.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Freeway"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the first project for the MC935rA/MO436A - Reinforcement Learning course, taught by Prof. Esther Colombini.\n",
    "\n",
    "In this project we propose to apply Reinforcement Learning methods to teach an agent how to play the Freeway Atari game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Group members:**\n",
    "- Aline Gabriel de Almeida\n",
    "- Dionisius Oliveira Mayr (229060)\n",
    "- Leonardo de Oliveira Ramos (171941)\n",
    "- Marianna de Pinho Severo (264960)\n",
    "- Victor Jesús Sotelo Chico (265173)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Freeway game\n",
    "\n",
    "![Baseline 1](./img/Freeway_logo.png)\n",
    "\n",
    "Freeway is a video game written by David Crane for the Atari 2600 and published by Activision [[1]](https://en.wikipedia.org/wiki/Freeway_(video_game)).\n",
    "\n",
    "In the game, two players compete against each other trying to make their chikens cross the street, while evading the cars passing by.\n",
    "There are three possible actions: staying still, moving forward or moving backward.\n",
    "Each time a chicken collides with a car, it is forced back some spaces and takes a while until the chiken regains its control.\n",
    "\n",
    "When a chicken is successfully guided across the freeway, it is awarded one point and moved to the initial space, where it will try to cross the street again.\n",
    "The game offers multiple scenarios with different vehicles configurations (varying the type, frequency and speed of them) and plays for 2 minutes and 16 seconds.\n",
    "During the 8 last seconds the scores will start blinking to indicate that the game is close to end.\n",
    "Whoever has the most points after this, wins the game!\n",
    "\n",
    "The image was extracted from the [manual of the game](https://www.gamesdatabase.org/Media/SYSTEM/Atari_2600/Manual/formated/Freeway_-_1981_-_Zellers.pdf).\n",
    "\n",
    "[1 - Wikipedia - Freeway](https://en.wikipedia.org/wiki/Freeway_(video_game))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the [OpenAI Gym](https://gym.openai.com/) toolkit.\n",
    "This toolkit uses the [Arcade Learning Environment](https://github.com/mgbellemare/Arcade-Learning-Environment) to simulate the game through the [Stella](https://stella-emu.github.io/) emulator.\n",
    "\n",
    "Although the game offers multiple scenarios, we are going to consider only the first one. Also, we will be controlling a *single chicken*, while we try to maximize its score.\n",
    "\n",
    "In this configuration, there are ten lanes and each lane contains exactly one car (with a different speed and direction).\n",
    "Whenever an action is chosen, it is repeated for $k$ frames, $k \\in \\{2, 3, 4\\}$.\n",
    "\n",
    "This means that our environment is **stochastic** and it is also **episodic**, with its terminal state being reached whenever 2 minutes and 16 seconds have passed.\n",
    "\n",
    "Our base state representation is given by the RAM of the Atari 2600 (more on this latter).\n",
    "\n",
    "You can find more information regarding the environment used at [Freeway-ram-v0](https://gym.openai.com/envs/Freeway-ram-v0/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the dependencies:\n",
    "```sh\n",
    "pip install gym\n",
    "pip install gym[atari]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can find a list of useful links and materials that were used during this project.\n",
    "\n",
    "* [Freeway-ram-v0 from OpenAI Gym](https://gym.openai.com/envs/Freeway-ram-v0/)\n",
    "* [Manual of the game](https://www.gamesdatabase.org/Media/SYSTEM/Atari_2600/Manual/formated/Freeway_-_1981_-_Zellers.pdf)\n",
    "* [Freeway Disassembly](http://www.bjars.com/disassemblies.html)\n",
    "* [Atari Ram Annotations](https://github.com/mila-iqia/atari-representation-learning/blob/master/atariari/benchmark/ram_annotations.py)\n",
    "* [Freeway Benchmarks](https://paperswithcode.com/sota/atari-games-on-atari-2600-freeway)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')  # Enable importing from `src` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from collections import defaultdict\n",
    "from functools import lru_cache\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import gym\n",
    "\n",
    "import src.agents as agents\n",
    "import src.episode as episode\n",
    "import src.environment as environment\n",
    "import src.aux_plots as aux_plots\n",
    "import src.serializer as serializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_result(i, scores, total_reward, score):\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Run [{i:4}] - Total reward: {total_reward:7.2f} Mean scores: {sum(scores) / len(scores):.2f} Means Scores[:-10]: {sum(scores[-10:]) / len(scores[-10:]):5.2f} Score: {score:2} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_int_array_from_file(fn: str):\n",
    "    with open(f\"./experiments/{fn}\") as f:\n",
    "        return [int(x) for x in f.read().splitlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Action space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we said above, the agent in this game has three possible actions at each frame, each represented by an integer:\n",
    "\n",
    "* 0: Stay\n",
    "* 1: Move forward\n",
    "* 2: Move backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In theory, a perfect chicken wouldn't ever need to move backward, since it is possible to know if moving forward would lead you into a collision (in the immediate frame or in the future frames).\n",
    "\n",
    "In our project we will be experimenting with agents using only two possible actions (staying and moving forward) and also agents using all the three possible actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State of the art benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image bellow (extracted from https://paperswithcode.com/sota/atari-games-on-atari-2600-freeway) shows the evolution of the scores over time using different techniques.\n",
    "\n",
    "Today, the state of the art approaches are making 34.0 points, using Deep Reinforcement Learning methods.\n",
    "\n",
    "However, since we are using tabular methods, we don't think it will be possible to beat this benchmark.\n",
    "Instead, we will be looking at a different, simpler baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Benchmarks](./img/state_of_art_scores.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple baseline agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a simple baseline, we are using an agent that always moves **up**, regardless of the rewards received or the current state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env, initial_state = environment.get_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent = agents.Baseline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_rewards = []\n",
    "# n_runs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# for i in range(n_runs):\n",
    "#     render = i % 10 == 0\n",
    "\n",
    "#     game_over = False\n",
    "#     state = env.reset()\n",
    "#     action = agent.act(state)\n",
    "\n",
    "#     total_reward = 0\n",
    "\n",
    "#     while not game_over:\n",
    "#         if render:\n",
    "#             time.sleep(0.01)\n",
    "#             env.render()\n",
    "\n",
    "#         ob, reward, game_over, _ = env.step(action)\n",
    "\n",
    "#         total_reward += reward\n",
    "#         action = agent.act(state)  # Next action\n",
    "\n",
    "#     total_rewards.append(total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline_mean_score = np.mean(total_rewards)\n",
    "# baseline_mean_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, this agent usually scores 21 or 23 points (as shown in the images bellow). It depends on the the values of $k$ sampled, and on average it scores about 21.8 points per run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Baseline 1](./img/baseline_1.png)\n",
    "![Baseline 2](./img/baseline_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the tabular methods we are going to use work with some representation of the actual environment state, we will need to understand it better in order to effectively approach this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Atari 2600"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before talking about the state representation, it is important to understand how the Atari 2600 works.\n",
    "\n",
    "Atari 2600 is a video game released in 1977 by the American Atari, Inc.\n",
    "Its **8-bit** microprocessor was of the MOS **6502** family and it had **128 bytes** of RAM.\n",
    "\n",
    "And these 128 bytes are what really matters here.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that Gym gives us the RAM memory of the Atari as the state representation.\n",
    "In other words, it gives us an 128-element `np.array`, where each element of the array is an `uint8` (*integer values ranging from 0 to 255*).\n",
    "\n",
    "That said, we have (in theory) $256^{128} \\approx 1.8 \\cdot 10^{308}$ possible game states!\n",
    "\n",
    "This is *far* from being manageable, and thus we need to come up with a different approch to represent our state if we want our algorithms to converge.\n",
    "\n",
    "One might argue that the RAM state is *sparse* and although that is true, it is still not sparse enough to apply tabular methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting useful bytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try to select only the bytes that are useful to deal with our problem.\n",
    "\n",
    "To do so, we will be looking at a [fan made disassembly](http://www.bjars.com/disassemblies.html) by Glenn Saunders.\n",
    "\n",
    "From the 6502 assembly we can see the variables locations in the memory, their size and count the amount of bytes since the initial offset to determine which byte represents what.\n",
    "\n",
    "Simplifying it a bit, we would end up with a list of candidate bytes for our state representation like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Description| Bytes |\n",
    "|----|----|\n",
    "| Chicken Y | 14 |\n",
    "| Chicken Lane Collide | 16 |\n",
    "| Chicken Collision flag | 18 |\n",
    "| Car X Direction | 22 |\n",
    "| Z Car Patterns | 23, 24, 25, 26, 27, 28, 29, 30, 31, 32 |\n",
    "| Car Motion Timmers | 33, 34, 35, 36, 37, 38, 39, 40, 41, 42 |\n",
    "| Car Motions | 43, 44, 45, 46, 47, 48, 49, 50, 51, 52 |\n",
    "| Car Shape Ptr | 87, 88 |\n",
    "| Chicken Shape Ptr | 89, 90 |\n",
    "| Chicken Sounds | 106, 107 |\n",
    "| Car X Coords | 108, 109, 110, 111, 112, 113, 114, 115, 116, 117 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After observing them, we were able to filter bytes that wouldn't be useful for us (like auxiliary variables used during function calls, e.g. `Car Motion Timmers`, `Chicken Sounds`) and bytes that contain constant values, like `Car X Direction` and`Car Motions`.\n",
    "\n",
    "The final list of bytes being used is given here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Description| Bytes |\n",
    "|----|----|\n",
    "| Chicken Y | 14 |\n",
    "| Chicken Lane Collide | 16 |\n",
    "| Car X Coords | 108, 109, 110, 111, 112, 113, 114, 115, 116, 117 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAM_mask = [\n",
    "      14  # Chicken Y\n",
    "    , 16  # Chicken Lane Collided\n",
    "    , 108, 109, 110, 111, 112, 113, 114, 115, 116, 117  # Car X Coords\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we went from using 128 bytes to only 12, with $256^{12} \\approx 7.9 \\cdot 10^{28}$ theoritical possible states.\n",
    "\n",
    "But this is still a lot.\n",
    "We need to reduce it even more, and that is exactly what the function `reduce_state` bellow does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_state(ob):\n",
    "    # Doesn't matter where we were hit\n",
    "    ob[16] = 1 if ob[16] != 255 else 0\n",
    "\n",
    "    # Reduce chicken y-position\n",
    "    ob[14] = ob[14] // 3\n",
    "\n",
    "    for b in range(108, 118):\n",
    "        # The chicken is in the x-posistion ~49\n",
    "        if ob[b] < 20 or ob[b] > 80:\n",
    "            # We don't need to represent cars far from the chicken\n",
    "            ob[b] = 0\n",
    "        else:\n",
    "            # Reduce the cars x-positions sample space\n",
    "            ob[b] = ob[b] // 3\n",
    "\n",
    "    return ob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using 4 strategies to reduce our state.\n",
    "\n",
    "* The `byte[16]` represents the lane where a collision happened. Instead of using it like this, we will make it binary, 1 being a collision (on any lane) and 0 otherwise.\n",
    "* The `byte[14]` represents the y-position of the chicken (from 0 to ~170). We will be losing a bit of precision here in order to reduce the state space, dividing (truncating the fractional part) the `byte[14]` by 3.\n",
    "* The `bytes[108:118]` are used as the cars x-position. If they are far from the chicken (the chicken x-position is fixed at ~49), we won't care about them, setting it as 0. If they are near the chicken (between x-20 and x-80), we will represent them again dividing by 3. The idea here is that the cars that are near the chicken contain more valuable information than the cars far from it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have $2 \\cdot 57 \\cdot 21^{10} \\approx 1.9 \\cdot 10^{15}$ theoritical possible states.\n",
    "\n",
    "This might seen like it is still too much, but as we will soon see, empirically this number is far smaller.\n",
    "\n",
    "The biggest factor of the $10^{15}$ states is due to the cars x-positions.\n",
    "But recall that each car has a constant speed (which can be different from the other cars). Because of it, their positions are periodical, meaning that they don't use the entire range of possible values in their byte.\n",
    "\n",
    "For instance: a car with speed 4 will *never* be at the x-coordinate 3, nor will a car with speed 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reward Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the base environment we are awarded on point each time we successfully cross the freeway.\n",
    "\n",
    "However, it stands to reason experimenting with other reward strategies, like penalizing collisions or standing still and also changing the numerical value of these quantities.\n",
    "\n",
    "The method bellow allows us to experiment multiple reward policies.\n",
    "\n",
    "We defined some conditions were the agent get a reward: \n",
    "- `REWARD_IF_CROSS`: If the chicken cross the 10 lanes; this is our fundamental goal and should be encouraged;\n",
    "- `REWARD_IF_COLLISION`: If the chicken collides with a car; as the collisions can delay the chicken to cross the lanes, it can be discouraged;\n",
    "- Each one of the agent's three actions can be rewarded according to how much they can help the chicken to cross all the lanes and doing so we can hopefully accelerate the conversion of the algorithms. The `REWARD_IF_STILL` is the reward given if the chicken doesn't  move up, and *can* be discouraged in some cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_policy(reward, ob, action):\n",
    "    if reward == 1:\n",
    "        reward = reward_policy.REWARD_IF_CROSS\n",
    "    elif ob[16] == 1:  # Collision!\n",
    "        reward = reward_policy.REWARD_IF_COLLISION\n",
    "    elif action != 1:  # Don't incentivate staying still\n",
    "        reward = reward_policy.REWARD_IF_STILL\n",
    "\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "AVAILABLE_ACTIONS = 2\n",
    "N0 = 2.5\n",
    "LAMBD = 0.2\n",
    "\n",
    "reward_policy.REWARD_IF_CROSS = 1\n",
    "reward_policy.REWARD_IF_COLLISION = 0\n",
    "reward_policy.REWARD_IF_STILL = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- não vamos treinar todos os modelos durante esse notebook.\n",
    "- já rodamos previamente e vamos colocar aqui apenas plots dos resultados obtidos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Q-learning algorithm receives the $\\gamma$ and the $N0$ parameters.  \n",
    "<br>\n",
    "$\\gamma$ is the discount factor; This parameters determines the importance of future rewards. A value of 0 makes the agent short-sighned by only considering current rewards, while a factor approaching 1 will make it strive for a long term reward.  \n",
    "<br>\n",
    "The $N0$ parameter is used to define the agent's exploration rate $\\epsilon$, where $\\epsilon = N0/(N0+n)$ and $n$ is the number of visits in the states.  \n",
    "<br>\n",
    "In the algorithm, the action-value function is initialized to zero. Then, at each time $t$ the agent selects an action $a_t$, observes a reward $r_t$, enters a new state $s_{s+1}$, and $Q$ is updated:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Q_{new}(s_t, a_t) :=  Q(s_t, a_t)+\\alpha (r_t + \\gamma.max_aQ(s_{t+1},a)-Q(s_t, a_t))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning rate $\\alpha$ is defined as $\\alpha = 1/Nsa$, where $Nsa$ is the number of times that the specific state-action pair has already occured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "env, initial_state = environment.get_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = agents.QLearning(gamma=GAMMA, available_actions=AVAILABLE_ACTIONS, N0=N0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "total_rewards = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run [   0] - Total reward:   10.00 Mean scores: 10.00 Means Scores[:-10]: 10.00 Score: 10 \n",
      "Run [  10] - Total reward:   11.00 Mean scores: 11.91 Means Scores[:-10]: 12.10 Score: 11 \n",
      "Run [  20] - Total reward:   12.00 Mean scores: 12.52 Means Scores[:-10]: 13.20 Score: 12 \n",
      "Run [  30] - Total reward:   15.00 Mean scores: 13.23 Means Scores[:-10]: 14.70 Score: 15 \n",
      "Run [  40] - Total reward:   16.00 Mean scores: 13.88 Means Scores[:-10]: 15.90 Score: 16 \n",
      "Run [  50] - Total reward:   19.00 Mean scores: 14.39 Means Scores[:-10]: 16.50 Score: 19 \n",
      "Run [  60] - Total reward:   19.00 Mean scores: 14.98 Means Scores[:-10]: 18.00 Score: 19 \n",
      "Run [  70] - Total reward:   19.00 Mean scores: 15.32 Means Scores[:-10]: 17.40 Score: 19 \n",
      "Run [  80] - Total reward:   17.00 Mean scores: 15.69 Means Scores[:-10]: 18.30 Score: 17 \n",
      "Run [  90] - Total reward:   19.00 Mean scores: 15.91 Means Scores[:-10]: 17.70 Score: 19 \n",
      "Run [ 100] - Total reward:   18.00 Mean scores: 16.08 Means Scores[:-10]: 17.60 Score: 18 \n",
      "Run [ 110] - Total reward:   18.00 Mean scores: 16.25 Means Scores[:-10]: 18.00 Score: 18 \n",
      "Run [ 120] - Total reward:   18.00 Mean scores: 16.40 Means Scores[:-10]: 18.10 Score: 18 \n",
      "Run [ 130] - Total reward:   18.00 Mean scores: 16.56 Means Scores[:-10]: 18.40 Score: 18 \n",
      "Run [ 140] - Total reward:   21.00 Mean scores: 16.76 Means Scores[:-10]: 19.40 Score: 21 \n",
      "Run [ 150] - Total reward:   19.00 Mean scores: 16.89 Means Scores[:-10]: 18.70 Score: 19 \n",
      "Run [ 160] - Total reward:   17.00 Mean scores: 17.05 Means Scores[:-10]: 19.50 Score: 17 \n",
      "Run [ 170] - Total reward:   17.00 Mean scores: 17.19 Means Scores[:-10]: 19.50 Score: 17 \n",
      "Run [ 180] - Total reward:   20.00 Mean scores: 17.33 Means Scores[:-10]: 19.60 Score: 20 \n",
      "Run [ 190] - Total reward:   19.00 Mean scores: 17.45 Means Scores[:-10]: 19.60 Score: 19 \n",
      "Run [ 200] - Total reward:   20.00 Mean scores: 17.57 Means Scores[:-10]: 19.90 Score: 20 \n",
      "Run [ 210] - Total reward:   17.00 Mean scores: 17.65 Means Scores[:-10]: 19.30 Score: 17 \n",
      "Run [ 220] - Total reward:   21.00 Mean scores: 17.69 Means Scores[:-10]: 18.60 Score: 21 \n",
      "Run [ 230] - Total reward:   20.00 Mean scores: 17.78 Means Scores[:-10]: 19.70 Score: 20 \n",
      "Run [ 240] - Total reward:   20.00 Mean scores: 17.84 Means Scores[:-10]: 19.30 Score: 20 \n",
      "Run [ 250] - Total reward:   19.00 Mean scores: 17.90 Means Scores[:-10]: 19.20 Score: 19 \n",
      "Run [ 260] - Total reward:   20.00 Mean scores: 17.94 Means Scores[:-10]: 19.00 Score: 20 \n",
      "Run [ 270] - Total reward:   17.00 Mean scores: 17.98 Means Scores[:-10]: 19.10 Score: 17 \n",
      "Run [ 280] - Total reward:   19.00 Mean scores: 18.07 Means Scores[:-10]: 20.40 Score: 19 \n",
      "Run [ 290] - Total reward:   19.00 Mean scores: 18.10 Means Scores[:-10]: 19.10 Score: 19 \n",
      "Run [ 300] - Total reward:   18.00 Mean scores: 18.15 Means Scores[:-10]: 19.50 Score: 18 \n",
      "Run [ 310] - Total reward:   18.00 Mean scores: 18.19 Means Scores[:-10]: 19.40 Score: 18 \n",
      "Run [ 320] - Total reward:   19.00 Mean scores: 18.23 Means Scores[:-10]: 19.50 Score: 19 \n",
      "Run [ 330] - Total reward:   19.00 Mean scores: 18.27 Means Scores[:-10]: 19.50 Score: 19 \n",
      "Run [ 340] - Total reward:   20.00 Mean scores: 18.33 Means Scores[:-10]: 20.20 Score: 20 \n",
      "Run [ 350] - Total reward:   20.00 Mean scores: 18.33 Means Scores[:-10]: 18.60 Score: 20 \n",
      "Run [ 360] - Total reward:   18.00 Mean scores: 18.35 Means Scores[:-10]: 19.00 Score: 18 \n",
      "Run [ 370] - Total reward:   19.00 Mean scores: 18.37 Means Scores[:-10]: 19.00 Score: 19 \n",
      "Run [ 380] - Total reward:   20.00 Mean scores: 18.41 Means Scores[:-10]: 19.80 Score: 20 \n",
      "Run [ 390] - Total reward:   18.00 Mean scores: 18.42 Means Scores[:-10]: 19.00 Score: 18 \n",
      "Run [ 400] - Total reward:   17.00 Mean scores: 18.43 Means Scores[:-10]: 18.80 Score: 17 \n",
      "Run [ 410] - Total reward:   22.00 Mean scores: 18.47 Means Scores[:-10]: 20.20 Score: 22 \n",
      "Run [ 420] - Total reward:   19.00 Mean scores: 18.50 Means Scores[:-10]: 19.70 Score: 19 \n",
      "Run [ 430] - Total reward:   21.00 Mean scores: 18.52 Means Scores[:-10]: 19.20 Score: 21 \n",
      "Run [ 440] - Total reward:   18.00 Mean scores: 18.53 Means Scores[:-10]: 18.90 Score: 18 \n",
      "Run [ 450] - Total reward:   19.00 Mean scores: 18.57 Means Scores[:-10]: 20.20 Score: 19 \n",
      "Run [ 460] - Total reward:   20.00 Mean scores: 18.58 Means Scores[:-10]: 19.20 Score: 20 \n",
      "Run [ 470] - Total reward:   22.00 Mean scores: 18.61 Means Scores[:-10]: 20.10 Score: 22 \n",
      "Run [ 480] - Total reward:   21.00 Mean scores: 18.64 Means Scores[:-10]: 20.00 Score: 21 \n",
      "Run [ 490] - Total reward:   19.00 Mean scores: 18.66 Means Scores[:-10]: 19.60 Score: 19 \n",
      "Run [ 500] - Total reward:   20.00 Mean scores: 18.68 Means Scores[:-10]: 19.80 Score: 20 \n",
      "Run [ 510] - Total reward:   19.00 Mean scores: 18.70 Means Scores[:-10]: 19.70 Score: 19 \n",
      "Run [ 520] - Total reward:   20.00 Mean scores: 18.73 Means Scores[:-10]: 20.30 Score: 20 \n",
      "Run [ 530] - Total reward:   19.00 Mean scores: 18.74 Means Scores[:-10]: 19.30 Score: 19 \n",
      "Run [ 540] - Total reward:   19.00 Mean scores: 18.75 Means Scores[:-10]: 19.20 Score: 19 \n",
      "Run [ 550] - Total reward:   20.00 Mean scores: 18.76 Means Scores[:-10]: 19.10 Score: 20 \n",
      "Run [ 560] - Total reward:   18.00 Mean scores: 18.76 Means Scores[:-10]: 19.10 Score: 18 \n",
      "Run [ 570] - Total reward:   23.00 Mean scores: 18.78 Means Scores[:-10]: 19.40 Score: 23 \n",
      "Run [ 580] - Total reward:   19.00 Mean scores: 18.79 Means Scores[:-10]: 19.50 Score: 19 \n",
      "Run [ 590] - Total reward:   21.00 Mean scores: 18.80 Means Scores[:-10]: 19.40 Score: 21 \n",
      "Run [ 600] - Total reward:   22.00 Mean scores: 18.81 Means Scores[:-10]: 19.60 Score: 22 \n",
      "Run [ 610] - Total reward:   19.00 Mean scores: 18.83 Means Scores[:-10]: 20.00 Score: 19 \n",
      "Run [ 620] - Total reward:   21.00 Mean scores: 18.84 Means Scores[:-10]: 19.50 Score: 21 \n",
      "Run [ 630] - Total reward:   19.00 Mean scores: 18.86 Means Scores[:-10]: 19.80 Score: 19 \n",
      "Run [ 640] - Total reward:   20.00 Mean scores: 18.88 Means Scores[:-10]: 20.20 Score: 20 \n",
      "Run [ 650] - Total reward:   19.00 Mean scores: 18.89 Means Scores[:-10]: 19.70 Score: 19 \n",
      "Run [ 660] - Total reward:   19.00 Mean scores: 18.90 Means Scores[:-10]: 19.20 Score: 19 \n",
      "Run [ 670] - Total reward:   20.00 Mean scores: 18.91 Means Scores[:-10]: 19.80 Score: 20 \n",
      "Run [ 680] - Total reward:   18.00 Mean scores: 18.92 Means Scores[:-10]: 19.70 Score: 18 \n",
      "Run [ 690] - Total reward:   22.00 Mean scores: 18.94 Means Scores[:-10]: 20.30 Score: 22 \n",
      "Run [ 700] - Total reward:   21.00 Mean scores: 18.96 Means Scores[:-10]: 20.20 Score: 21 \n",
      "Run [ 710] - Total reward:   18.00 Mean scores: 18.97 Means Scores[:-10]: 20.10 Score: 18 \n",
      "Run [ 720] - Total reward:   20.00 Mean scores: 18.99 Means Scores[:-10]: 20.40 Score: 20 \n",
      "Run [ 730] - Total reward:   20.00 Mean scores: 19.01 Means Scores[:-10]: 20.00 Score: 20 \n",
      "Run [ 740] - Total reward:   19.00 Mean scores: 19.01 Means Scores[:-10]: 19.50 Score: 19 \n",
      "Run [ 750] - Total reward:   22.00 Mean scores: 19.04 Means Scores[:-10]: 20.60 Score: 22 \n",
      "Run [ 760] - Total reward:   19.00 Mean scores: 19.05 Means Scores[:-10]: 19.80 Score: 19 \n",
      "Run [ 770] - Total reward:   22.00 Mean scores: 19.06 Means Scores[:-10]: 20.20 Score: 22 \n",
      "Run [ 780] - Total reward:   20.00 Mean scores: 19.08 Means Scores[:-10]: 20.20 Score: 20 \n",
      "Run [ 790] - Total reward:   23.00 Mean scores: 19.08 Means Scores[:-10]: 19.80 Score: 23 \n",
      "Run [ 800] - Total reward:   22.00 Mean scores: 19.10 Means Scores[:-10]: 20.20 Score: 22 \n",
      "Run [ 810] - Total reward:   21.00 Mean scores: 19.12 Means Scores[:-10]: 20.50 Score: 21 \n",
      "Run [ 820] - Total reward:   22.00 Mean scores: 19.14 Means Scores[:-10]: 20.70 Score: 22 \n",
      "Run [ 830] - Total reward:   21.00 Mean scores: 19.15 Means Scores[:-10]: 20.20 Score: 21 \n",
      "Run [ 840] - Total reward:   23.00 Mean scores: 19.16 Means Scores[:-10]: 20.50 Score: 23 \n",
      "Run [ 850] - Total reward:   17.00 Mean scores: 19.17 Means Scores[:-10]: 19.90 Score: 17 \n",
      "Run [ 860] - Total reward:   20.00 Mean scores: 19.18 Means Scores[:-10]: 20.00 Score: 20 \n",
      "Run [ 870] - Total reward:   20.00 Mean scores: 19.20 Means Scores[:-10]: 20.40 Score: 20 \n",
      "Run [ 880] - Total reward:   20.00 Mean scores: 19.21 Means Scores[:-10]: 20.00 Score: 20 \n",
      "Run [ 890] - Total reward:   21.00 Mean scores: 19.22 Means Scores[:-10]: 20.40 Score: 21 \n",
      "Run [ 900] - Total reward:   22.00 Mean scores: 19.24 Means Scores[:-10]: 20.70 Score: 22 \n",
      "Run [ 910] - Total reward:   18.00 Mean scores: 19.24 Means Scores[:-10]: 19.80 Score: 18 \n",
      "Run [ 920] - Total reward:   20.00 Mean scores: 19.25 Means Scores[:-10]: 20.40 Score: 20 \n",
      "Run [ 930] - Total reward:   21.00 Mean scores: 19.27 Means Scores[:-10]: 21.10 Score: 21 \n",
      "Run [ 940] - Total reward:   21.00 Mean scores: 19.29 Means Scores[:-10]: 20.40 Score: 21 \n",
      "Run [ 950] - Total reward:   20.00 Mean scores: 19.30 Means Scores[:-10]: 20.70 Score: 20 \n",
      "Run [ 960] - Total reward:   20.00 Mean scores: 19.30 Means Scores[:-10]: 19.50 Score: 20 \n",
      "Run [ 970] - Total reward:   21.00 Mean scores: 19.31 Means Scores[:-10]: 20.30 Score: 21 \n",
      "Run [ 980] - Total reward:   22.00 Mean scores: 19.33 Means Scores[:-10]: 20.90 Score: 22 \n",
      "Run [ 990] - Total reward:   20.00 Mean scores: 19.34 Means Scores[:-10]: 20.70 Score: 20 \n",
      "Run [1000] - Total reward:   21.00 Mean scores: 19.35 Means Scores[:-10]: 19.90 Score: 21 \n",
      "Run [1010] - Total reward:   21.00 Mean scores: 19.36 Means Scores[:-10]: 20.90 Score: 21 \n",
      "Run [1020] - Total reward:   19.00 Mean scores: 19.38 Means Scores[:-10]: 20.80 Score: 19 \n",
      "Run [1030] - Total reward:   21.00 Mean scores: 19.39 Means Scores[:-10]: 20.90 Score: 21 \n",
      "Run [1040] - Total reward:   19.00 Mean scores: 19.40 Means Scores[:-10]: 20.40 Score: 19 \n",
      "Run [1050] - Total reward:   21.00 Mean scores: 19.42 Means Scores[:-10]: 21.10 Score: 21 \n",
      "Run [1060] - Total reward:   18.00 Mean scores: 19.43 Means Scores[:-10]: 20.30 Score: 18 \n",
      "Run [1070] - Total reward:   21.00 Mean scores: 19.44 Means Scores[:-10]: 20.70 Score: 21 \n",
      "Run [1080] - Total reward:   20.00 Mean scores: 19.44 Means Scores[:-10]: 19.50 Score: 20 \n",
      "Run [1090] - Total reward:   21.00 Mean scores: 19.45 Means Scores[:-10]: 20.10 Score: 21 \n",
      "Run [1100] - Total reward:   22.00 Mean scores: 19.45 Means Scores[:-10]: 20.40 Score: 22 \n",
      "Run [1110] - Total reward:   20.00 Mean scores: 19.46 Means Scores[:-10]: 19.70 Score: 20 \n",
      "Run [1120] - Total reward:   22.00 Mean scores: 19.47 Means Scores[:-10]: 21.00 Score: 22 \n",
      "Run [1130] - Total reward:   21.00 Mean scores: 19.49 Means Scores[:-10]: 21.90 Score: 21 \n",
      "Run [1140] - Total reward:   21.00 Mean scores: 19.50 Means Scores[:-10]: 20.50 Score: 21 \n",
      "Run [1150] - Total reward:   20.00 Mean scores: 19.51 Means Scores[:-10]: 20.90 Score: 20 \n",
      "Run [1160] - Total reward:   18.00 Mean scores: 19.52 Means Scores[:-10]: 20.50 Score: 18 \n",
      "Run [1170] - Total reward:   21.00 Mean scores: 19.52 Means Scores[:-10]: 19.70 Score: 21 \n",
      "Run [1180] - Total reward:   19.00 Mean scores: 19.54 Means Scores[:-10]: 21.10 Score: 19 \n",
      "Run [1190] - Total reward:   21.00 Mean scores: 19.55 Means Scores[:-10]: 20.90 Score: 21 \n",
      "Run [1200] - Total reward:   20.00 Mean scores: 19.56 Means Scores[:-10]: 20.90 Score: 20 \n",
      "Run [1210] - Total reward:   18.00 Mean scores: 19.56 Means Scores[:-10]: 19.90 Score: 18 \n",
      "Run [1220] - Total reward:   22.00 Mean scores: 19.57 Means Scores[:-10]: 20.10 Score: 22 \n",
      "Run [1230] - Total reward:   21.00 Mean scores: 19.57 Means Scores[:-10]: 20.60 Score: 21 \n",
      "Run [1240] - Total reward:   21.00 Mean scores: 19.58 Means Scores[:-10]: 20.70 Score: 21 \n",
      "Run [1250] - Total reward:   20.00 Mean scores: 19.59 Means Scores[:-10]: 20.60 Score: 20 \n",
      "Run [1260] - Total reward:   20.00 Mean scores: 19.60 Means Scores[:-10]: 21.20 Score: 20 \n",
      "Run [1270] - Total reward:   21.00 Mean scores: 19.61 Means Scores[:-10]: 20.30 Score: 21 \n",
      "Run [1280] - Total reward:   19.00 Mean scores: 19.61 Means Scores[:-10]: 20.00 Score: 19 \n",
      "Run [1290] - Total reward:   23.00 Mean scores: 19.63 Means Scores[:-10]: 21.30 Score: 23 \n",
      "Run [1300] - Total reward:   22.00 Mean scores: 19.63 Means Scores[:-10]: 20.50 Score: 22 \n",
      "Run [1310] - Total reward:   20.00 Mean scores: 19.63 Means Scores[:-10]: 19.70 Score: 20 \n",
      "Run [1320] - Total reward:   22.00 Mean scores: 19.64 Means Scores[:-10]: 19.90 Score: 22 \n",
      "Run [1330] - Total reward:   22.00 Mean scores: 19.64 Means Scores[:-10]: 20.50 Score: 22 \n",
      "Run [1340] - Total reward:   20.00 Mean scores: 19.65 Means Scores[:-10]: 20.20 Score: 20 \n",
      "Run [1350] - Total reward:   19.00 Mean scores: 19.65 Means Scores[:-10]: 20.70 Score: 19 \n",
      "Run [1360] - Total reward:   22.00 Mean scores: 19.66 Means Scores[:-10]: 20.30 Score: 22 \n",
      "Run [1370] - Total reward:   20.00 Mean scores: 19.67 Means Scores[:-10]: 20.60 Score: 20 \n",
      "Run [1380] - Total reward:   21.00 Mean scores: 19.67 Means Scores[:-10]: 20.70 Score: 21 \n",
      "Run [1390] - Total reward:   20.00 Mean scores: 19.67 Means Scores[:-10]: 19.80 Score: 20 \n",
      "Run [1400] - Total reward:   17.00 Mean scores: 19.67 Means Scores[:-10]: 19.50 Score: 17 \n",
      "Run [1410] - Total reward:   22.00 Mean scores: 19.68 Means Scores[:-10]: 20.20 Score: 22 \n",
      "Run [1420] - Total reward:   22.00 Mean scores: 19.68 Means Scores[:-10]: 20.10 Score: 22 \n",
      "Run [1430] - Total reward:   20.00 Mean scores: 19.68 Means Scores[:-10]: 20.20 Score: 20 \n",
      "Run [1440] - Total reward:   21.00 Mean scores: 19.68 Means Scores[:-10]: 19.80 Score: 21 \n",
      "Run [1450] - Total reward:   21.00 Mean scores: 19.69 Means Scores[:-10]: 20.30 Score: 21 \n",
      "Run [1460] - Total reward:   20.00 Mean scores: 19.69 Means Scores[:-10]: 20.40 Score: 20 \n",
      "Run [1470] - Total reward:   21.00 Mean scores: 19.69 Means Scores[:-10]: 19.90 Score: 21 \n",
      "Run [1480] - Total reward:   19.00 Mean scores: 19.70 Means Scores[:-10]: 20.00 Score: 19 \n",
      "Run [1490] - Total reward:   22.00 Mean scores: 19.70 Means Scores[:-10]: 20.10 Score: 22 \n",
      "Run [1500] - Total reward:   19.00 Mean scores: 19.71 Means Scores[:-10]: 20.80 Score: 19 \n",
      "Run [1510] - Total reward:   21.00 Mean scores: 19.71 Means Scores[:-10]: 21.00 Score: 21 \n",
      "Run [1520] - Total reward:   22.00 Mean scores: 19.73 Means Scores[:-10]: 21.60 Score: 22 \n",
      "Run [1530] - Total reward:   23.00 Mean scores: 19.73 Means Scores[:-10]: 20.50 Score: 23 \n",
      "Run [1540] - Total reward:   19.00 Mean scores: 19.74 Means Scores[:-10]: 20.20 Score: 19 \n",
      "Run [1550] - Total reward:   23.00 Mean scores: 19.74 Means Scores[:-10]: 20.30 Score: 23 \n",
      "Run [1560] - Total reward:   20.00 Mean scores: 19.75 Means Scores[:-10]: 21.10 Score: 20 \n",
      "Run [1570] - Total reward:   24.00 Mean scores: 19.75 Means Scores[:-10]: 20.70 Score: 24 \n",
      "Run [1580] - Total reward:   21.00 Mean scores: 19.76 Means Scores[:-10]: 20.80 Score: 21 \n",
      "Run [1590] - Total reward:   21.00 Mean scores: 19.77 Means Scores[:-10]: 20.70 Score: 21 \n",
      "Run [1600] - Total reward:   22.00 Mean scores: 19.77 Means Scores[:-10]: 21.10 Score: 22 \n",
      "Run [1610] - Total reward:   21.00 Mean scores: 19.78 Means Scores[:-10]: 20.70 Score: 21 \n",
      "Run [1620] - Total reward:   19.00 Mean scores: 19.79 Means Scores[:-10]: 20.70 Score: 19 \n",
      "Run [1630] - Total reward:   22.00 Mean scores: 19.79 Means Scores[:-10]: 19.90 Score: 22 \n",
      "Run [1640] - Total reward:   20.00 Mean scores: 19.79 Means Scores[:-10]: 20.20 Score: 20 \n",
      "Run [1650] - Total reward:   21.00 Mean scores: 19.80 Means Scores[:-10]: 20.90 Score: 21 \n",
      "Run [1660] - Total reward:   20.00 Mean scores: 19.80 Means Scores[:-10]: 20.70 Score: 20 \n",
      "Run [1670] - Total reward:   21.00 Mean scores: 19.81 Means Scores[:-10]: 20.60 Score: 21 \n",
      "Run [1680] - Total reward:   24.00 Mean scores: 19.81 Means Scores[:-10]: 20.70 Score: 24 \n",
      "Run [1690] - Total reward:   22.00 Mean scores: 19.81 Means Scores[:-10]: 20.30 Score: 22 \n",
      "Run [1700] - Total reward:   19.00 Mean scores: 19.82 Means Scores[:-10]: 20.60 Score: 19 \n",
      "Run [1710] - Total reward:   19.00 Mean scores: 19.82 Means Scores[:-10]: 20.50 Score: 19 \n",
      "Run [1720] - Total reward:   19.00 Mean scores: 19.83 Means Scores[:-10]: 20.70 Score: 19 \n",
      "Run [1730] - Total reward:   22.00 Mean scores: 19.83 Means Scores[:-10]: 20.80 Score: 22 \n",
      "Run [1740] - Total reward:   20.00 Mean scores: 19.84 Means Scores[:-10]: 20.90 Score: 20 \n",
      "Run [1750] - Total reward:   19.00 Mean scores: 19.85 Means Scores[:-10]: 21.00 Score: 19 \n",
      "Run [1760] - Total reward:   19.00 Mean scores: 19.85 Means Scores[:-10]: 20.40 Score: 19 \n",
      "Run [1770] - Total reward:   22.00 Mean scores: 19.85 Means Scores[:-10]: 20.70 Score: 22 \n",
      "Run [1780] - Total reward:   18.00 Mean scores: 19.86 Means Scores[:-10]: 20.20 Score: 18 \n",
      "Run [1790] - Total reward:   21.00 Mean scores: 19.86 Means Scores[:-10]: 20.40 Score: 21 \n",
      "Run [1800] - Total reward:   21.00 Mean scores: 19.86 Means Scores[:-10]: 20.60 Score: 21 \n",
      "Run [1810] - Total reward:   20.00 Mean scores: 19.87 Means Scores[:-10]: 20.20 Score: 20 \n",
      "Run [1820] - Total reward:   21.00 Mean scores: 19.87 Means Scores[:-10]: 20.60 Score: 21 \n",
      "Run [1830] - Total reward:   23.00 Mean scores: 19.88 Means Scores[:-10]: 21.20 Score: 23 \n",
      "Run [1840] - Total reward:   20.00 Mean scores: 19.88 Means Scores[:-10]: 20.70 Score: 20 \n",
      "Run [1850] - Total reward:   18.00 Mean scores: 19.88 Means Scores[:-10]: 19.10 Score: 18 \n",
      "Run [1860] - Total reward:   23.00 Mean scores: 19.89 Means Scores[:-10]: 21.70 Score: 23 \n",
      "Run [1870] - Total reward:   20.00 Mean scores: 19.89 Means Scores[:-10]: 20.90 Score: 20 \n",
      "Run [1880] - Total reward:   20.00 Mean scores: 19.89 Means Scores[:-10]: 20.40 Score: 20 \n",
      "Run [1890] - Total reward:   19.00 Mean scores: 19.90 Means Scores[:-10]: 20.30 Score: 19 \n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "n_runs = 4000\n",
    "\n",
    "for i in range(n_runs):\n",
    "    render = i % 200 == 201\n",
    "\n",
    "    game_over = False\n",
    "    state = env.reset()\n",
    "    state = reduce_state(state)[RAM_mask].data.tobytes()  # Select useful bytes\n",
    "    action = agent.act(state)\n",
    "    \n",
    "    score = 0\n",
    "    total_reward = 0\n",
    "\n",
    "    while not game_over:\n",
    "        if render:\n",
    "            time.sleep(0.025)\n",
    "            env.render()\n",
    "\n",
    "        old_state = state\n",
    "        ob, reward, game_over, _ = env.step(action)\n",
    "\n",
    "        ob = reduce_state(ob)\n",
    "        reward = reward_policy(reward, ob, action)\n",
    "\n",
    "        total_reward += reward\n",
    "\n",
    "        if reward == reward_policy.REWARD_IF_CROSS:\n",
    "            score += 1\n",
    "\n",
    "        state = ob[RAM_mask].data.tobytes()\n",
    "\n",
    "        agent.update_Q(old_state, state, action, reward)\n",
    "\n",
    "        action = agent.act(state)  # Next action\n",
    "\n",
    "    scores.append(score)\n",
    "    total_rewards.append(total_reward)\n",
    "\n",
    "    print_result(i, scores, total_reward, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_plots.plot_scores(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_plots.plot_rewards(total_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = serializer.Experiment(agent, scores, total_rewards, reduce_state, reward_policy)\n",
    "exp.save_experiment('QL_base_2_actions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e2 = serializer.Experiment.load_experiment('./serialized_models/QL_2020_12_23_01_23_20.dill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e2.print_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e2.scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3rjTCHMkwfKE"
   },
   "source": [
    "### Influence of the number of actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the goal of the game is to cross all the lanes, we propose that it would be enought if the chicken use just the two actions move **up** or **stay** and never move **down**.\n",
    "<br>\n",
    "In order to test our hypothesis, we ran the agent using 3 and 2 actions.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters used:\n",
    "```\n",
    "GAMMA = 0.99\n",
    "AVAILABLE_ACTIONS = {2, 3}\n",
    "N0 = 2.5\n",
    "\n",
    "reward_policy.REWARD_IF_CROSS = 1\n",
    "reward_policy.REWARD_IF_COLLISION = 0\n",
    "reward_policy.REWARD_IF_STILL = 0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_2act = read_int_array_from_file(\"QL/QL_scores_2act.txt\")\n",
    "total_rewards_2act = read_int_array_from_file(\"QL/QL_total_rewards_2act.txt\")\n",
    "scores_3act = read_int_array_from_file(\"QL/QL_scores_3act.txt\")\n",
    "total_rewards_3act = read_int_array_from_file(\"QL/QL_total_rewards_3act.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Scores of the agent with 2 and 3 actions ----------\n",
    "\n",
    "baseline = [baseline_mean_score for i in range(4000)]\n",
    "ax = aux_plots.plot_2scores(scores_3act[:4000], scores_2act[:4000], \"3 actions (up, down or stay)\", \"2 actions (up or stay)\", \"Baseline mean score (up)\")\n",
    "aux_plots.moving_average(scores_2act[:4000], ax, label='2 actions moving avg', color='darkmagenta')\n",
    "aux_plots.moving_average(scores_3act, ax, label='3 actions moving avg', color='darkcyan')\n",
    "aux_plots.moving_average(baseline, ax, label='Baseline', color='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the graph above, we can see that when using only two actions the agent performs a bit better than when using 3 actions.\n",
    "\n",
    "This was expected as we state before, since the chicken is able to predict if a collision is going to happen using our state representation, thus not needing to move down to avoid a collision.\n",
    "\n",
    "From now on we will focus on exploring agents that use only 2 actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eJ_JiwOewfKK"
   },
   "source": [
    "### Influence of the reward values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to investigate the influence of the reward values in the agent behavior, we ran the algorithm using three different set of reward values.  \n",
    "<br>\n",
    "**R1:** Positive rewards for crossing: it offered a sparse reward to the agent: +1 if the chicken cross all the lanes.  \n",
    "<br>\n",
    "**R2:** Positive rewards for crossing and negative reward for colliding: it given +1 for crossing all the lanes and -1 if the chicken collide. Thus, we expect the agent to be encouraged to cross the lanes (positive reward) and to avoid collisions (negative reward).  \n",
    "<br>\n",
    "**R3:** Huge positive reward for crossing and negative reward for colliding or staying in the same position: it increased the reward for crossing all the lanes to +500 and give -10 when the chicken collide. It also gives a negative reward of -1 when the agent decide to **still**. Thus we expect the chicken to cross all the lanes faster and increase the final score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_2act_R1 = read_int_array_from_file('QL_scores_2act_R1.txt')\n",
    "scores_2act_R2 = read_int_array_from_file('QL_scores_2act_R2.txt')\n",
    "scores_2act_R3 = read_int_array_from_file('QL_scores_2act_R3.txt')\n",
    "\n",
    "total_rewards_2act_R1 = read_int_array_from_file('QL_total_rewards_2act_R1.txt')\n",
    "total_rewards_2act_R2 = read_int_array_from_file('QL_total_rewards_2act_R2.txt')\n",
    "total_rewards_2act_R3 = read_int_array_from_file('QL_total_rewards_2act_R3.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Scores of the agent with different rewards ----------\n",
    "baseline = [baseline_mean_score for i in range(4000)]\n",
    "\n",
    "ax = aux_plots.plot_3scores(scores_2act_R1[:4000], scores_2act_R2[:4000], scores_2act_R3[:4000], \"R1 (+1 if cross)\", \"R2 (+1 if cross, -1 if collide)\", \"R3 (+500 if cross, -10 if collide, -1 if stay)\")\n",
    "\n",
    "aux_plots.moving_average(scores_2act_R1[:4000], ax, label=\"R1 (+1 if cross)\", color='darkcyan')\n",
    "aux_plots.moving_average(scores_2act_R2[:4000], ax, label=\"R2 (+1 if cross, -1 if collide)\", color='darkmagenta')\n",
    "aux_plots.moving_average(scores_2act_R3[:4000], ax, label=\"R3 (+500 if cross, -10 if collide, -1 if stay)\", color='#FF3341')\n",
    "aux_plots.moving_average(baseline, ax, label='Baseline', color='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Rewards of the agent with different rewards ----------\n",
    "ax = aux_plots.plot_3rewards(total_rewards_2act_R1[:4000], total_rewards_2act_R2[:4000], total_rewards_2act_R3[:4000], \"R1 (+1 if cross)\", \"R2 (+1 if cross, -1 if collide)\", \"R3 (+500 if cross, -10 if collide, -1 if stay)\")\n",
    "\n",
    "aux_plots.moving_average(total_rewards_2act_R1[:4000], ax, label=\"R1 (+1 if cross)\", color='#1F8FFF')\n",
    "aux_plots.moving_average(total_rewards_2act_R2[:4000], ax, label=\"R2 (+1 if cross, -1 if collide)\", color='#9C4BE7')\n",
    "aux_plots.moving_average(total_rewards_2act_R3[:4000], ax, label=\"R3 (+500 if cross, -10 if collide, -1 if stay)\", color='#FF3341')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b0_OosKUwfKM"
   },
   "source": [
    "#### Results got from agents receiving different reward values:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in the plots below, the agent that receives the **R1** reward (+1 if cross all the lanes) initially performs better than the others but converges to a lower value as the agent starts to colliding. It has no penalty for collide or stay, so theres no urgency to cross the lanes.  \n",
    "<br>\n",
    "The agent that receives the **R2** reward (+1 if cross all the lanes and -1 if collide) converges to a lower score value where the chicken most of the time chooses to avoid collisions insted of cross all the lanes.  \n",
    "<br>\n",
    "The agent using **R3** (+500 if cross all the lanes, -10 if collide and -1 if still) converges to the higher score. As the reward for crossing the lanes is much higher than the others, the chicken primarily seeks to cross all the lanes but she is also discouraged to still or to collide with a small penalty. The results shows that it achieves higher scores, confirming our proposed hypothesis.   \n",
    "<br>\n",
    "Below the plots are showing the score and the total rewards got using the two agents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x480 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ---------- Scores of the agent with different rewards ----------\n",
    "\n",
    "aux_plots.plot_3scores(scores_2act_sparseR[:4000], scores_2act[:4000], scores_2act_sparseR_cross[:4000], \"R1 (+1 if cross)\", \"R3(+500 if cross, -10 if collide, -1 if stay)\", \"R2 (+1 if cross, -1 if collide)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SARSA(λ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env, initial_state = environment.get_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = agents.SarsaLambda(gamma=GAMMA, available_actions=AVAILABLE_ACTIONS, N0=N0, lambd=LAMBD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "total_rewards = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "n_runs = 1\n",
    "\n",
    "for i in range(n_runs):\n",
    "    render = i % 200 == 199\n",
    "    \n",
    "    agent.reset_E()\n",
    "\n",
    "    game_over = False\n",
    "    state = env.reset()\n",
    "    state = reduce_state(state)[RAM_mask].data.tobytes()  # Select useful bytes\n",
    "    action = agent.act(state)\n",
    "    \n",
    "    score = 0\n",
    "    total_reward = 0\n",
    "\n",
    "    while not game_over:\n",
    "        if render:\n",
    "            time.sleep(0.005)\n",
    "            env.render()\n",
    "\n",
    "        old_state = state\n",
    "        old_action = action\n",
    "        ob, reward, game_over, _ = env.step(action)\n",
    "\n",
    "        ob = reduce_state(ob)\n",
    "        reward = reward_policy(reward, ob, action)\n",
    "\n",
    "        total_reward += reward\n",
    "\n",
    "        if reward == reward_policy.REWARD_IF_CROSS:\n",
    "            score += 1\n",
    "\n",
    "        state = ob[RAM_mask].data.tobytes()\n",
    "\n",
    "        action = agent.act(state)  # Next action\n",
    "\n",
    "        agent.update_Q(old_s=old_state, new_s=state, old_a=old_action, new_a=action, reward=reward)\n",
    "\n",
    "    scores.append(score)\n",
    "    total_rewards.append(total_reward)\n",
    "\n",
    "    print_result(i, scores, total_reward, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#10.2 s ± 186 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_plots.plot_scores(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_plots.plot_rewards(total_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Monte Carlo algorithm receives the $\\gamma$ and the $N0$ parameters.  \n",
    "<br>\n",
    "$\\gamma$ is the discount factor; This parameters determines the importance of future rewards. A value of 0 makes the agent short-sighned by only considering current rewards, while a factor approaching 1 will make it strive for a long term reward.  \n",
    "<br>\n",
    "The $N0$ parameter is used to define the agent's exploration rate $\\epsilon$, where $\\epsilon = N0/(N0+n)$ and $n$ is the number of visits in the states.  \n",
    "<br>\n",
    "In the algorithm, the value function is initialized to zero. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env, initial_state = environment.get_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = agents.MonteCarloControl(gamma=GAMMA, available_actions=AVAILABLE_ACTIONS, N0=N0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MonteCarloES(RAM_mask: List[int], render: bool=False):\n",
    "    epi = episode.generate_episode(env\n",
    "                                   , reduce_state=reduce_state\n",
    "                                   , reward_policy=reward_policy\n",
    "                                   , agent=agent\n",
    "                                   , RAM_mask=RAM_mask\n",
    "                                   , render=render)\n",
    "    return agent.update_policy(epi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "MonteCarloES(RAM_mask=RAM_mask, render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "total_rewards = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "n_runs = 1000\n",
    "\n",
    "for i in range(n_runs):\n",
    "    render = i % 201 == 200\n",
    "\n",
    "    score, total_reward = MonteCarloES(RAM_mask=RAM_mask, render=render)\n",
    "\n",
    "    scores.append(score)\n",
    "    total_rewards.append(total_reward)\n",
    "\n",
    "    print_result(i, scores, total_reward, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_plots.plot_scores(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_plots.plot_rewards(total_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixed epsilon Monte Carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env, initial_state = environment.get_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = agents.MonteCarloControlFixedEpsilon(gamma=GAMMA, available_actions=AVAILABLE_ACTIONS, epsilon=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MonteCarloES(RAM_mask: List[int], render: bool=False):\n",
    "    epi = episode.generate_episode(env\n",
    "                                   , reduce_state=reduce_state\n",
    "                                   , reward_policy=reward_policy\n",
    "                                   , agent=agent\n",
    "                                   , RAM_mask=RAM_mask\n",
    "                                   , render=render)\n",
    "    return agent.update_policy(epi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "total_rewards = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "n_runs = 1000\n",
    "\n",
    "for i in range(n_runs):\n",
    "    render = i % 201 == 200\n",
    "\n",
    "    score, total_reward = MonteCarloES(RAM_mask=RAM_mask, render=render)\n",
    "\n",
    "    scores.append(score)\n",
    "    total_rewards.append(total_reward)\n",
    "\n",
    "    print_result(i, scores, total_reward, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_plots.plot_scores(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_plots.plot_rewards(total_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import animation\n",
    "# From https://gist.github.com/botforge/64cbb71780e6208172bbf03cd9293553\n",
    "def _save_frames_as_gif(frames, path='./', filename='gym_animation.gif'):\n",
    "\n",
    "    #Mess with this to change frame size\n",
    "    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi=72)\n",
    "\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "\n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "\n",
    "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
    "    anim.save(path + filename, writer='imagemagick', fps=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gif(fn: str, agent):\n",
    "    env, initial_state = environment.get_env()\n",
    "    game_over = False\n",
    "    state = reduce_state(initial_state)[RAM_mask].data.tobytes()  # Select useful bytes\n",
    "    action = agent.act(state)\n",
    "\n",
    "    frames = []\n",
    "    FRAME_FREQ = 2\n",
    "\n",
    "    for t in range(1000):\n",
    "        if t % FRAME_FREQ == 0:\n",
    "            frames.append(env.render(mode=\"rgb_array\"))\n",
    "\n",
    "        ob, _, game_over, _ = env.step(action)\n",
    "\n",
    "        ob = reduce_state(ob)\n",
    "        state = ob[RAM_mask].data.tobytes()\n",
    "        action = agent.act(state)  # Next action\n",
    "\n",
    "        if game_over:\n",
    "            break\n",
    "\n",
    "    _save_frames_as_gif(frames=frames, path='./gif/', filename=f'{fn}.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "generate_gif('MC_fixed_epsilon', agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Colocar um título aqui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![SegmentLocal](./gif/MC_fixed_epsilon.gif)\n",
    "![SegmentLocal](./gif/MC_fixed_epsilon.gif)\n",
    "![SegmentLocal](./gif/MC_fixed_epsilon.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|GIF| Description |\n",
    "|----|----|\n",
    "| ![SegmentLocal](./gif/MC_fixed_epsilon.gif) | MC tralalalalal |\n",
    "| ![SegmentLocal](./gif/MC_fixed_epsilon.gif) | MC tralalalalal |\n",
    "| ![SegmentLocal](./gif/MC_fixed_epsilon.gif) | MC tralalalalal |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear function approximators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous algorithms aim to solve control problems using a model free approach that depends on Q tables, which are structures that store the values associated with how good it is to take an action A in a state S, called Q values.\n",
    "\n",
    "Although using Q tables are good for solving small learning problems, they suffer with a trouble called the curse of dimensionality, in which even some small environments can generate a huge amount of possible states, requiring a large amount of memory that is not available. This drawback often prevents those techniques to be used in many tasks from real life that could benefit from it.\n",
    "\n",
    "One of the solutions developed to deal with this problem is Function Approximation. On it, instead of trying to find the optimal values for a very large table, we try to find the best parameters for a parameterized function whose objective is to approximate the optimal values that we would find on that table.\n",
    "\n",
    "Mathematically, we say that we have a family of parameterized functions $\\mathcal{Q}$ given by $Q_{\\theta}: S\\times A \\rightarrow \\mathbb{R}$, where $\\theta$ is an array of parameters in $\\mathbb{R}^d$, called weights, and $d << |S|$. Given that, the objective of a function approximator is to find the array of weights $\\theta^*$ that produces the $Q_{\\theta}^*$ that better approximates the optimal Q values ($Q^*$) for the problem addressed. Some of the greatest advantages of these algorithms is that they learn to generalize for unseen states and requires a much smaller set of values to be learned (d instead of |S|).\n",
    "\n",
    "For this project, we experimented with linear function approximators, which are given by the following equation, where $\\hat{q}(s,a)$ is the approximated value of choosing action **a** in state **s**, $\\theta_i$ is the i-th element of the array of parameters and $x_i$ is a function that turns the action-state pair into the i-th feature of a d-dimensional feature array.\n",
    "\n",
    "$\\hat{q}(s,a,\\theta) = \\sum_{i = 1}^d \\theta_i*x_i(s,a)$\n",
    "\n",
    "In order to discover the best parameters, we applied a Stochastic Gradient Descent algorithm to update the weights, given by the equation below:\n",
    "\n",
    "$\\theta = \\theta + \\alpha*(target - \\hat{q}(S_t, A_t, \\theta))*x(S_t, A_t)$\n",
    "\n",
    "The **target** is the value that our function approximator tries to achieve at each update and it changes for each algorithm approximated (Monte Carlo, Q Learning and Sarsa Lambda), and $\\alpha$ is a learning step size.\n",
    "\n",
    "One of the critical points of function approximators is the choice of the features to be used to represent the states. Depending on their choice, we can build good or bad function approximators. For this project, we experimented with a set of different features, as described in the next sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Thoughts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computational cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the biggest problems on this project was the computational cost of running an episode.\n",
    "\n",
    "Since each run plays for 2 minutes and 16 seconds in the original game, there are quite a lot of frames that need to be computed for each episode.\n",
    "Even though the frame-sync is deactivated in our environment, each time we execute on episode it takes about 2 seconds to compute it for Q-Learn and Monte Carlo, and XXXXXXXXXX seconds for SARSA(λ).\n",
    "\n",
    "The memory usage is fairly low compared to the time that it takes to run the algorithms, even with a decent amount of unique states in our problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..............."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
