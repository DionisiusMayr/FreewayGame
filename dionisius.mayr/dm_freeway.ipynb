{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the dependencies:\n",
    "```sh\n",
    "pip install gym\n",
    "pip install gym[atari]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful Resources\n",
    "* [Manual of the game](https://www.gamesdatabase.org/Media/SYSTEM/Atari_2600/Manual/formated/Freeway_-_1981_-_Zellers.pdf)\n",
    "* [Freeway Disassembly](http://www.bjars.com/disassemblies.html)\n",
    "* [Atari Ram Annotations](https://github.com/mila-iqia/atari-representation-learning/blob/master/atariari/benchmark/ram_annotations.py)\n",
    "* [Freeway Benchmarks](https://paperswithcode.com/sota/atari-games-on-atari-2600-freeway)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description of the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')  # Enable importing from `src` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from collections import defaultdict\n",
    "from functools import lru_cache\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import gym\n",
    "\n",
    "import src.agents as agents\n",
    "import src.episode as episode\n",
    "import src.environment as environment\n",
    "import src.aux_plots as aux_plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_result(i, scores, total_reward, score):\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Run [{i:4}] - Total reward: {total_reward:7.2f} Mean scores: {sum(scores) / len(scores):.2f} Means Scores[:-10]: {sum(scores[-10:]) / len(scores[-10:]):5.2f} Score: {score:2} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the Open AI Gym framework in this study......."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env, initial_state = environment.get_env()\n",
    "\n",
    "# print(\"Action Space:\", env.action_space)\n",
    "# print(\"Observation Space:\", env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent in this game has three possible actions:\n",
    "\n",
    "* 0: Stay\n",
    "* 1: Move forward\n",
    "* 2: Move back"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Talk a bit about the observation space of 128 bytes of RAM..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a simple baseline, we are using an agent that moves always **up**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores = environment.run(agents.Baseline, render=True, n_runs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean score\n",
    "# print(\"Mean score:\", sum(scores) / len(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It usually scores from 21 to 23 points, as shown in the images below:\n",
    "\n",
    "![Baseline 1](./img/baseline_1.png)\n",
    "![Baseline 2](./img/baseline_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Representing the state of the game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: explain why we must reduce the state space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "      14  # Chicken Y\n",
    "    , 16  # Chicken Lane Collide\n",
    "    , 18  # Chicken Collision flag (with the bottom car)\n",
    "    , 22  # Car X Direction\n",
    "    , 23, 24, 25, 26, 27, 28, 29, 30, 31, 32  # Z Car Patterns\n",
    "    , 33, 34, 35, 36, 37, 38, 39, 40, 41, 42  # Car Motion Timmers\n",
    "    , 43, 44, 45, 46, 47, 48, 49, 50, 51, 52  # Car Motions\n",
    "    , 87, 88  # Car Shape Ptr\n",
    "    # TODO: test if this makes any difference\n",
    "    , 89, 90  # Chicken Shape Ptr\n",
    "    # TODO: test if this makes any difference\n",
    "    , 106, 107  # Chicken Sounds\n",
    "    , 108, 109, 110, 111, 112, 113, 114, 115, 116, 117  # Car X Coords\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAM_mask = [\n",
    "      14  # Chicken Y\n",
    "    , 16  # Chicken Lane Collide\n",
    "    , 108, 109, 110, 111, 112, 113, 114, 115, 116, 117  # Car X Coords\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_state(ob):\n",
    "    # Doesn't matter where we were hit\n",
    "    ob[16] = 1 if ob[16] != 255 else 0\n",
    "\n",
    "    # Reduce chicken y-position\n",
    "    ob[14] = ob[14] // 3\n",
    "\n",
    "    for b in range(108, 118):\n",
    "        # The chicken is in the x-posistion ~49\n",
    "        if ob[b] < 20 or ob[b] > 80:\n",
    "            # We don't need to represent cars far from the chicken\n",
    "            ob[b] = 0\n",
    "        else:\n",
    "            # Reduce the cars x-positions sample space\n",
    "            ob[b] = ob[b] // 3\n",
    "\n",
    "    return ob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reward Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_policy(reward, ob, action):\n",
    "    if reward == 1:\n",
    "        reward = reward_policy.REWARD_IF_CROSS\n",
    "    elif ob[16] == 1:  # Collision!\n",
    "        reward = reward_policy.REWARD_IF_COLISION\n",
    "    elif action != 1:  # Don't incentivate staying still\n",
    "        reward = reward_policy.REWARD_IF_STILL\n",
    "\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.95\n",
    "AVAILABLE_ACTIONS = 2\n",
    "N0 = 0.0001\n",
    "LAMBD = 0.2\n",
    "\n",
    "reward_policy.REWARD_IF_CROSS = 200\n",
    "reward_policy.REWARD_IF_COLISION = -1\n",
    "reward_policy.REWARD_IF_STILL = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "env, initial_state = environment.get_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = agents.MonteCarloControl(gamma=GAMMA, available_actions=AVAILABLE_ACTIONS, N0=N0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MonteCarloES(RAM_mask: List[int], render: bool=False):\n",
    "    epi = episode.generate_episode(env\n",
    "                                   , reduce_state=reduce_state\n",
    "                                   , reward_policy=reward_policy\n",
    "                                   , agent=agent\n",
    "                                   , RAM_mask=RAM_mask\n",
    "                                   , render=render)\n",
    "    return agent.update_policy(epi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.74 s, sys: 6.69 ms, total: 1.75 s\n",
      "Wall time: 1.75 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(12, 2321.0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "MonteCarloES(RAM_mask=RAM_mask, render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "total_rewards = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run [   0] - Total reward: 2332.00 Mean scores: 12.00 Means Scores[:-10]: 12.00 Score: 12 \n",
      "Run [  10] - Total reward: 2345.00 Mean scores: 11.91 Means Scores[:-10]: 11.90 Score: 12 \n",
      "Run [  20] - Total reward: 2128.00 Mean scores: 12.05 Means Scores[:-10]: 12.20 Score: 11 \n",
      "Run [  30] - Total reward: 2330.00 Mean scores: 11.94 Means Scores[:-10]: 11.70 Score: 12 \n",
      "Run [  40] - Total reward: 2548.00 Mean scores: 12.15 Means Scores[:-10]: 12.80 Score: 13 \n",
      "Run [  50] - Total reward: 2550.00 Mean scores: 12.29 Means Scores[:-10]: 12.90 Score: 13 \n",
      "Run [  60] - Total reward: 2962.00 Mean scores: 12.39 Means Scores[:-10]: 12.90 Score: 15 \n",
      "Run [  70] - Total reward: 2124.00 Mean scores: 12.34 Means Scores[:-10]: 12.00 Score: 11 \n",
      "Run [  80] - Total reward: 2562.00 Mean scores: 12.35 Means Scores[:-10]: 12.40 Score: 13 \n",
      "Run [  90] - Total reward: 2340.00 Mean scores: 12.34 Means Scores[:-10]: 12.30 Score: 12 \n",
      "Run [ 100] - Total reward: 2546.00 Mean scores: 12.37 Means Scores[:-10]: 12.60 Score: 13 \n",
      "Run [ 110] - Total reward: 2747.00 Mean scores: 12.42 Means Scores[:-10]: 13.00 Score: 14 \n",
      "Run [ 120] - Total reward: 2756.00 Mean scores: 12.48 Means Scores[:-10]: 13.10 Score: 14 \n",
      "Run [ 130] - Total reward: 2142.00 Mean scores: 12.46 Means Scores[:-10]: 12.20 Score: 11 \n",
      "Run [ 140] - Total reward: 2332.00 Mean scores: 12.48 Means Scores[:-10]: 12.70 Score: 12 \n",
      "Run [ 150] - Total reward: 2138.00 Mean scores: 12.48 Means Scores[:-10]: 12.50 Score: 11 \n",
      "Run [ 160] - Total reward: 2140.00 Mean scores: 12.47 Means Scores[:-10]: 12.40 Score: 11 \n",
      "Run [ 170] - Total reward: 2534.00 Mean scores: 12.46 Means Scores[:-10]: 12.20 Score: 13 \n",
      "Run [ 180] - Total reward: 2555.00 Mean scores: 12.46 Means Scores[:-10]: 12.60 Score: 13 \n",
      "Run [ 190] - Total reward: 2330.00 Mean scores: 12.48 Means Scores[:-10]: 12.80 Score: 12 \n",
      "Run [ 200] - Total reward: 2964.00 Mean scores: 12.51 Means Scores[:-10]: 13.10 Score: 15 \n",
      "Run [ 210] - Total reward: 2552.00 Mean scores: 12.50 Means Scores[:-10]: 12.30 Score: 13 \n",
      "Run [ 220] - Total reward: 2337.00 Mean scores: 12.52 Means Scores[:-10]: 13.00 Score: 12 \n",
      "Run [ 230] - Total reward: 2763.00 Mean scores: 12.52 Means Scores[:-10]: 12.50 Score: 14 \n",
      "Run [ 240] - Total reward: 2529.00 Mean scores: 12.55 Means Scores[:-10]: 13.20 Score: 13 \n",
      "Run [ 250] - Total reward: 1732.00 Mean scores: 12.56 Means Scores[:-10]: 12.70 Score:  9 \n",
      "Run [ 260] - Total reward: 2340.00 Mean scores: 12.56 Means Scores[:-10]: 12.60 Score: 12 \n",
      "Run [ 270] - Total reward: 2335.00 Mean scores: 12.57 Means Scores[:-10]: 12.80 Score: 12 \n",
      "Run [ 280] - Total reward: 2545.00 Mean scores: 12.57 Means Scores[:-10]: 12.60 Score: 13 \n",
      "Run [ 290] - Total reward: 2754.00 Mean scores: 12.57 Means Scores[:-10]: 12.50 Score: 14 \n",
      "Run [ 300] - Total reward: 2541.00 Mean scores: 12.57 Means Scores[:-10]: 12.70 Score: 13 \n",
      "Run [ 310] - Total reward: 2548.00 Mean scores: 12.58 Means Scores[:-10]: 12.90 Score: 13 \n",
      "Run [ 320] - Total reward: 2343.00 Mean scores: 12.61 Means Scores[:-10]: 13.40 Score: 12 \n",
      "Run [ 330] - Total reward: 1937.00 Mean scores: 12.60 Means Scores[:-10]: 12.50 Score: 10 \n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "n_runs = 1000\n",
    "\n",
    "for i in range(n_runs):\n",
    "    render = i % 201 == 200\n",
    "\n",
    "    score, total_reward = MonteCarloES(RAM_mask=RAM_mask, render=render)\n",
    "\n",
    "    scores.append(score)\n",
    "    total_rewards.append(total_reward)\n",
    "\n",
    "    print_result(i, scores, total_reward, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_plots.plot_scores(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_plots.plot_rewards(total_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env, initial_state = environment.get_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = agents.QLearning(gamma=GAMMA, available_actions=AVAILABLE_ACTIONS, N0=N0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "total_rewards = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "n_runs = 10\n",
    "\n",
    "for i in range(n_runs):\n",
    "    render = i % 200 == 199\n",
    "\n",
    "    game_over = False\n",
    "    state = env.reset()\n",
    "    state = reduce_state(state)[RAM_mask].data.tobytes()  # Select useful bytes\n",
    "    action = agent.act(state)\n",
    "    \n",
    "    score = 0\n",
    "    total_reward = 0\n",
    "\n",
    "    while not game_over:\n",
    "        if render:\n",
    "            time.sleep(0.025)\n",
    "            env.render()\n",
    "\n",
    "        old_state = state\n",
    "        ob, reward, game_over, _ = env.step(action)\n",
    "\n",
    "        ob = reduce_state(ob)\n",
    "        reward = reward_policy(reward, ob, action)\n",
    "\n",
    "        total_reward += reward\n",
    "\n",
    "        if reward == reward_policy.REWARD_IF_CROSS:\n",
    "            score += 1\n",
    "\n",
    "        state = ob[RAM_mask].data.tobytes()\n",
    "\n",
    "        agent.update_Q(old_state, state, action, reward)\n",
    "\n",
    "        action = agent.act(state)  # Next action\n",
    "\n",
    "    scores.append(score)\n",
    "    total_rewards.append(total_reward)\n",
    "\n",
    "    print_result(i, scores, total_reward, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 19.5 s ± 103 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_plots.plot_scores(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_plots.plot_rewards(total_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SARSA(λ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SarsaLambda():  ## ADD THE AGENT\n",
    "    def __init__(self, gamma: float, available_actions: int, N0: float, lambd: float):\n",
    "        self.gamma = gamma\n",
    "        self.available_actions = available_actions\n",
    "        self.N0 = N0\n",
    "        self.lambd = lambd\n",
    "\n",
    "        self.Q = defaultdict(lambda: np.zeros(self.available_actions))\n",
    "        self.state_visits = defaultdict(lambda: 0)\n",
    "        self.Nsa = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "\n",
    "    def act(self, state):\n",
    "        epsilon = self.N0 / (self.N0 + self.state_visits[state])\n",
    "\n",
    "        if np.random.choice(np.arange(self.available_actions), p=[1 - epsilon, epsilon]):\n",
    "            action = np.random.choice(self.available_actions)  # Explore!\n",
    "        elif self.Q[state].max() == 0.0 and self.Q[state].min() == 0.0:\n",
    "            action = 1  # Bias toward going forward\n",
    "        else:\n",
    "            action = self.Q[state].argmax()  # Greedy action\n",
    "\n",
    "        self.state_visits[state] += 1\n",
    "\n",
    "        self.Nsa[state][action] += 1\n",
    "\n",
    "        return action\n",
    "\n",
    "    def update_Q(self, old_s, new_s, old_a, new_a, reward, E):\n",
    "        delta = reward + self.gamma * self.Q[new_s][new_a] - self.Q[old_s][old_a]\n",
    "        alpha = (1 / self.Nsa[old_s][old_a])\n",
    "\n",
    "        for s, A in E.items():\n",
    "            for a in range(len(A)):\n",
    "                self.Q[old_s][old_a] = self.Q[old_s][old_a] + alpha * delta * E[s][a]\n",
    "                E[s][a] = self.gamma * self.lambd * E[s][a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env, initial_state = environment.get_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent = agents.SarsaLambda(gamma=GAMMA, available_actions=AVAILABLE_ACTIONS, N0=N0, lambd=LAMBD)\n",
    "agent = SarsaLambda(gamma=GAMMA, available_actions=AVAILABLE_ACTIONS, N0=N0, lambd=LAMBD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "total_rewards = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "n_runs = 1000\n",
    "\n",
    "for i in range(n_runs):\n",
    "    render = i % 200 == 199\n",
    "\n",
    "    E = defaultdict(lambda: np.zeros(2)) # TODO available actions\n",
    "\n",
    "    game_over = False\n",
    "    state = env.reset()\n",
    "    state = reduce_state(state)[RAM_mask].data.tobytes()  # Select useful bytes\n",
    "    action = agent.act(state)\n",
    "    \n",
    "    score = 0\n",
    "    total_reward = 0\n",
    "\n",
    "    while not game_over:\n",
    "        if render:\n",
    "            time.sleep(0.005)\n",
    "            env.render()\n",
    "\n",
    "        old_state = state\n",
    "        old_action = action\n",
    "        ob, reward, game_over, _ = env.step(action)\n",
    "\n",
    "        ob = reduce_state(ob)\n",
    "        reward = reward_policy(reward, ob, action)\n",
    "\n",
    "        total_reward += reward\n",
    "\n",
    "        if reward == reward_policy.REWARD_IF_CROSS:\n",
    "            score += 1\n",
    "\n",
    "        state = ob[RAM_mask].data.tobytes()\n",
    "\n",
    "        action = agent.act(state)  # Next action\n",
    "        \n",
    "        E[old_state][old_action] += 1\n",
    "\n",
    "        agent.update_Q(old_s=old_state, new_s=state, old_a=old_action, new_a=action, reward=reward, E=E)\n",
    "\n",
    "    scores.append(score)\n",
    "    total_rewards.append(total_reward)\n",
    "\n",
    "    print_result(i, scores, total_reward, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_plots.plot_scores(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_plots.plot_rewards(total_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
